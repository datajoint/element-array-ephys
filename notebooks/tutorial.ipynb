{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process NeuroPixels Electrophysiology data with DataJoint Elements\n",
    "\n",
    "This notebook will walk through processing NeuroPixels array electrophysiology\n",
    "data acquired with spikeGLX and processed with kilosort. While anyone can work\n",
    "through this notebook to process electrophysiology data through DataJoint's\n",
    "`element-array-ephys` pipeline, for a detailed tutorial about the fundamentals of\n",
    "DataJoint including table types, make functions, and querying, please see the [DataJoint Tutorial](https://github.com/datajoint/datajoint-tutorials).\n",
    "\n",
    "The DataJoint Python API and Element Array Electrophysiology offer a lot of features to support collaboration, automation, reproducibility, and visualizations.\n",
    "\n",
    "For more information on these topics, please visit our documentation: \n",
    " \n",
    "- [DataJoint Core](https://datajoint.com/docs/core/): General principles\n",
    "\n",
    "- DataJoint [Python](https://datajoint.com/docs/core/datajoint-python/) and\n",
    "  [MATLAB](https://datajoint.com/docs/core/datajoint-matlab/) APIs: in-depth reviews of\n",
    "  specifics\n",
    "\n",
    "- [DataJoint Element Array Ephys](https://datajoint.com/docs/elements/element-array-ephys/):\n",
    "  A modular pipeline for electrophysiology analysis\n",
    "\n",
    "\n",
    "Let's start by importing the packages necessary to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics:\n",
    "\n",
    "Any DataJoint workflow can be broken down into basic 3 parts:\n",
    "\n",
    "- `Insert`\n",
    "- `Populate` (or process)\n",
    "- `Query`\n",
    "\n",
    "In this demo we will:\n",
    "- `Insert` metadata about an animal subject, recording session, and \n",
    "  parameters related to processing electrophysiology data.\n",
    "- `Populate` tables with outputs of ephys recording data including LFPs, and spike\n",
    "  sorted waveforms and units.\n",
    "- `Query` the processed data from the database and plot waveform traces.\n",
    "\n",
    "Each of these topics will be explained thoroughly in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow diagram\n",
    "\n",
    "This workflow is assembled from 4 DataJoint elements:\n",
    "+ [element-lab](https://github.com/datajoint/element-lab)\n",
    "+ [element-animal](https://github.com/datajoint/element-animal)\n",
    "+ [element-session](https://github.com/datajoint/element-session)\n",
    "+ [element-array-ephys](https://github.com/datajoint/element-array-ephys)\n",
    "\n",
    "Each element declares its own schema in the database. These schemas can be imported like\n",
    "any other Python package. This workflow is composed of schemas from each of the Elements\n",
    "above and correspond to a module within `workflow_array_ephys.pipeline`.\n",
    "\n",
    "The schema diagram is a good reference for understanding the order of the tables\n",
    "within the workflow, as well as the corresponding table type.\n",
    "Let's activate the elements and view the schema diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow_array_ephys.pipeline import lab, subject, session, probe, ephys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(probe)\n",
    "    + dj.Diagram(ephys)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram Breakdown\n",
    "\n",
    "While the diagram above seems complex at first, it becomes more clear when it's\n",
    "approached as a hierarchy of tables that **define the order** in which the\n",
    "workflow **expects to receive data** in each of its tables. \n",
    "\n",
    "- Tables with a green, or rectangular shape expect to receive data manually\n",
    "  using the `insert()` function. The tables higher up in the diagram such as\n",
    "  `subject.Subject()` should be the first to receive data. This ensures data\n",
    "  integrity by preventing orphaned data within DataJoint schemas. \n",
    "- Tables with a purple oval or red circle can be automatically filled with relevant data\n",
    "  by calling `populate()`. For example `ephys.EphysRecording` and its part-table\n",
    "  `ephys.EphysRecording.EphysFile` are both populated with\n",
    "  `ephys.EphysRecording.populate()`.\n",
    "- Tables connected by a solid line depend on attributes (entries) in the table\n",
    "  above it.\n",
    "\n",
    "#### Table Types\n",
    "\n",
    "There are 5 table types in DataJoint. Each of these appear in the diagram above.\n",
    "\n",
    "- **Manual table**: green box, manually inserted table, expect new entries daily, e.g. `Subject`, `ProbeInsertion`.  \n",
    "- **Lookup table**: gray box, pre inserted table, commonly used for general facts or parameters. e.g. `Strain`, `ClusteringMethod`, `ClusteringParamSet`.  \n",
    "- **Imported table**: blue oval, auto-processing table, the processing depends on the importing of external files. e.g. process of `Clustering` requires output files from kilosort2.  \n",
    "- **Computed table**: red circle, auto-processing table, the processing does not\n",
    "  depend on files external to the database.  \n",
    "- **Part table**: plain text, as an appendix to the master table, all the part\n",
    "  entries of a given master entry represent a intact set of the master entry.\n",
    "  e.g. `Unit` of a `CuratedClustering`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the workflow: Insert\n",
    "\n",
    "### Insert entries into manual tables\n",
    "\n",
    "To view details about a table's dependencies and attributes, use functions `.describe()`\n",
    "and `.heading`, respectively.\n",
    "\n",
    "Let's start with the first table in the schema diagram (the `subject` table) and view\n",
    "the table attributes we need to insert. There are two ways you can do this: *run each\n",
    "of the two cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table. These are particularly useful functions if you are new to\n",
    "DataJoint Elements and are unsure of the attributes required for each table. We will insert data into the\n",
    "`subject.Subject` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(subject=\"subject5\", sex=\"M\", subject_birth_date=\"2020-01-04\"),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between\n",
    "`.describe` and `.heading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show the dependencies and attributes for the `session.Session` table.\n",
    "Notice that `describe` shows the dependencies of the table on upstream tables. The\n",
    "`Session` table depends on the upstream `Subject` table. \n",
    "\n",
    "Whereas `heading` lists all the attributes of the `Session` table, regardless of\n",
    "whether they are declared in an upstream table. \n",
    "\n",
    "Here we will demonstrate a very useful way of inserting data by assigning the dictionary\n",
    "to a variable `session_key`. This variable can be used to insert entries into tables that\n",
    "contain the `Session` table as one of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject5\", session_datetime=\"2023-01-01 00:00:00.000\")\n",
    "session.Session.insert1(session_key, skip_duplicates=True)\n",
    "session.Session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SessionDirectory` table locates the relevant data files in a directory path\n",
    "relative to the root directory defined in your `dj.config[\"custom\"]`. More\n",
    "information about `dj.config` is provided at the end of this tutorial and is\n",
    "particularly useful for local deployments of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(\n",
    "        **session_key,\n",
    "        session_dir=\"subject5/session1\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the workflow diagram indicates, the tables in the `probe` schemas need to\n",
    "contain data before the tables in the `ephys` schema accept any data. Let's\n",
    "start by inserting into `probe.Probe`, a table containing metadata about a\n",
    "multielectrode probe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.Probe.insert1(\n",
    "    dict(probe=\"17131311651\", probe_type=\"neuropixels 1.0 - 3B\"), skip_duplicates=True\n",
    ")  # this info could be achieve from neuropixels meta file.\n",
    "probe.Probe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe metadata is used by the downstream `ProbeInsertion` table which we\n",
    "insert data into in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ProbeInsertion.insert1(\n",
    "    dict(\n",
    "        **session_key,\n",
    "        insertion_number=0,\n",
    "        probe=\"17131311651\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")  # probe, subject, session_datetime needs to follow the restrictions of foreign keys.\n",
    "ephys.ProbeInsertion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate\n",
    "\n",
    "### Automatically populate tables\n",
    "\n",
    "`ephys.EphysRecording` is the first table in the pipeline that can be populated automatically.\n",
    "If a table contains a part table, this part table is also populated during the\n",
    "`populate()` call. `populate()` takes several arguments including the a session\n",
    "key. This key restricts `populate()` to performing the operation on the session\n",
    "of interest rather than all possible sessions which could be a time-intensive\n",
    "process for databases with lots of entries.\n",
    "\n",
    "Let's view the `ephys.EphysRecording` and its part table\n",
    "`ephys.EphysRecording.EphysFile` and populate both through a single `populate()`\n",
    "call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.populate(session_key, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from element_interface.utils import dict_to_uuid, find_full_path, find_root_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (ephys.ProbeInsertion & \"subject = 'subject5'\").fetch1(\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from element_array_ephys.ephys_acute import get_ephys_root_data_dir, get_session_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_pattern = \"*.ap.meta\"\n",
    "session_dir = find_full_path(\n",
    "    get_ephys_root_data_dir(), get_session_directory(key)\n",
    "        )\n",
    "ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(session_dir.rglob(ephys_pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into each of these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.EphysRecording.EphysFile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to perform image processing with `Suite2p`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "import suite2p and insert the default parameters into a DataJoint table\n",
    "`ProcessingParamSet`. This table keeps track of all combinations of your image processing\n",
    "parameters. You can choose which parameters are used during processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `imaging.ProcessingParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert clustering task manually\n",
    "params_ks = {\n",
    "    \"fs\": 30000,\n",
    "    \"fshigh\": 150,\n",
    "    \"minfr_goodchannels\": 0.1,\n",
    "    \"Th\": [10, 4],\n",
    "    \"lam\": 10,\n",
    "    \"AUCsplit\": 0.9,\n",
    "    \"minFR\": 0.02,\n",
    "    \"momentum\": [20, 400],\n",
    "    \"sigmaMask\": 30,\n",
    "    \"ThPr\": 8,\n",
    "    \"spkTh\": -6,\n",
    "    \"reorder\": 1,\n",
    "    \"nskip\": 25,\n",
    "    \"GPU\": 1,\n",
    "    \"Nfilt\": 1024,\n",
    "    \"nfilt_factor\": 4,\n",
    "    \"ntbuff\": 64,\n",
    "    \"whiteningRange\": 32,\n",
    "    \"nSkipCov\": 25,\n",
    "    \"scaleproc\": 200,\n",
    "    \"nPCs\": 3,\n",
    "    \"useRAM\": 0,\n",
    "}\n",
    "ephys.ClusteringParamSet.insert_new_params(\n",
    "    clustering_method=\"kilosort2\",\n",
    "    paramset_idx=0,\n",
    "    params=params_ks,\n",
    "    paramset_desc=\"Spike sorting using Kilosort2\",\n",
    ")\n",
    "ephys.ClusteringParamSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted suite2p parameters into the `ProcessingParamSet` table,\n",
    "we're almost ready to run image processing. DataJoint uses a `ProcessingTask` table to\n",
    "manage which `Scan` and `ProcessingParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=0,\n",
    "        paramset_idx=0,\n",
    "        clustering_output_dir=\"subject6/session1/towersTask_g0_imec0\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Clustering.populate(display_progress=True)\n",
    "ephys.CuratedClustering.populate(session_key, display_progress=True)\n",
    "ephys.LFP.populate(session_key, display_progress=True)\n",
    "ephys.WaveformSet.populate(session_key, display_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff52d424e56dd643d8b2ec122f40a2e279e94970100b4e6430cb9025a65ba4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
