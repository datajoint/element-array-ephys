{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element Array Electrophysiology", "text": "<p>This Element features DataJoint schemas for analyzing extracellular array electrophysiology data acquired with Neuropixels probes and spike sorted using Kilosort spike sorter. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p> <p></p> <p>The Element is comprised of <code>probe</code> and <code>ephys</code> schemas. Several <code>ephys</code> schemas are developed to handle various use cases of this pipeline and workflow:</p> <ul> <li><code>ephys_acute</code>: A probe is inserted into a new location during each session.</li> </ul> <ul> <li><code>ephys_chronic</code>: A probe is inserted once and used to record across multiple   sessions.</li> </ul> <ul> <li><code>ephys_precluster</code>: A probe is inserted into a new location during each session.   Pre-clustering steps are performed on the data from each probe prior to Kilosort   analysis.</li> </ul> <ul> <li><code>ephys_no_curation</code>: A probe is inserted into a new location during each session and   Kilosort-triggered clustering is performed without the option to manually curate the   results.</li> </ul> <p>Visit the Concepts page for more information about the use cases of <code>ephys</code> schemas and an explanation of the tables. To get started with building your own data pipeline, visit the Tutorials page.</p>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and  Keep a Changelog convention.</p>"}, {"location": "changelog/#0211-2023-06-29", "title": "[0.2.11] - 2023-06-29", "text": "<ul> <li>Update - Improve kilosort triggering routine - better logging, remove temporary files, robust resumable processing</li> <li>Add - Null value for <code>package_version</code> to patch bug</li> <li>Update - GitHub Actions workflows</li> <li>Update - README instructions</li> </ul>"}, {"location": "changelog/#0210-2023-05-26", "title": "0.2.10 - 2023-05-26", "text": "<ul> <li>Add - Kilosort, NWB, and DANDI citations</li> <li>Fix - CSS to improve readability of tables in dark mode</li> <li>Update - mkdocs.yaml</li> </ul>"}, {"location": "changelog/#029-2023-05-11", "title": "0.2.9 - 2023-05-11", "text": "<ul> <li>Fix - <code>.ipynb</code> dark mode output for all notebooks.</li> </ul>"}, {"location": "changelog/#028-2023-04-28", "title": "0.2.8 - 2023-04-28", "text": "<ul> <li>Fix - <code>.ipynb</code> output in tutorials is not visible in dark mode.</li> </ul>"}, {"location": "changelog/#027-2023-04-19", "title": "0.2.7 - 2023-04-19", "text": "<ul> <li>Bugfix - A name remapping dictionary was added to ensure consistency between the column names of the <code>metrics.csv</code> file and the attribute names of the <code>QualityMetrics</code> table</li> </ul>"}, {"location": "changelog/#026-2023-04-17", "title": "0.2.6 - 2023-04-17", "text": "<ul> <li>Fix - Update Pandas DataFrame column name to insert <code>pt_ratio</code> in <code>QualityMetrics.Waveform</code> table</li> </ul>"}, {"location": "changelog/#025-2023-04-12", "title": "0.2.5 - 2023-04-12", "text": "<ul> <li>Add - docstrings for quality metric tables</li> <li>Fix - docstring errors</li> <li>Update - <code>concepts.md</code></li> <li>Update - schema diagrams with quality metrics tables</li> </ul>"}, {"location": "changelog/#024-2023-03-10", "title": "0.2.4 - 2023-03-10", "text": "<ul> <li>Update - Requirements with <code>ipywidgets</code> and <code>scikit-image</code> for plotting widget</li> </ul>"}, {"location": "changelog/#023-2023-02-14", "title": "0.2.3 - 2023-02-14", "text": "<ul> <li>Add - extras_require install options for nwb and development requirement sets</li> <li>Add - mkdocs notebook rendering</li> <li>Add - markdown linting and spellcheck config files, with implementation edits</li> <li>Update - license for 2023</li> <li>Update - blackify previous updates</li> </ul>"}, {"location": "changelog/#022-2022-01-11", "title": "0.2.2 - 2022-01-11", "text": "<ul> <li>Bugfix - Revert import order in <code>__init__.py</code> to avoid circular import error.</li> <li>Update - <code>.pre-commit-config.yaml</code> to disable automatic positioning of import   statement at the top.</li> <li>Bugfix - Update docstrings to render API for documentation website.</li> </ul>"}, {"location": "changelog/#021-2022-01-06", "title": "0.2.1 - 2022-01-06", "text": "<ul> <li>Add - <code>build_electrode_layouts</code> function in <code>probe.py</code> to compute the electrode layout   for all types of probes.</li> <li>Update - parameterize run_CatGT step from parameters retrieved from   <code>ClusteringParamSet</code> table</li> <li>Update - clustering step, update duration for \"median_subtraction\" step</li> <li>Bugfix - handles single probe recording in \"Neuropix-PXI\" format</li> <li>Update - safeguard in creating/inserting probe types upon probe activation</li> <li>Add - quality control metric dashboard</li> <li>Update &amp; fix docstrings</li> <li>Update - <code>ephys_report.UnitLevelReport</code> to add <code>ephys.ClusterQualityLabel</code> as a   foreign key reference</li> <li>Add - <code>.pre-commit-config.yaml</code></li> </ul>"}, {"location": "changelog/#020-2022-10-28", "title": "0.2.0 - 2022-10-28", "text": "<ul> <li>Add - New schema <code>ephys_report</code> to compute and store figures from results</li> <li>Add - Widget to display figures</li> <li>Add - Add <code>ephys_no_curation</code> and routines to trigger spike-sorting analysis   using Kilosort (2.0, 2.5)</li> <li>Add - mkdocs for Element Documentation</li> <li>Add - New <code>QualityMetrics</code> table to store clusters' and waveforms' metrics after the   spike sorting analysis.</li> </ul>"}, {"location": "changelog/#014-2022-07-11", "title": "0.1.4 - 2022-07-11", "text": "<ul> <li>Bugfix - Handle case where <code>spike_depths</code> data is present.</li> </ul>"}, {"location": "changelog/#013-2022-06-16", "title": "0.1.3 - 2022-06-16", "text": "<ul> <li>Update - Allow for the <code>precluster_output_dir</code> attribute to be nullable when no   pre-clustering is performed.</li> </ul>"}, {"location": "changelog/#012-2022-06-09", "title": "0.1.2 - 2022-06-09", "text": "<ul> <li>Bugfix - Handle case where <code>pc_features.npy</code> does not exist.</li> </ul>"}, {"location": "changelog/#011-2022-06-01", "title": "0.1.1 - 2022-06-01", "text": "<ul> <li>Add - Secondary attributes to <code>PreClusterParamSteps</code> table</li> </ul>"}, {"location": "changelog/#010-2022-05-26", "title": "0.1.0 - 2022-05-26", "text": "<ul> <li>Update - Rename module for acute probe insertions from <code>ephys.py</code> to <code>ephys_acute.py</code>.</li> <li>Add - Module for pre-clustering steps (<code>ephys_precluster.py</code>), which is built off of   <code>ephys_acute.py</code>.</li> <li>Add - Module for chronic probe insertions (<code>ephys_chronic.py</code>).</li> <li>Bugfix - Missing <code>fileTimeSecs</code> key in SpikeGLX meta file.</li> <li>Update - Move common functions to <code>element-interface</code> package.</li> <li>Add - NWB export function</li> </ul>"}, {"location": "changelog/#010b4-2021-11-29", "title": "0.1.0b4 - 2021-11-29", "text": "<ul> <li>Add - Processing with Kilosort and pyKilosort for Open Ephys and SpikeGLX</li> </ul>"}, {"location": "changelog/#010b0-2021-05-07", "title": "0.1.0b0 - 2021-05-07", "text": "<ul> <li>Update - First beta release</li> </ul>"}, {"location": "changelog/#010a5-2021-05-05", "title": "0.1.0a5 - 2021-05-05", "text": "<ul> <li>Add - GitHub Action release process</li> <li>Add - <code>probe</code> and <code>ephys</code> elements</li> <li>Add - Readers for: <code>SpikeGLX</code>, <code>Open Ephys</code>, <code>Kilosort</code></li> <li>Add - Probe table supporting: Neuropixels probes 1.0 - 3A, 1.0 - 3B, 2.0 - SS,   2.0 - MS</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the following resources, please cite the respective manuscript and/or Research Resource Identifier (RRID):</p> <ul> <li> <p>DataJoint Element Array Electrophysiology - Version 0.2.11</p> <ul> <li>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,   Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for   Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>RRID:SCR_021894</li> </ul> </li> </ul> <ul> <li>Kilosort<ul> <li>Manuscripts</li> </ul> </li> </ul> <ul> <li>NWB<ul> <li>Manuscript</li> </ul> </li> </ul> <ul> <li>DANDI<ul> <li>Citation options</li> </ul> </li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#acquisition-tools-for-electrophysiology", "title": "Acquisition Tools for Electrophysiology", "text": ""}, {"location": "concepts/#neuropixels-probes", "title": "Neuropixels Probes", "text": "<p>Neuropixels probes were developed by a collaboration between HHMI Janelia, industry partners, and others1. Since their initial release in October 2018, 300 labs have ordered 1200 probes. Since the rollout of Neuropixels 2.0 in October 2020, IMEC has been shipping 100+ probes monthly (correspondence with Tim Harris).</p> <p>Neuropixels probes offer 960 electrode sites along a 10mm long shank, with 384 recordable channels per probe that can record hundreds of units spanning multiple brain regions (Neuropixels 2.0 version is a 4-shank probe with 1280 electrode sites per shank). Such large recording capacity has offered tremendous opportunities for the field of neurophysiology research, yet this is accompanied by an equally great challenge in terms of data and computation management.</p>"}, {"location": "concepts/#data-acquisition-tools", "title": "Data Acquisition Tools", "text": "<p>Some commonly used acquisition tools and systems by the neuroscience research community include:</p> <ul> <li>Neuropixels probes</li> <li>Tetrodes</li> <li>SpikeGLX</li> <li>OpenEphys</li> <li>Neuralynx</li> <li>Axona</li> <li>...</li> </ul>"}, {"location": "concepts/#data-preprocessing-tools", "title": "Data Preprocessing Tools", "text": "<p>The preprocessing pipeline includes bandpass filtering for LFP extraction, bandpass filtering for spike sorting, spike sorting, manual curation of the spike sorting results, and calculation of quality control metrics. In trial-based experiments, the spike trains are aligned and separated into trials. Standard processing may include PSTH computation aligned to trial onset or other events, and often grouped by different trial types. Neuroscience groups have traditionally developed custom home-made toolchains.</p> <p>In recent years, several leaders have been emerging as de facto standards with significant community uptake:</p> <ul> <li>Kilosort</li> <li>pyKilosort</li> <li>JRClust</li> <li>KlustaKwik</li> <li>Mountainsort</li> <li>spikeinterface (wrapper)</li> <li>spyking-circus</li> <li>spikeforest</li> <li>...</li> </ul> <p>Kilosort provides most automation and has gained significant popularity, being adopted as one of the key spike sorting methods in the majority of the teams/collaborations we have worked with. As part of our Year-1 NIH U24 effort, we provide support for data ingestion of spike sorting results from Kilosort. Further effort will be devoted for the ingestion support of other spike sorting methods. On this end, a framework for unifying existing spike sorting methods, named SpikeInterface, has been developed by Alessio Buccino, et al. SpikeInterface provides a convenient Python-based wrapper to invoke, extract, compare spike sorting results from different sorting algorithms.</p>"}, {"location": "concepts/#key-partnerships", "title": "Key Partnerships", "text": "<p>Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for Neuropixels probes. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experimental workflow, pipeline design, associated tools, and interfaces. These teams include:</p> <ul> <li>International Brain Lab:   https://github.com/int-brain-lab/IBL-pipeline</li> </ul> <ul> <li>Mesoscale Activity Project (HHMI Janelia):   https://github.com/mesoscale-activity-map/map-ephys</li> </ul> <ul> <li>Moser Group (Norwegian University of Science and Technology): see    pipeline design</li> </ul> <ul> <li>Andreas Tolias Lab (Baylor College of Medicine)</li> </ul> <ul> <li>BrainCoGs (Princeton Neuroscience Institute)</li> </ul> <ul> <li>Brody Lab (Princeton University)</li> </ul>"}, {"location": "concepts/#element-architecture", "title": "Element Architecture", "text": "<p>Each of the DataJoint Elements creates a set of tables for common neuroscience data modalities to organize, preprocess, and analyze data. Each node in the following diagram is a table within the Element or a table connected to the Element.</p>"}, {"location": "concepts/#ephys_acute-module", "title": "<code>ephys_acute</code> module", "text": ""}, {"location": "concepts/#ephys_chronic-module", "title": "<code>ephys_chronic</code> module", "text": ""}, {"location": "concepts/#ephys_precluster-module", "title": "<code>ephys_precluster</code> module", "text": ""}, {"location": "concepts/#ephys_no_curation-module", "title": "<code>ephys_no_curation</code> module", "text": ""}, {"location": "concepts/#subject-schema-api-docs", "title": "<code>subject</code> schema (API docs)", "text": "<p>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</p> Table Description Subject A table containing basic information of the research subject."}, {"location": "concepts/#session-schema-api-docs", "title": "<code>session</code> schema (API docs)", "text": "Table Description Session A table for unique experimental session identifiers."}, {"location": "concepts/#probe-schema-api-docs", "title": "<code>probe</code> schema (API docs)", "text": "<p>Tables related to the Neuropixels probe and electrode configuration.</p> Table Description ProbeType A lookup table specifying the type of Neuropixels probe (e.g. \"neuropixels 1.0\", \"neuropixels 2.0 single-shank\"). ProbeType.Electrode A table containing electrodes and their properties for a particular probe type. Probe A record of an actual physical probe. ElectrodeConfig A record of a particular electrode configuration to be used for ephys recording. ElectrodeConfig.Electrode A record of electrodes out of those in <code>ProbeType.Electrode</code> that are used for recording."}, {"location": "concepts/#ephys-schema-api-docs", "title": "<code>ephys</code> schema (API docs)", "text": "<p>Tables related to information about physiological recordings and automatic ingestion of spike sorting results.</p> Table Description ProbeInsertion A record of surgical insertions of a probe in the brain. EphysRecording A table with metadata about each electrophysiogical recording. Clustering A table with clustering data for spike sorting extracellular electrophysiology data. Curation A table to declare optional manual curation of spike sorting results. CuratedClustering A table with metadata for sorted data generated after each curation. CuratedClustering.Unit A part table containing single unit information after spike sorting and optional curation. WaveformSet A table containing spike waveforms for single units."}, {"location": "concepts/#ephys_report-schema-api-docs", "title": "<code>ephys_report</code> schema (API docs)", "text": "<p>Tables for storing probe or unit-level visualization results.</p> Table Description ProbeLevelReport A table to store drift map figures generated from each recording probe. UnitLevelReport A table to store figures (waveforms, autocorrelogram, peak waveform + neighbors) generated for each unit. QualityMetricCutoffs A table to store cut-off values for cluster quality metrics. QualityMetricSet A manual table to match a set of cluster quality metric values with desired cut-offs. QualityMetricReport A table to store quality metric figures."}, {"location": "concepts/#element-development", "title": "Element Development", "text": "<p>Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the  Array Electrophysiology Element.</p> <p>Major features of the Array Electrophysiology Element include:</p> <ul> <li>Pipeline architecture detailed by:<p>+ Probe, electrode configuration compatible with Neuropixels probes and     generalizable to other types of probes (e.g. tetrodes) - supporting both <code>chronic</code>     and <code>acute</code> probe insertion modes.</p> <p>+ Probe-insertion, ephys-recordings, LFP extraction, clusterings, curations, sorted     units and the associated data (e.g. spikes, waveforms, etc.).</p> <p>+ Store/track/manage different curations of the spike sorting results - supporting     both curated clustering and kilosort triggered clustering (i.e., <code>no_curation</code>).</p> </li> </ul> <ul> <li>Ingestion support for data acquired with SpikeGLX and OpenEphys acquisition systems. </li> <li>Ingestion support for spike sorting outputs from Kilosort.</li> <li>Triggering support for workflow integrated Kilosort processing.</li> <li>Sample data and complete test suite for quality assurance.</li> </ul>"}, {"location": "concepts/#data-export-and-publishing", "title": "Data Export and Publishing", "text": "<p>Element Array Electrophysiology supports exporting of all data into standard Neurodata Without Borders (NWB) files. This makes it easy to share files with collaborators and publish results on DANDI Archive. NWB, as an organization, is dedicated to standardizing data formats and maximizing interoperability across tools for neurophysiology. For more information on uploading NWB files to DANDI within the DataJoint Elements ecosystem see the corresponding notebook on the tutorials page. </p> <p>To use the export functionality with additional related dependencies, install the Element with the <code>nwb</code> option as follows:</p> <pre><code>pip install element-array-ephys[nwb]\n</code></pre>"}, {"location": "concepts/#roadmap", "title": "Roadmap", "text": "<p>Incorporation of SpikeInterface into the Array Electrophysiology Element will be on DataJoint Elements development roadmap. Dr. Loren Frank has led a development effort of a DataJoint pipeline with SpikeInterface framework and NeurodataWithoutBorders format integrated https://github.com/LorenFrankLab/nwb_datajoint.</p> <p>Future additions to this element will add functionality to support large (&gt; 48 hours) neuropixel recordings via an overlapping segmented processing approach. </p> <p>Further development of this Element is community driven. Upon user requests we will continue adding features to this Element.</p> <ol> <li> <p>Jun, J., Steinmetz, N., Siegle, J. et al. Fully integrated silicon probes for high-density recording of neural activity. Nature 551, 232\u2013236 (2017). https://doi.org/10.1038/nature24636.\u00a0\u21a9</p> </li> </ol>"}, {"location": "api/make_pages/", "title": "Make pages", "text": "In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the api pages and navigation.\nNOTE: Works best when following the Google style guide\nhttps://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\nhttps://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\n\"\"\"\n</pre> \"\"\"Generate the api pages and navigation. NOTE: Works best when following the Google style guide https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings \"\"\" In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\nfrom pathlib import Path\nimport os\n</pre> import mkdocs_gen_files from pathlib import Path import os In\u00a0[\u00a0]: Copied! <pre>package = os.getenv(\"PACKAGE\")\n</pre> package = os.getenv(\"PACKAGE\") In\u00a0[\u00a0]: Copied! <pre>element = package.split(\"_\", 1)[1]\n</pre> element = package.split(\"_\", 1)[1] In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\nfor path in sorted(Path(package).glob(\"**/*.py\")) + sorted(\n    Path(f\"workflow_{element}\").glob(\"**/*.py\")\n):\n    if path.stem == \"__init__\":\n        continue\n    with mkdocs_gen_files.open(f\"api/{path.with_suffix('')}.md\", \"w\") as f:\n        module_path = \".\".join(\n            [p for p in path.with_suffix(\"\").parts if p != \"__init__\"]\n        )\n        print(f\"::: {module_path}\", file=f)\n    nav[path.parts] = f\"{path.with_suffix('')}.md\"\n</pre> nav = mkdocs_gen_files.Nav() for path in sorted(Path(package).glob(\"**/*.py\")) + sorted(     Path(f\"workflow_{element}\").glob(\"**/*.py\") ):     if path.stem == \"__init__\":         continue     with mkdocs_gen_files.open(f\"api/{path.with_suffix('')}.md\", \"w\") as f:         module_path = \".\".join(             [p for p in path.with_suffix(\"\").parts if p != \"__init__\"]         )         print(f\"::: {module_path}\", file=f)     nav[path.parts] = f\"{path.with_suffix('')}.md\" In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"api/navigation.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"api/navigation.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"}, {"location": "api/element_array_ephys/ephys_acute/", "title": "ephys_acute.py", "text": ""}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.activate", "title": "<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe schema.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe schema.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n        get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    root_directories = _linking_module.get_ephys_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string for the full path to the root directory for processed data.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_processed_root_data_dir() -&gt; str:\n\"\"\"Retrieve the root directory for all processed data.\n\n    Returns:\n        A string for the full path to the root directory for processed data.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_ephys_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.AcquisitionSoftware", "title": "<code>AcquisitionSoftware</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Software used for recording of neuropixels probes\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ProbeInsertion", "title": "<code>ProbeInsertion</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion implanted into an animal for a given session.\n    -&gt; Session\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    \"\"\"\n\n    @classmethod\n    def auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(session_key)\n        )\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n            )\n\n        probe_list, probe_insertion_list = [], []\n        if acq_software == \"SpikeGLX\":\n            for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                probe_key = {\n                    \"probe_type\": spikeglx_meta.probe_model,\n                    \"probe\": spikeglx_meta.probe_SN,\n                }\n                if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                    probe_list.append(probe_key)\n\n                probe_dir = meta_filepath.parent\n                try:\n                    probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                    probe_number = int(probe_number.replace(\"imec\", \"\"))\n                except AttributeError:\n                    probe_number = meta_fp_idx\n\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": spikeglx_meta.probe_SN,\n                        \"insertion_number\": int(probe_number),\n                    }\n                )\n        elif acq_software == \"Open Ephys\":\n            loaded_oe = openephys.OpenEphys(session_dir)\n            for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n                probe_key = {\n                    \"probe_type\": oe_probe.probe_model,\n                    \"probe\": oe_probe.probe_SN,\n                }\n                if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                    probe_list.append(probe_key)\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": oe_probe.probe_SN,\n                        \"insertion_number\": probe_idx,\n                    }\n                )\n        else:\n            raise NotImplementedError(f\"Unknown acquisition software: {acq_software}\")\n\n        probe.Probe.insert(probe_list, skip_duplicates=True)\n        cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ProbeInsertion.auto_generate_entries", "title": "<code>auto_generate_entries(session_key)</code>  <code>classmethod</code>", "text": "<p>Automatically populate entries in ProbeInsertion table for a session.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(session_key)\n    )\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n        )\n\n    probe_list, probe_insertion_list = [], []\n    if acq_software == \"SpikeGLX\":\n        for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n            probe_key = {\n                \"probe_type\": spikeglx_meta.probe_model,\n                \"probe\": spikeglx_meta.probe_SN,\n            }\n            if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                probe_list.append(probe_key)\n\n            probe_dir = meta_filepath.parent\n            try:\n                probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                probe_number = int(probe_number.replace(\"imec\", \"\"))\n            except AttributeError:\n                probe_number = meta_fp_idx\n\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": spikeglx_meta.probe_SN,\n                    \"insertion_number\": int(probe_number),\n                }\n            )\n    elif acq_software == \"Open Ephys\":\n        loaded_oe = openephys.OpenEphys(session_dir)\n        for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n            probe_key = {\n                \"probe_type\": oe_probe.probe_model,\n                \"probe\": oe_probe.probe_SN,\n            }\n            if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                probe_list.append(probe_key)\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": oe_probe.probe_SN,\n                    \"insertion_number\": probe_idx,\n                }\n            )\n    else:\n        raise NotImplementedError(f\"Unknown acquisition software: {acq_software}\")\n\n    probe.Probe.insert(probe_list, skip_duplicates=True)\n    cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.InsertionLocation", "title": "<code>InsertionLocation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording", "title": "<code>EphysRecording</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found\"\n                f\" in {session_dir}\"\n            )\n\n        supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n\n            if spikeglx_meta.probe_model in supported_probe_types:\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(key)\n                )\n\n            if not probe_data.ap_meta:\n                raise IOError(\n                    'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n                )\n\n            if probe_data.probe_model in supported_probe_types:\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n            # explicitly garbage collect \"dataset\"\n            # as these may have large memory footprint and may not be cleared fast enough\n            del probe_data, dataset\n            gc.collect()\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording.EphysFile", "title": "<code>EphysFile</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording.make", "title": "<code>make(key)</code>", "text": "<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found\"\n            f\" in {session_dir}\"\n        )\n\n    supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No SpikeGLX data found for probe insertion: {}\".format(key)\n            )\n\n        if spikeglx_meta.probe_model in supported_probe_types:\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(key)\n            )\n\n        if not probe_data.ap_meta:\n            raise IOError(\n                'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n            )\n\n        if probe_data.probe_model in supported_probe_types:\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n        # explicitly garbage collect \"dataset\"\n        # as these may have large memory footprint and may not be cleared fast enough\n        del probe_data, dataset\n        gc.collect()\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP", "title": "<code>LFP</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; EphysRecording\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            oe_probe = get_openephys_probe_data(key)\n\n            lfp_channel_ind = np.r_[\n                len(oe_probe.lfp_meta[\"channels_indices\"])\n                - 1 : 0 : -self._skip_channel_counts\n            ]\n\n            # (sample x channel)\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n            lfp = (\n                lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_keys.extend(\n                probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n            )\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP.make", "title": "<code>make(key)</code>", "text": "<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(key)\n\n        lfp_channel_ind = np.r_[\n            len(oe_probe.lfp_meta[\"channels_indices\"])\n            - 1 : 0 : -self._skip_channel_counts\n        ]\n\n        # (sample x channel)\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n        lfp = (\n            lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        electrode_keys.extend(\n            probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n        )\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringMethod", "title": "<code>ClusteringMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n        (\"kilosort2.5\", \"kilosort2.5 clustering method\"),\n        (\"kilosort3\", \"kilosort3 clustering method\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringParamSet", "title": "<code>ClusteringParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameters for clustering with Kilosort.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob): Parameters for clustering with Kilosort.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        clustering_method: str,\n        paramset_desc: str,\n        params: dict,\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"clustering_method\": clustering_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(\n                {**params, \"clustering_method\": clustering_method}\n            ),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringParamSet.insert_new_params", "title": "<code>insert_new_params(clustering_method, paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>", "text": "<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    clustering_method: str,\n    paramset_desc: str,\n    params: dict,\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"clustering_method\": clustering_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(\n            {**params, \"clustering_method\": clustering_method}\n        ),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusterQualityLabel", "title": "<code>ClusterQualityLabel</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code> varchar(4000) </code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description ( varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)  # cluster quality type - e.g. 'good', 'MUA', 'noise', etc.\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask", "title": "<code>ClusteringTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_output_dir</code> <code> varchar (255) </code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_output_dir ( varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir='': varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(\n        cls, key: dict, relative: bool = False, mkdir: bool = False\n    ) -&gt; pathlib.Path:\n\"\"\"Infer output directory if it is not provided.\n\n        Args:\n            key (dict): ClusteringTask primary key.\n\n        Returns:\n            Expected clustering_output_dir based on the following convention:\n                processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n                e.g.: sub4/sess1/probe_2/kilosort2_0\n        \"\"\"\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n        root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n        method = (\n            (ClusteringParamSet * ClusteringMethod &amp; key)\n            .fetch1(\"clustering_method\")\n            .replace(\".\", \"-\")\n        )\n\n        output_dir = (\n            processed_dir\n            / session_dir.relative_to(root_dir)\n            / f'probe_{key[\"insertion_number\"]}'\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            log.info(f\"{output_dir} created!\")\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n        Args:\n            ephys_recording_key (dict): EphysRecording primary key.\n            paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n        \"\"\"\n        key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n        try:\n            kilosort.Kilosort(\n                output_dir\n            )  # check if the directory is a valid Kilosort output\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"clustering_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer output directory if it is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>ClusteringTask primary key.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef infer_output_dir(\n    cls, key: dict, relative: bool = False, mkdir: bool = False\n) -&gt; pathlib.Path:\n\"\"\"Infer output directory if it is not provided.\n\n    Args:\n        key (dict): ClusteringTask primary key.\n\n    Returns:\n        Expected clustering_output_dir based on the following convention:\n            processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n            e.g.: sub4/sess1/probe_2/kilosort2_0\n    \"\"\"\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n    root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n    method = (\n        (ClusteringParamSet * ClusteringMethod &amp; key)\n        .fetch1(\"clustering_method\")\n        .replace(\".\", \"-\")\n    )\n\n    output_dir = (\n        processed_dir\n        / session_dir.relative_to(root_dir)\n        / f'probe_{key[\"insertion_number\"]}'\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        log.info(f\"{output_dir} created!\")\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask.auto_generate_entries", "title": "<code>auto_generate_entries(ephys_recording_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Autogenerate entries based on a particular ephys recording.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_recording_key</code> <code>dict</code> <p>EphysRecording primary key.</p> required <code>paramset_idx</code> <code>int</code> <p>Parameter index to use for clustering task. Defaults to 0.</p> <code>0</code> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n    Args:\n        ephys_recording_key (dict): EphysRecording primary key.\n        paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n    \"\"\"\n    key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n    try:\n        kilosort.Kilosort(\n            output_dir\n        )  # check if the directory is a valid Kilosort output\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"clustering_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering", "title": "<code>Clustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code> varchar(16) </code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version ( varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update clustering_output_dir\n            ClusteringTask.update1(\n                {**key, \"clustering_output_dir\": output_dir.as_posix()}\n            )\n\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n        elif task_mode == \"trigger\":\n            acq_software, clustering_method, params = (\n                ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n            ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n            if \"kilosort\" in clustering_method:\n                from element_array_ephys.readers import kilosort_triggering\n\n                # add additional probe-recording and channels details into `params`\n                params = {**params, **get_recording_channels_details(key)}\n                params[\"fs\"] = params[\"sample_rate\"]\n\n                if acq_software == \"SpikeGLX\":\n                    spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                    spikeglx_recording = spikeglx.SpikeGLX(\n                        spikeglx_meta_filepath.parent\n                    )\n                    spikeglx_recording.validate_file(\"ap\")\n                    run_CatGT = (\n                        params.pop(\"run_CatGT\", True)\n                        and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                    )\n\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=spikeglx_recording.root_dir\n                            / (spikeglx_recording.root_name + \".ap.bin\"),\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                            npx_input_dir=spikeglx_meta_filepath.parent,\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                            run_CatGT=run_CatGT,\n                        )\n                        run_kilosort.run_modules()\n                elif acq_software == \"Open Ephys\":\n                    oe_probe = get_openephys_probe_data(key)\n\n                    assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                    # run kilosort\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=pathlib.Path(\n                                oe_probe.recording_info[\"recording_files\"][0]\n                            )\n                            / \"continuous.dat\",\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                            npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        )\n                        run_kilosort.run_modules()\n            else:\n                raise NotImplementedError(\n                    f\"Automatic triggering of {clustering_method}\"\n                    f\" clustering analysis is not yet supported\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering.make", "title": "<code>make(key)</code>", "text": "<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update clustering_output_dir\n        ClusteringTask.update1(\n            {**key, \"clustering_output_dir\": output_dir.as_posix()}\n        )\n\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n    elif task_mode == \"trigger\":\n        acq_software, clustering_method, params = (\n            ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n        ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n        if \"kilosort\" in clustering_method:\n            from element_array_ephys.readers import kilosort_triggering\n\n            # add additional probe-recording and channels details into `params`\n            params = {**params, **get_recording_channels_details(key)}\n            params[\"fs\"] = params[\"sample_rate\"]\n\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                spikeglx_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent\n                )\n                spikeglx_recording.validate_file(\"ap\")\n                run_CatGT = (\n                    params.pop(\"run_CatGT\", True)\n                    and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                )\n\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=spikeglx_recording.root_dir\n                        / (spikeglx_recording.root_name + \".ap.bin\"),\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                        npx_input_dir=spikeglx_meta_filepath.parent,\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        run_CatGT=run_CatGT,\n                    )\n                    run_kilosort.run_modules()\n            elif acq_software == \"Open Ephys\":\n                oe_probe = get_openephys_probe_data(key)\n\n                assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                # run kilosort\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=pathlib.Path(\n                            oe_probe.recording_info[\"recording_files\"][0]\n                        )\n                        / \"continuous.dat\",\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                        npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                    )\n                    run_kilosort.run_modules()\n        else:\n            raise NotImplementedError(\n                f\"Automatic triggering of {clustering_method}\"\n                f\" clustering analysis is not yet supported\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curation procedure table.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> <code>curation_id</code> <code>foreign key, int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time when curation results are generated.</p> <code>curation_output_dir</code> <code> varchar(255) </code> <p>Output directory of the curated results.</p> <code>quality_control</code> <code>bool</code> <p>If True, this clustering result has undergone quality control.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this clustering result.</p> <code>curation_note</code> <code> varchar(2000) </code> <p>Notes about the curation task.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curation procedure table.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n        curation_id (foreign key, int): Unique curation ID.\n        curation_time (datetime): Time when curation results are generated.\n        curation_output_dir ( varchar(255) ): Output directory of the curated results.\n        quality_control (bool): If True, this clustering result has undergone quality control.\n        manual_curation (bool): If True, manual curation has been performed on this clustering result.\n        curation_note ( varchar(2000) ): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual curation procedure\n    -&gt; Clustering\n    curation_id: int\n    ---\n    curation_time: datetime             # time of generation of this set of curated clustering results\n    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n    quality_control: bool               # has this clustering result undergone quality control?\n    manual_curation: bool               # has manual curation been performed on this clustering result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_clustering_task(self, key, curation_note=\"\"):\n\"\"\"\n        A function to create a new corresponding \"Curation\" for a particular\n        \"ClusteringTask\"\n        \"\"\"\n        if key not in Clustering():\n            raise ValueError(\n                f\"No corresponding entry in Clustering available\"\n                f\" for: {key}; do `Clustering.populate(key)`\"\n            )\n\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n            kilosort_dir\n        )\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": creation_time,\n                \"curation_output_dir\": output_dir,\n                \"quality_control\": is_qc,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation.create1_from_clustering_task", "title": "<code>create1_from_clustering_task(key, curation_note='')</code>", "text": "<p>A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\"</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def create1_from_clustering_task(self, key, curation_note=\"\"):\n\"\"\"\n    A function to create a new corresponding \"Curation\" for a particular\n    \"ClusteringTask\"\n    \"\"\"\n    if key not in Clustering():\n        raise ValueError(\n            f\"No corresponding entry in Clustering available\"\n            f\" for: {key}; do `Clustering.populate(key)`\"\n        )\n\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n        kilosort_dir\n    )\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": creation_time,\n            \"curation_output_dir\": output_dir,\n            \"quality_control\": is_qc,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering", "title": "<code>CuratedClustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Curation primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Curation (foreign key): Curation primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of a curation.\n    -&gt; Curation\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n            \"acq_software\", \"sampling_rate\"\n        )\n\n        sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / sample_rate\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ]\n                        if spike_depths is not None\n                        else None,\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering.Unit", "title": "<code>Unit</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering.make", "title": "<code>make(key)</code>", "text": "<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / sample_rate\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ]\n                    if spike_depths is not None\n                    else None,\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet", "title": "<code>WaveformSet</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if is_qc:\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            if unit_peak_waveform:\n                self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n            if unit_electrode_waveforms:\n                self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.PeakWaveform", "title": "<code>PeakWaveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.make", "title": "<code>make(key)</code>", "text": "<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if is_qc:\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        if unit_peak_waveform:\n            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n        if unit_electrode_waveforms:\n            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics", "title": "<code>QualityMetrics</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n        rename_dict = {\n            \"isi_viol\": \"isi_violation\",\n            \"num_viol\": \"number_violation\",\n            \"contam_rate\": \"contamination_rate\",\n        }\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        metrics_df.columns = metrics_df.columns.str.lower()\n        metrics_df.rename(columns=rename_dict, inplace=True)\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.Cluster", "title": "<code>Cluster</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> <code>velocity_below</code> <code>float</code> <p>inverse velocity of waveform propagation from soma toward the bottom of the probe.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n    rename_dict = {\n        \"isi_viol\": \"isi_violation\",\n        \"num_viol\": \"number_violation\",\n        \"contam_rate\": \"contamination_rate\",\n    }\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    metrics_df.columns = metrics_df.columns.str.lower()\n    metrics_df.rename(columns=rename_dict, inplace=True)\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_spikeglx_meta_filepath", "title": "<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>", "text": "<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = pathlib.Path(\n        (\n            EphysRecording.EphysFile\n            &amp; ephys_recording_key\n            &amp; 'file_path LIKE \"%.ap.meta\"'\n        ).fetch1(\"file_path\")\n    )\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_openephys_probe_data", "title": "<code>get_openephys_probe_data(ephys_recording_key)</code>", "text": "<p>Get OpenEphys probe data from file.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_openephys_probe_data(ephys_recording_key: dict) -&gt; list:\n\"\"\"Get OpenEphys probe data from file.\"\"\"\n    inserted_probe_serial_number = (\n        ProbeInsertion * probe.Probe &amp; ephys_recording_key\n    ).fetch1(\"probe\")\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n    )\n    loaded_oe = openephys.OpenEphys(session_dir)\n    probe_data = loaded_oe.probes[inserted_probe_serial_number]\n\n    # explicitly garbage collect \"loaded_oe\"\n    # as these may have large memory footprint and may not be cleared fast enough\n    del loaded_oe\n    gc.collect()\n\n    return probe_data\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_neuropixels_channel2electrode_map", "title": "<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>", "text": "<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        probe_dataset = get_openephys_probe_data(ephys_recording_key)\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_indices\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.generate_electrode_config", "title": "<code>generate_electrode_config(probe_type, electrode_keys)</code>", "text": "<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid({k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_recording_channels_details", "title": "<code>get_recording_channels_details(ephys_recording_key)</code>", "text": "<p>Get details of recording channels for a given recording.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_recording_channels_details(ephys_recording_key: dict) -&gt; np.array:\n\"\"\"Get details of recording channels for a given recording.\"\"\"\n    channels_details = {}\n\n    acq_software, sample_rate = (EphysRecording &amp; ephys_recording_key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    probe_type = (ProbeInsertion * probe.Probe &amp; ephys_recording_key).fetch1(\n        \"probe_type\"\n    )\n    channels_details[\"probe_type\"] = {\n        \"neuropixels 1.0 - 3A\": \"3A\",\n        \"neuropixels 1.0 - 3B\": \"NP1\",\n        \"neuropixels UHD\": \"NP1100\",\n        \"neuropixels 2.0 - SS\": \"NP21\",\n        \"neuropixels 2.0 - MS\": \"NP24\",\n    }[probe_type]\n\n    electrode_config_key = (\n        probe.ElectrodeConfig * EphysRecording &amp; ephys_recording_key\n    ).fetch1(\"KEY\")\n    (\n        channels_details[\"channel_ind\"],\n        channels_details[\"x_coords\"],\n        channels_details[\"y_coords\"],\n        channels_details[\"shank_ind\"],\n    ) = (\n        probe.ElectrodeConfig.Electrode * probe.ProbeType.Electrode\n        &amp; electrode_config_key\n    ).fetch(\n        \"electrode\", \"x_coord\", \"y_coord\", \"shank\"\n    )\n    channels_details[\"sample_rate\"] = sample_rate\n    channels_details[\"num_channels\"] = len(channels_details[\"channel_ind\"])\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        channels_details[\"uVPerBit\"] = spikeglx_recording.get_channel_bit_volts(\"ap\")[0]\n        channels_details[\"connected\"] = np.array(\n            [v for *_, v in spikeglx_recording.apmeta.shankmap[\"data\"]]\n        )\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(ephys_recording_key)\n        channels_details[\"uVPerBit\"] = oe_probe.ap_meta[\"channels_gains\"][0]\n        channels_details[\"connected\"] = np.array(\n            [\n                int(v == 1)\n                for c, v in oe_probe.channels_connected.items()\n                if c in channels_details[\"channel_ind\"]\n            ]\n        )\n\n    return channels_details\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/", "title": "ephys_chronic.py", "text": ""}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.activate", "title": "<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe schema.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe schema.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n        get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    root_directories = _linking_module.get_ephys_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string for the full path to the root directory for processed data.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_processed_root_data_dir() -&gt; str:\n\"\"\"Retrieve the root directory for all processed data.\n\n    Returns:\n        A string for the full path to the root directory for processed data.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_ephys_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.AcquisitionSoftware", "title": "<code>AcquisitionSoftware</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Software used for recording of neuropixels probes\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ProbeInsertion", "title": "<code>ProbeInsertion</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion chronically implanted into an animal.\n    -&gt; Subject\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    insertion_datetime=null: datetime\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.InsertionLocation", "title": "<code>InsertionLocation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording", "title": "<code>EphysRecording</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; Session\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found\"\n                f\" in {session_dir}\"\n            )\n\n        supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    f\"No SpikeGLX data found for probe insertion: {key}\"\n                    + \" The probe serial number does not match.\"\n                )\n\n            if spikeglx_meta.probe_model in supported_probe_types:\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(key)\n                )\n\n            if not probe_data.ap_meta:\n                raise IOError(\n                    'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n                )\n\n            if probe_data.probe_model in supported_probe_types:\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n            # explicitly garbage collect \"dataset\"\n            # as these may have large memory footprint and may not be cleared fast enough\n            del probe_data, dataset\n            gc.collect()\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording.EphysFile", "title": "<code>EphysFile</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording.make", "title": "<code>make(key)</code>", "text": "<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found\"\n            f\" in {session_dir}\"\n        )\n\n    supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                f\"No SpikeGLX data found for probe insertion: {key}\"\n                + \" The probe serial number does not match.\"\n            )\n\n        if spikeglx_meta.probe_model in supported_probe_types:\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(key)\n            )\n\n        if not probe_data.ap_meta:\n            raise IOError(\n                'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n            )\n\n        if probe_data.probe_model in supported_probe_types:\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n        # explicitly garbage collect \"dataset\"\n        # as these may have large memory footprint and may not be cleared fast enough\n        del probe_data, dataset\n        gc.collect()\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP", "title": "<code>LFP</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; EphysRecording\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            oe_probe = get_openephys_probe_data(key)\n\n            lfp_channel_ind = np.r_[\n                len(oe_probe.lfp_meta[\"channels_indices\"])\n                - 1 : 0 : -self._skip_channel_counts\n            ]\n\n            # (sample x channel)\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n            lfp = (\n                lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_keys.extend(\n                probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n            )\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP.make", "title": "<code>make(key)</code>", "text": "<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(key)\n\n        lfp_channel_ind = np.r_[\n            len(oe_probe.lfp_meta[\"channels_indices\"])\n            - 1 : 0 : -self._skip_channel_counts\n        ]\n\n        # (sample x channel)\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n        lfp = (\n            lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        electrode_keys.extend(\n            probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n        )\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringMethod", "title": "<code>ClusteringMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n        (\"kilosort2.5\", \"kilosort2.5 clustering method\"),\n        (\"kilosort3\", \"kilosort3 clustering method\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringParamSet", "title": "<code>ClusteringParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameters for clustering with Kilosort.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob): Parameters for clustering with Kilosort.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        clustering_method: str,\n        paramset_desc: str,\n        params: dict,\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"clustering_method\": clustering_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(\n                {**params, \"clustering_method\": clustering_method}\n            ),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringParamSet.insert_new_params", "title": "<code>insert_new_params(clustering_method, paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>", "text": "<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    clustering_method: str,\n    paramset_desc: str,\n    params: dict,\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"clustering_method\": clustering_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(\n            {**params, \"clustering_method\": clustering_method}\n        ),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusterQualityLabel", "title": "<code>ClusterQualityLabel</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code>varchar(4000)</code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description (varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)  # cluster quality type - e.g. 'good', 'MUA', 'noise', etc.\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask", "title": "<code>ClusteringTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_outdir_dir</code> <code>varchar(255)</code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_outdir_dir (varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir='': varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False) -&gt; pathlib.Path:\n\"\"\"Infer output directory if it is not provided.\n\n        Args:\n            key (dict): ClusteringTask primary key.\n\n        Returns:\n            Expected clustering_output_dir based on the following convention:\n                processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n                e.g.: sub4/sess1/probe_2/kilosort2_0\n        \"\"\"\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        sess_dir = find_full_path(get_ephys_root_data_dir(), get_session_directory(key))\n        root_dir = find_root_directory(get_ephys_root_data_dir(), sess_dir)\n\n        method = (\n            (ClusteringParamSet * ClusteringMethod &amp; key)\n            .fetch1(\"clustering_method\")\n            .replace(\".\", \"-\")\n        )\n\n        output_dir = (\n            processed_dir\n            / sess_dir.relative_to(root_dir)\n            / f'probe_{key[\"insertion_number\"]}'\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            log.info(f\"{output_dir} created!\")\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n        Args:\n            ephys_recording_key (dict): EphysRecording primary key.\n            paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n        \"\"\"\n        key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n        try:\n            kilosort.Kilosort(\n                output_dir\n            )  # check if the directory is a valid Kilosort output\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"clustering_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer output directory if it is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>ClusteringTask primary key.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False) -&gt; pathlib.Path:\n\"\"\"Infer output directory if it is not provided.\n\n    Args:\n        key (dict): ClusteringTask primary key.\n\n    Returns:\n        Expected clustering_output_dir based on the following convention:\n            processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n            e.g.: sub4/sess1/probe_2/kilosort2_0\n    \"\"\"\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    sess_dir = find_full_path(get_ephys_root_data_dir(), get_session_directory(key))\n    root_dir = find_root_directory(get_ephys_root_data_dir(), sess_dir)\n\n    method = (\n        (ClusteringParamSet * ClusteringMethod &amp; key)\n        .fetch1(\"clustering_method\")\n        .replace(\".\", \"-\")\n    )\n\n    output_dir = (\n        processed_dir\n        / sess_dir.relative_to(root_dir)\n        / f'probe_{key[\"insertion_number\"]}'\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        log.info(f\"{output_dir} created!\")\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask.auto_generate_entries", "title": "<code>auto_generate_entries(ephys_recording_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Autogenerate entries based on a particular ephys recording.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_recording_key</code> <code>dict</code> <p>EphysRecording primary key.</p> required <code>paramset_idx</code> <code>int</code> <p>Parameter index to use for clustering task. Defaults to 0.</p> <code>0</code> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n    Args:\n        ephys_recording_key (dict): EphysRecording primary key.\n        paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n    \"\"\"\n    key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n    try:\n        kilosort.Kilosort(\n            output_dir\n        )  # check if the directory is a valid Kilosort output\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"clustering_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering", "title": "<code>Clustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version (varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update clustering_output_dir\n            ClusteringTask.update1(\n                {**key, \"clustering_output_dir\": output_dir.as_posix()}\n            )\n\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n        elif task_mode == \"trigger\":\n            acq_software, clustering_method, params = (\n                ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n            ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n            if \"kilosort\" in clustering_method:\n                from element_array_ephys.readers import kilosort_triggering\n\n                # add additional probe-recording and channels details into `params`\n                params = {**params, **get_recording_channels_details(key)}\n                params[\"fs\"] = params[\"sample_rate\"]\n\n                if acq_software == \"SpikeGLX\":\n                    spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                    spikeglx_recording = spikeglx.SpikeGLX(\n                        spikeglx_meta_filepath.parent\n                    )\n                    spikeglx_recording.validate_file(\"ap\")\n                    run_CatGT = (\n                        params.pop(\"run_CatGT\", True)\n                        and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                    )\n\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=spikeglx_recording.root_dir\n                            / (spikeglx_recording.root_name + \".ap.bin\"),\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                            npx_input_dir=spikeglx_meta_filepath.parent,\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                            run_CatGT=run_CatGT,\n                        )\n                        run_kilosort.run_modules()\n                elif acq_software == \"Open Ephys\":\n                    oe_probe = get_openephys_probe_data(key)\n\n                    assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                    # run kilosort\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=pathlib.Path(\n                                oe_probe.recording_info[\"recording_files\"][0]\n                            )\n                            / \"continuous.dat\",\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                            npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        )\n                        run_kilosort.run_modules()\n            else:\n                raise NotImplementedError(\n                    f\"Automatic triggering of {clustering_method}\"\n                    f\" clustering analysis is not yet supported\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering.make", "title": "<code>make(key)</code>", "text": "<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update clustering_output_dir\n        ClusteringTask.update1(\n            {**key, \"clustering_output_dir\": output_dir.as_posix()}\n        )\n\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n    elif task_mode == \"trigger\":\n        acq_software, clustering_method, params = (\n            ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n        ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n        if \"kilosort\" in clustering_method:\n            from element_array_ephys.readers import kilosort_triggering\n\n            # add additional probe-recording and channels details into `params`\n            params = {**params, **get_recording_channels_details(key)}\n            params[\"fs\"] = params[\"sample_rate\"]\n\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                spikeglx_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent\n                )\n                spikeglx_recording.validate_file(\"ap\")\n                run_CatGT = (\n                    params.pop(\"run_CatGT\", True)\n                    and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                )\n\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=spikeglx_recording.root_dir\n                        / (spikeglx_recording.root_name + \".ap.bin\"),\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                        npx_input_dir=spikeglx_meta_filepath.parent,\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        run_CatGT=run_CatGT,\n                    )\n                    run_kilosort.run_modules()\n            elif acq_software == \"Open Ephys\":\n                oe_probe = get_openephys_probe_data(key)\n\n                assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                # run kilosort\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=pathlib.Path(\n                            oe_probe.recording_info[\"recording_files\"][0]\n                        )\n                        / \"continuous.dat\",\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                        npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                    )\n                    run_kilosort.run_modules()\n        else:\n            raise NotImplementedError(\n                f\"Automatic triggering of {clustering_method}\"\n                f\" clustering analysis is not yet supported\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curation procedure table.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> <code>curation_id</code> <code>foreign key, int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time when curation results are generated.</p> <code>curation_output_dir</code> <code>varchar(255)</code> <p>Output directory of the curated results.</p> <code>quality_control</code> <code>bool</code> <p>If True, this clustering result has undergone quality control.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this clustering result.</p> <code>curation_note</code> <code>varchar(2000)</code> <p>Notes about the curation task.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curation procedure table.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n        curation_id (foreign key, int): Unique curation ID.\n        curation_time (datetime): Time when curation results are generated.\n        curation_output_dir (varchar(255) ): Output directory of the curated results.\n        quality_control (bool): If True, this clustering result has undergone quality control.\n        manual_curation (bool): If True, manual curation has been performed on this clustering result.\n        curation_note (varchar(2000) ): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual curation procedure\n    -&gt; Clustering\n    curation_id: int\n    ---\n    curation_time: datetime             # time of generation of this set of curated clustering results\n    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n    quality_control: bool               # has this clustering result undergone quality control?\n    manual_curation: bool               # has manual curation been performed on this clustering result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n        A function to create a new corresponding \"Curation\" for a particular\n        \"ClusteringTask\"\n        \"\"\"\n        if key not in Clustering():\n            raise ValueError(\n                f\"No corresponding entry in Clustering available\"\n                f\" for: {key}; do `Clustering.populate(key)`\"\n            )\n\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n            kilosort_dir\n        )\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": creation_time,\n                \"curation_output_dir\": output_dir,\n                \"quality_control\": is_qc,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation.create1_from_clustering_task", "title": "<code>create1_from_clustering_task(key, curation_note='')</code>", "text": "<p>A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\"</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n    A function to create a new corresponding \"Curation\" for a particular\n    \"ClusteringTask\"\n    \"\"\"\n    if key not in Clustering():\n        raise ValueError(\n            f\"No corresponding entry in Clustering available\"\n            f\" for: {key}; do `Clustering.populate(key)`\"\n        )\n\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n        kilosort_dir\n    )\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": creation_time,\n            \"curation_output_dir\": output_dir,\n            \"quality_control\": is_qc,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering", "title": "<code>CuratedClustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Curation primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Curation (foreign key): Curation primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of a curation.\n    -&gt; Curation\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n            \"acq_software\", \"sampling_rate\"\n        )\n\n        sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / sample_rate\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ]\n                        if spike_depths is not None\n                        else None,\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering.Unit", "title": "<code>Unit</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering.make", "title": "<code>make(key)</code>", "text": "<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / sample_rate\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ]\n                    if spike_depths is not None\n                    else None,\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet", "title": "<code>WaveformSet</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if is_qc:\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            if unit_peak_waveform:\n                self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n            if unit_electrode_waveforms:\n                self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.PeakWaveform", "title": "<code>PeakWaveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.make", "title": "<code>make(key)</code>", "text": "<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if is_qc:\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        if unit_peak_waveform:\n            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n        if unit_electrode_waveforms:\n            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics", "title": "<code>QualityMetrics</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n        rename_dict = {\n            \"isi_viol\": \"isi_violation\",\n            \"num_viol\": \"number_violation\",\n            \"contam_rate\": \"contamination_rate\",\n        }\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        metrics_df.columns = metrics_df.columns.str.lower()\n        metrics_df.rename(columns=rename_dict, inplace=True)\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.Cluster", "title": "<code>Cluster</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> <code>velocity_below</code> <code>float</code> <p>inverse velocity of waveform propagation from soma toward the bottom of the probe.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n    rename_dict = {\n        \"isi_viol\": \"isi_violation\",\n        \"num_viol\": \"number_violation\",\n        \"contam_rate\": \"contamination_rate\",\n    }\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    metrics_df.columns = metrics_df.columns.str.lower()\n    metrics_df.rename(columns=rename_dict, inplace=True)\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_spikeglx_meta_filepath", "title": "<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>", "text": "<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = pathlib.Path(\n        (\n            EphysRecording.EphysFile\n            &amp; ephys_recording_key\n            &amp; 'file_path LIKE \"%.ap.meta\"'\n        ).fetch1(\"file_path\")\n    )\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_openephys_probe_data", "title": "<code>get_openephys_probe_data(ephys_recording_key)</code>", "text": "<p>Get OpenEphys probe data from file.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_openephys_probe_data(ephys_recording_key: dict) -&gt; list:\n\"\"\"Get OpenEphys probe data from file.\"\"\"\n    inserted_probe_serial_number = (\n        ProbeInsertion * probe.Probe &amp; ephys_recording_key\n    ).fetch1(\"probe\")\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n    )\n    loaded_oe = openephys.OpenEphys(session_dir)\n    probe_data = loaded_oe.probes[inserted_probe_serial_number]\n\n    # explicitly garbage collect \"loaded_oe\"\n    # as these may have large memory footprint and may not be cleared fast enough\n    del loaded_oe\n    gc.collect()\n\n    return probe_data\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_neuropixels_channel2electrode_map", "title": "<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>", "text": "<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        probe_dataset = get_openephys_probe_data(ephys_recording_key)\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_indices\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.generate_electrode_config", "title": "<code>generate_electrode_config(probe_type, electrode_keys)</code>", "text": "<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid({k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_recording_channels_details", "title": "<code>get_recording_channels_details(ephys_recording_key)</code>", "text": "<p>Get details of recording channels for a given recording.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_recording_channels_details(ephys_recording_key: dict) -&gt; np.array:\n\"\"\"Get details of recording channels for a given recording.\"\"\"\n    channels_details = {}\n\n    acq_software, sample_rate = (EphysRecording &amp; ephys_recording_key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    probe_type = (ProbeInsertion * probe.Probe &amp; ephys_recording_key).fetch1(\n        \"probe_type\"\n    )\n    channels_details[\"probe_type\"] = {\n        \"neuropixels 1.0 - 3A\": \"3A\",\n        \"neuropixels 1.0 - 3B\": \"NP1\",\n        \"neuropixels UHD\": \"NP1100\",\n        \"neuropixels 2.0 - SS\": \"NP21\",\n        \"neuropixels 2.0 - MS\": \"NP24\",\n    }[probe_type]\n\n    electrode_config_key = (\n        probe.ElectrodeConfig * EphysRecording &amp; ephys_recording_key\n    ).fetch1(\"KEY\")\n    (\n        channels_details[\"channel_ind\"],\n        channels_details[\"x_coords\"],\n        channels_details[\"y_coords\"],\n        channels_details[\"shank_ind\"],\n    ) = (\n        probe.ElectrodeConfig.Electrode * probe.ProbeType.Electrode\n        &amp; electrode_config_key\n    ).fetch(\n        \"electrode\", \"x_coord\", \"y_coord\", \"shank\"\n    )\n    channels_details[\"sample_rate\"] = sample_rate\n    channels_details[\"num_channels\"] = len(channels_details[\"channel_ind\"])\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        channels_details[\"uVPerBit\"] = spikeglx_recording.get_channel_bit_volts(\"ap\")[0]\n        channels_details[\"connected\"] = np.array(\n            [v for *_, v in spikeglx_recording.apmeta.shankmap[\"data\"]]\n        )\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(ephys_recording_key)\n        channels_details[\"uVPerBit\"] = oe_probe.ap_meta[\"channels_gains\"][0]\n        channels_details[\"connected\"] = np.array(\n            [\n                int(v == 1)\n                for c, v in oe_probe.channels_connected.items()\n                if c in channels_details[\"channel_ind\"]\n            ]\n        )\n\n    return channels_details\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/", "title": "ephys_no_curation.py", "text": ""}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.activate", "title": "<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe schema.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe schema.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n        get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.\n\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    root_directories = _linking_module.get_ephys_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Retrieve the root directory for all processed data.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string for the full path to the root directory for processed data.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_processed_root_data_dir() -&gt; str:\n\"\"\"Retrieve the root directory for all processed data.\n\n    Returns:\n        A string for the full path to the root directory for processed data.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_ephys_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.AcquisitionSoftware", "title": "<code>AcquisitionSoftware</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ProbeInsertion", "title": "<code>ProbeInsertion</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion implanted into an animal for a given session.\n    -&gt; Session\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    \"\"\"\n\n    @classmethod\n    def auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(session_key)\n        )\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n            )\n\n        probe_list, probe_insertion_list = [], []\n        if acq_software == \"SpikeGLX\":\n            for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                probe_key = {\n                    \"probe_type\": spikeglx_meta.probe_model,\n                    \"probe\": spikeglx_meta.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n\n                probe_dir = meta_filepath.parent\n                try:\n                    probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                    probe_number = int(probe_number.replace(\"imec\", \"\"))\n                except AttributeError:\n                    probe_number = meta_fp_idx\n\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": spikeglx_meta.probe_SN,\n                        \"insertion_number\": int(probe_number),\n                    }\n                )\n        elif acq_software == \"Open Ephys\":\n            loaded_oe = openephys.OpenEphys(session_dir)\n            for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n                probe_key = {\n                    \"probe_type\": oe_probe.probe_model,\n                    \"probe\": oe_probe.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": oe_probe.probe_SN,\n                        \"insertion_number\": probe_idx,\n                    }\n                )\n        else:\n            raise NotImplementedError(f\"Unknown acquisition software: {acq_software}\")\n\n        probe.Probe.insert(probe_list)\n        cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ProbeInsertion.auto_generate_entries", "title": "<code>auto_generate_entries(session_key)</code>  <code>classmethod</code>", "text": "<p>Automatically populate entries in ProbeInsertion table for a session.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(session_key)\n    )\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n        )\n\n    probe_list, probe_insertion_list = [], []\n    if acq_software == \"SpikeGLX\":\n        for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n            probe_key = {\n                \"probe_type\": spikeglx_meta.probe_model,\n                \"probe\": spikeglx_meta.probe_SN,\n            }\n            if (\n                probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                and probe_key not in probe.Probe()\n            ):\n                probe_list.append(probe_key)\n\n            probe_dir = meta_filepath.parent\n            try:\n                probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                probe_number = int(probe_number.replace(\"imec\", \"\"))\n            except AttributeError:\n                probe_number = meta_fp_idx\n\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": spikeglx_meta.probe_SN,\n                    \"insertion_number\": int(probe_number),\n                }\n            )\n    elif acq_software == \"Open Ephys\":\n        loaded_oe = openephys.OpenEphys(session_dir)\n        for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n            probe_key = {\n                \"probe_type\": oe_probe.probe_model,\n                \"probe\": oe_probe.probe_SN,\n            }\n            if (\n                probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                and probe_key not in probe.Probe()\n            ):\n                probe_list.append(probe_key)\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": oe_probe.probe_SN,\n                    \"insertion_number\": probe_idx,\n                }\n            )\n    else:\n        raise NotImplementedError(f\"Unknown acquisition software: {acq_software}\")\n\n    probe.Probe.insert(probe_list)\n    cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.InsertionLocation", "title": "<code>InsertionLocation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis\n\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording", "title": "<code>EphysRecording</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                \"Ephys recording data not found!\"\n                \" Neither SpikeGLX nor Open Ephys recording files found\"\n            )\n\n        supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n\n            if spikeglx_meta.probe_model in supported_probe_types:\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(key)\n                )\n\n            if not probe_data.ap_meta:\n                raise IOError(\n                    'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n                )\n\n            if probe_data.probe_model in supported_probe_types:\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n            # explicitly garbage collect \"dataset\"\n            # as these may have large memory footprint and may not be cleared fast enough\n            del probe_data, dataset\n            gc.collect()\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording.EphysFile", "title": "<code>EphysFile</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording.make", "title": "<code>make(key)</code>", "text": "<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            \"Ephys recording data not found!\"\n            \" Neither SpikeGLX nor Open Ephys recording files found\"\n        )\n\n    supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No SpikeGLX data found for probe insertion: {}\".format(key)\n            )\n\n        if spikeglx_meta.probe_model in supported_probe_types:\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(key)\n            )\n\n        if not probe_data.ap_meta:\n            raise IOError(\n                'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n            )\n\n        if probe_data.probe_model in supported_probe_types:\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n        # explicitly garbage collect \"dataset\"\n        # as these may have large memory footprint and may not be cleared fast enough\n        del probe_data, dataset\n        gc.collect()\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP", "title": "<code>LFP</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; EphysRecording\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            oe_probe = get_openephys_probe_data(key)\n\n            lfp_channel_ind = np.r_[\n                len(oe_probe.lfp_meta[\"channels_indices\"])\n                - 1 : 0 : -self._skip_channel_counts\n            ]\n\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n            lfp = (\n                lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_keys.extend(\n                probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n            )\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP.make", "title": "<code>make(key)</code>", "text": "<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(key)\n\n        lfp_channel_ind = np.r_[\n            len(oe_probe.lfp_meta[\"channels_indices\"])\n            - 1 : 0 : -self._skip_channel_counts\n        ]\n\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n        lfp = (\n            lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        electrode_keys.extend(\n            probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n        )\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringMethod", "title": "<code>ClusteringMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n        (\"kilosort2.5\", \"kilosort2.5 clustering method\"),\n        (\"kilosort3\", \"kilosort3 clustering method\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringParamSet", "title": "<code>ClusteringParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Set of clustering parameters</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob): Set of clustering parameters\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        clustering_method: str,\n        paramset_desc: str,\n        params: dict,\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"clustering_method\": clustering_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(\n                {**params, \"clustering_method\": clustering_method}\n            ),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringParamSet.insert_new_params", "title": "<code>insert_new_params(clustering_method, paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>", "text": "<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    clustering_method: str,\n    paramset_desc: str,\n    params: dict,\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"clustering_method\": clustering_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(\n            {**params, \"clustering_method\": clustering_method}\n        ),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusterQualityLabel", "title": "<code>ClusterQualityLabel</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code>varchar(4000)</code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description (varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)  # cluster quality type - e.g. 'good', 'MUA', 'noise', etc.\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask", "title": "<code>ClusteringTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_outdir_dir</code> <code>varchar(255)</code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_outdir_dir (varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir='': varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(\n        cls, key, relative: bool = False, mkdir: bool = False\n    ) -&gt; pathlib.Path:\n\"\"\"Infer output directory if it is not provided.\n\n        Args:\n            key (dict): ClusteringTask primary key.\n\n        Returns:\n            Expected clustering_output_dir based on the following convention:\n                processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n                e.g.: sub4/sess1/probe_2/kilosort2_0\n        \"\"\"\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n        root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n        method = (\n            (ClusteringParamSet * ClusteringMethod &amp; key)\n            .fetch1(\"clustering_method\")\n            .replace(\".\", \"-\")\n        )\n\n        output_dir = (\n            processed_dir\n            / session_dir.relative_to(root_dir)\n            / f'probe_{key[\"insertion_number\"]}'\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            log.info(f\"{output_dir} created!\")\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n        Args:\n            ephys_recording_key (dict): EphysRecording primary key.\n            paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n        \"\"\"\n        key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n        try:\n            kilosort.Kilosort(\n                output_dir\n            )  # check if the directory is a valid Kilosort output\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"clustering_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer output directory if it is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>ClusteringTask primary key.</p> required <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef infer_output_dir(\n    cls, key, relative: bool = False, mkdir: bool = False\n) -&gt; pathlib.Path:\n\"\"\"Infer output directory if it is not provided.\n\n    Args:\n        key (dict): ClusteringTask primary key.\n\n    Returns:\n        Expected clustering_output_dir based on the following convention:\n            processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n            e.g.: sub4/sess1/probe_2/kilosort2_0\n    \"\"\"\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n    root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n    method = (\n        (ClusteringParamSet * ClusteringMethod &amp; key)\n        .fetch1(\"clustering_method\")\n        .replace(\".\", \"-\")\n    )\n\n    output_dir = (\n        processed_dir\n        / session_dir.relative_to(root_dir)\n        / f'probe_{key[\"insertion_number\"]}'\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        log.info(f\"{output_dir} created!\")\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask.auto_generate_entries", "title": "<code>auto_generate_entries(ephys_recording_key, paramset_idx=0)</code>  <code>classmethod</code>", "text": "<p>Autogenerate entries based on a particular ephys recording.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_recording_key</code> <code>dict</code> <p>EphysRecording primary key.</p> required <code>paramset_idx</code> <code>int</code> <p>Parameter index to use for clustering task. Defaults to 0.</p> <code>0</code> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n    Args:\n        ephys_recording_key (dict): EphysRecording primary key.\n        paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n    \"\"\"\n    key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n    try:\n        kilosort.Kilosort(\n            output_dir\n        )  # check if the directory is a valid Kilosort output\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"clustering_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.Clustering", "title": "<code>Clustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version (varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update clustering_output_dir\n            ClusteringTask.update1(\n                {**key, \"clustering_output_dir\": output_dir.as_posix()}\n            )\n\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n        elif task_mode == \"trigger\":\n            acq_software, clustering_method, params = (\n                ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n            ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n            if \"kilosort\" in clustering_method:\n                from element_array_ephys.readers import kilosort_triggering\n\n                # add additional probe-recording and channels details into `params`\n                params = {**params, **get_recording_channels_details(key)}\n                params[\"fs\"] = params[\"sample_rate\"]\n\n                if acq_software == \"SpikeGLX\":\n                    spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                    spikeglx_recording = spikeglx.SpikeGLX(\n                        spikeglx_meta_filepath.parent\n                    )\n                    spikeglx_recording.validate_file(\"ap\")\n                    run_CatGT = (\n                        params.pop(\"run_CatGT\", True)\n                        and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                    )\n\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=spikeglx_recording.root_dir\n                            / (spikeglx_recording.root_name + \".ap.bin\"),\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                            npx_input_dir=spikeglx_meta_filepath.parent,\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                            run_CatGT=run_CatGT,\n                        )\n                        run_kilosort.run_modules()\n                elif acq_software == \"Open Ephys\":\n                    oe_probe = get_openephys_probe_data(key)\n\n                    assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                    # run kilosort\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=pathlib.Path(\n                                oe_probe.recording_info[\"recording_files\"][0]\n                            )\n                            / \"continuous.dat\",\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                            npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        )\n                        run_kilosort.run_modules()\n            else:\n                raise NotImplementedError(\n                    f\"Automatic triggering of {clustering_method}\"\n                    f\" clustering analysis is not yet supported\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.Clustering.make", "title": "<code>make(key)</code>", "text": "<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update clustering_output_dir\n        ClusteringTask.update1(\n            {**key, \"clustering_output_dir\": output_dir.as_posix()}\n        )\n\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n    elif task_mode == \"trigger\":\n        acq_software, clustering_method, params = (\n            ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n        ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n        if \"kilosort\" in clustering_method:\n            from element_array_ephys.readers import kilosort_triggering\n\n            # add additional probe-recording and channels details into `params`\n            params = {**params, **get_recording_channels_details(key)}\n            params[\"fs\"] = params[\"sample_rate\"]\n\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                spikeglx_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent\n                )\n                spikeglx_recording.validate_file(\"ap\")\n                run_CatGT = (\n                    params.pop(\"run_CatGT\", True)\n                    and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                )\n\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=spikeglx_recording.root_dir\n                        / (spikeglx_recording.root_name + \".ap.bin\"),\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                        npx_input_dir=spikeglx_meta_filepath.parent,\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        run_CatGT=run_CatGT,\n                    )\n                    run_kilosort.run_modules()\n            elif acq_software == \"Open Ephys\":\n                oe_probe = get_openephys_probe_data(key)\n\n                assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                # run kilosort\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=pathlib.Path(\n                            oe_probe.recording_info[\"recording_files\"][0]\n                        )\n                        / \"continuous.dat\",\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                        npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                    )\n                    run_kilosort.run_modules()\n        else:\n            raise NotImplementedError(\n                f\"Automatic triggering of {clustering_method}\"\n                f\" clustering analysis is not yet supported\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering", "title": "<code>CuratedClustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of the spike sorting step.\n    -&gt; Clustering\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (foreign key): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n            \"acq_software\", \"sampling_rate\"\n        )\n\n        sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / sample_rate\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering.Unit", "title": "<code>Unit</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>foreign key</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (foreign key): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering.make", "title": "<code>make(key)</code>", "text": "<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / sample_rate\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet", "title": "<code>WaveformSet</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if (kilosort_dir / \"mean_waveforms.npy\").exists():\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            if unit_peak_waveform:\n                self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n            if unit_electrode_waveforms:\n                self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.PeakWaveform", "title": "<code>PeakWaveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.make", "title": "<code>make(key)</code>", "text": "<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if (kilosort_dir / \"mean_waveforms.npy\").exists():\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        if unit_peak_waveform:\n            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n        if unit_electrode_waveforms:\n            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics", "title": "<code>QualityMetrics</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n        rename_dict = {\n            \"isi_viol\": \"isi_violation\",\n            \"num_viol\": \"number_violation\",\n            \"contam_rate\": \"contamination_rate\",\n        }\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        metrics_df.columns = metrics_df.columns.str.lower()\n        metrics_df.rename(columns=rename_dict, inplace=True)\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.Cluster", "title": "<code>Cluster</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> <code>velocity_below</code> <code>float</code> <p>inverse velocity of waveform propagation from soma toward the bottom of the probe.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n    rename_dict = {\n        \"isi_viol\": \"isi_violation\",\n        \"num_viol\": \"number_violation\",\n        \"contam_rate\": \"contamination_rate\",\n    }\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    metrics_df.columns = metrics_df.columns.str.lower()\n    metrics_df.rename(columns=rename_dict, inplace=True)\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_spikeglx_meta_filepath", "title": "<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>", "text": "<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = pathlib.Path(\n        (\n            EphysRecording.EphysFile\n            &amp; ephys_recording_key\n            &amp; 'file_path LIKE \"%.ap.meta\"'\n        ).fetch1(\"file_path\")\n    )\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_openephys_probe_data", "title": "<code>get_openephys_probe_data(ephys_recording_key)</code>", "text": "<p>Get OpenEphys probe data from file.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_openephys_probe_data(ephys_recording_key: dict) -&gt; list:\n\"\"\"Get OpenEphys probe data from file.\"\"\"\n    inserted_probe_serial_number = (\n        ProbeInsertion * probe.Probe &amp; ephys_recording_key\n    ).fetch1(\"probe\")\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n    )\n    loaded_oe = openephys.OpenEphys(session_dir)\n    probe_data = loaded_oe.probes[inserted_probe_serial_number]\n\n    # explicitly garbage collect \"loaded_oe\"\n    # as these may have large memory footprint and may not be cleared fast enough\n    del loaded_oe\n    gc.collect()\n\n    return probe_data\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_neuropixels_channel2electrode_map", "title": "<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>", "text": "<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        probe_dataset = get_openephys_probe_data(ephys_recording_key)\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_indices\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.generate_electrode_config", "title": "<code>generate_electrode_config(probe_type, electrode_keys)</code>", "text": "<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid({k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_recording_channels_details", "title": "<code>get_recording_channels_details(ephys_recording_key)</code>", "text": "<p>Get details of recording channels for a given recording.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_recording_channels_details(ephys_recording_key: dict) -&gt; np.array:\n\"\"\"Get details of recording channels for a given recording.\"\"\"\n    channels_details = {}\n\n    acq_software, sample_rate = (EphysRecording &amp; ephys_recording_key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    probe_type = (ProbeInsertion * probe.Probe &amp; ephys_recording_key).fetch1(\n        \"probe_type\"\n    )\n    channels_details[\"probe_type\"] = {\n        \"neuropixels 1.0 - 3A\": \"3A\",\n        \"neuropixels 1.0 - 3B\": \"NP1\",\n        \"neuropixels UHD\": \"NP1100\",\n        \"neuropixels 2.0 - SS\": \"NP21\",\n        \"neuropixels 2.0 - MS\": \"NP24\",\n    }[probe_type]\n\n    electrode_config_key = (\n        probe.ElectrodeConfig * EphysRecording &amp; ephys_recording_key\n    ).fetch1(\"KEY\")\n    (\n        channels_details[\"channel_ind\"],\n        channels_details[\"x_coords\"],\n        channels_details[\"y_coords\"],\n        channels_details[\"shank_ind\"],\n    ) = (\n        probe.ElectrodeConfig.Electrode * probe.ProbeType.Electrode\n        &amp; electrode_config_key\n    ).fetch(\n        \"electrode\", \"x_coord\", \"y_coord\", \"shank\"\n    )\n    channels_details[\"sample_rate\"] = sample_rate\n    channels_details[\"num_channels\"] = len(channels_details[\"channel_ind\"])\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        channels_details[\"uVPerBit\"] = spikeglx_recording.get_channel_bit_volts(\"ap\")[0]\n        channels_details[\"connected\"] = np.array(\n            [v for *_, v in spikeglx_recording.apmeta.shankmap[\"data\"]]\n        )\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(ephys_recording_key)\n        channels_details[\"uVPerBit\"] = oe_probe.ap_meta[\"channels_gains\"][0]\n        channels_details[\"connected\"] = np.array(\n            [\n                int(v == 1)\n                for c, v in oe_probe.channels_connected.items()\n                if c in channels_details[\"channel_ind\"]\n            ]\n        )\n\n    return channels_details\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/", "title": "ephys_precluster.py", "text": ""}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.activate", "title": "<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe schema.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe schema.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    return _linking_module.get_ephys_root_data_dir()\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.AcquisitionSoftware", "title": "<code>AcquisitionSoftware</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ProbeInsertion", "title": "<code>ProbeInsertion</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion implanted into an animal for a given session.\n    -&gt; Session\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.InsertionLocation", "title": "<code>InsertionLocation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis\n\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording", "title": "<code>EphysRecording</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found\"\n                f\" in {session_dir}\"\n            )\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n\n            if re.search(\"(1.0|2.0)\", spikeglx_meta.probe_model):\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(key)\n                )\n\n            if re.search(\"(1.0|2.0)\", probe_data.probe_model):\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_ids\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording.EphysFile", "title": "<code>EphysFile</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording.make", "title": "<code>make(key)</code>", "text": "<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found\"\n            f\" in {session_dir}\"\n        )\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No SpikeGLX data found for probe insertion: {}\".format(key)\n            )\n\n        if re.search(\"(1.0|2.0)\", spikeglx_meta.probe_model):\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(key)\n            )\n\n        if re.search(\"(1.0|2.0)\", probe_data.probe_model):\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_ids\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterMethod", "title": "<code>PreClusterMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Pre-clustering method</p> <p>Attributes:</p> Name Type Description <code>precluster_method</code> <code>foreign key, varchar(16) </code> <p>Pre-clustering method for the dataset.</p> <code>precluster_method_desc(varchar(1000)</code> <p>Pre-clustering method description.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterMethod(dj.Lookup):\n\"\"\"Pre-clustering method\n\n    Attributes:\n        precluster_method (foreign key, varchar(16) ): Pre-clustering method for the dataset.\n        precluster_method_desc(varchar(1000) ): Pre-clustering method description.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for pre-clustering\n    precluster_method: varchar(16)\n    ---\n    precluster_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [(\"catgt\", \"Time shift, Common average referencing, Zeroing\")]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSet", "title": "<code>PreClusterParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters for the pre-clustering method.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique parameter set ID.</p> <code>PreClusterMethod</code> <code>dict</code> <p>PreClusterMethod query for this dataset.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description for the pre-clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>Unique hash for parameter set.</p> <code>params</code> <code>longblob</code> <p>All parameters for the pre-clustering method.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterParamSet(dj.Lookup):\n\"\"\"Parameters for the pre-clustering method.\n\n    Attributes:\n        paramset_idx (foreign key): Unique parameter set ID.\n        PreClusterMethod (dict): PreClusterMethod query for this dataset.\n        paramset_desc (varchar(128) ): Description for the pre-clustering parameter set.\n        param_set_hash (uuid): Unique hash for parameter set.\n        params (longblob): All parameters for the pre-clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; PreClusterMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls, precluster_method: str, paramset_idx: int, paramset_desc: str, params: dict\n    ):\n        param_dict = {\n            \"precluster_method\": precluster_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set\"\n                    \" already exists - paramset_idx: {}\".format(existing_paramset_idx)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSteps", "title": "<code>PreClusterParamSteps</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Ordered list of parameter sets that will be run.</p> <p>Attributes:</p> Name Type Description <code>precluster_param_steps_id</code> <code>foreign key</code> <p>Unique ID for the pre-clustering parameter sets to be run.</p> <code>precluster_param_steps_name</code> <code>varchar(32)</code> <p>User-friendly name for the parameter steps.</p> <code>precluster_param_steps_desc</code> <code>varchar(128)</code> <p>Description of the parameter steps.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterParamSteps(dj.Manual):\n\"\"\"Ordered list of parameter sets that will be run.\n\n    Attributes:\n        precluster_param_steps_id (foreign key): Unique ID for the pre-clustering parameter sets to be run.\n        precluster_param_steps_name (varchar(32) ): User-friendly name for the parameter steps.\n        precluster_param_steps_desc (varchar(128) ): Description of the parameter steps.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ordered list of paramset_idx that are to be run\n    # When pre-clustering is not performed, do not create an entry in `Step` Part table\n    precluster_param_steps_id: smallint\n    ---\n    precluster_param_steps_name: varchar(32)\n    precluster_param_steps_desc: varchar(128)\n    \"\"\"\n\n    class Step(dj.Part):\n\"\"\"Define the order of operations for parameter sets.\n\n        Attributes:\n            PreClusterParamSteps (foreign key): PreClusterParamSteps primary key.\n            step_number (foreign key, smallint): Order of operations.\n            PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        step_number: smallint                  # Order of operations\n        ---\n        -&gt; PreClusterParamSet\n        \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSteps.Step", "title": "<code>Step</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Define the order of operations for parameter sets.</p> <p>Attributes:</p> Name Type Description <code>PreClusterParamSteps</code> <code>foreign key</code> <p>PreClusterParamSteps primary key.</p> <code>step_number</code> <code>foreign key, smallint</code> <p>Order of operations.</p> <code>PreClusterParamSet</code> <code>dict</code> <p>PreClusterParamSet to be used in pre-clustering.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Step(dj.Part):\n\"\"\"Define the order of operations for parameter sets.\n\n    Attributes:\n        PreClusterParamSteps (foreign key): PreClusterParamSteps primary key.\n        step_number (foreign key, smallint): Order of operations.\n        PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    step_number: smallint                  # Order of operations\n    ---\n    -&gt; PreClusterParamSet\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterTask", "title": "<code>PreClusterTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Defines a pre-clustering task ready to be run.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>PreclusterParamSteps</code> <code>foreign key</code> <p>PreClusterParam Steps primary key.</p> <code>precluster_output_dir</code> <code>varchar(255)</code> <p>relative path to directory for storing results of pre-clustering.</p> <code>task_mode</code> <code>enum</code> <p><code>none</code> (no pre-clustering), <code>load</code> results from file, or <code>trigger</code> automated pre-clustering.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterTask(dj.Manual):\n\"\"\"Defines a pre-clustering task ready to be run.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        PreclusterParamSteps (foreign key): PreClusterParam Steps primary key.\n        precluster_output_dir (varchar(255) ): relative path to directory for storing results of pre-clustering.\n        task_mode (enum ): `none` (no pre-clustering), `load` results from file, or `trigger` automated pre-clustering.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; PreClusterParamSteps\n    ---\n    precluster_output_dir='': varchar(255)  #  pre-clustering output directory relative to the root data directory\n    task_mode='none': enum('none','load', 'trigger') # 'none': no pre-clustering analysis\n                                                     # 'load': load analysis results\n                                                     # 'trigger': trigger computation\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster", "title": "<code>PreCluster</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each PreClusterTask:</p> <p>Attributes:</p> Name Type Description <code>PreClusterTask</code> <code>foreign key</code> <p>PreClusterTask primary key.</p> <code>precluster_time</code> <code>datetime</code> <p>Time of generation of this set of pre-clustering results.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for performing pre-clustering.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreCluster(dj.Imported):\n\"\"\"\n    A processing table to handle each PreClusterTask:\n\n    Attributes:\n        PreClusterTask (foreign key): PreClusterTask primary key.\n        precluster_time (datetime): Time of generation of this set of pre-clustering results.\n        package_version (varchar(16) ): Package version used for performing pre-clustering.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PreClusterTask\n    ---\n    precluster_time: datetime  # time of generation of this set of pre-clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Populate pre-clustering tables.\"\"\"\n        task_mode, output_dir = (PreClusterTask &amp; key).fetch1(\n            \"task_mode\", \"precluster_output_dir\"\n        )\n        precluster_output_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"none\":\n            if len((PreClusterParamSteps.Step &amp; key).fetch()) &gt; 0:\n                raise ValueError(\n                    \"There are entries in the PreClusterParamSteps.Step \"\n                    \"table and task_mode=none\"\n                )\n            creation_time = (EphysRecording &amp; key).fetch1(\"recording_datetime\")\n        elif task_mode == \"load\":\n            acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n            inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n                \"probe\"\n            )\n\n            if acq_software == \"SpikeGLX\":\n                for meta_filepath in precluster_output_dir.rglob(\"*.ap.meta\"):\n                    spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                    if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                        creation_time = spikeglx_meta.recording_time\n                        break\n                else:\n                    raise FileNotFoundError(\n                        \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                    )\n            else:\n                raise NotImplementedError(\n                    f\"Pre-clustering analysis of {acq_software}\" \"is not yet supported.\"\n                )\n        elif task_mode == \"trigger\":\n            raise NotImplementedError(\n                \"Automatic triggering of\"\n                \" pre-clustering analysis is not yet supported.\"\n            )\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1({**key, \"precluster_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster.make", "title": "<code>make(key)</code>", "text": "<p>Populate pre-clustering tables.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populate pre-clustering tables.\"\"\"\n    task_mode, output_dir = (PreClusterTask &amp; key).fetch1(\n        \"task_mode\", \"precluster_output_dir\"\n    )\n    precluster_output_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"none\":\n        if len((PreClusterParamSteps.Step &amp; key).fetch()) &gt; 0:\n            raise ValueError(\n                \"There are entries in the PreClusterParamSteps.Step \"\n                \"table and task_mode=none\"\n            )\n        creation_time = (EphysRecording &amp; key).fetch1(\"recording_datetime\")\n    elif task_mode == \"load\":\n        acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in precluster_output_dir.rglob(\"*.ap.meta\"):\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    creation_time = spikeglx_meta.recording_time\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n        else:\n            raise NotImplementedError(\n                f\"Pre-clustering analysis of {acq_software}\" \"is not yet supported.\"\n            )\n    elif task_mode == \"trigger\":\n        raise NotImplementedError(\n            \"Automatic triggering of\"\n            \" pre-clustering analysis is not yet supported.\"\n        )\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1({**key, \"precluster_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP", "title": "<code>LFP</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; PreCluster\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software, probe_sn = (EphysRecording * ProbeInsertion &amp; key).fetch1(\n            \"acq_software\", \"probe\"\n        )\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n\n            loaded_oe = openephys.OpenEphys(session_dir)\n            oe_probe = loaded_oe.probes[probe_sn]\n\n            lfp_channel_ind = np.arange(len(oe_probe.lfp_meta[\"channels_ids\"]))[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n            lfp = (\n                lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            for channel_idx in np.array(oe_probe.lfp_meta[\"channels_ids\"])[\n                lfp_channel_ind\n            ]:\n                electrode_keys.append(probe_electrodes[channel_idx])\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP.make", "title": "<code>make(key)</code>", "text": "<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software, probe_sn = (EphysRecording * ProbeInsertion &amp; key).fetch1(\n        \"acq_software\", \"probe\"\n    )\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        loaded_oe = openephys.OpenEphys(session_dir)\n        oe_probe = loaded_oe.probes[probe_sn]\n\n        lfp_channel_ind = np.arange(len(oe_probe.lfp_meta[\"channels_ids\"]))[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n        lfp = (\n            lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        for channel_idx in np.array(oe_probe.lfp_meta[\"channels_ids\"])[\n            lfp_channel_ind\n        ]:\n            electrode_keys.append(probe_electrodes[channel_idx])\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringMethod", "title": "<code>ClusteringMethod</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort\", \"kilosort clustering method\"),\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringParamSet", "title": "<code>ClusteringParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Paramset, dictionary of all applicable parameters.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob): Paramset, dictionary of all applicable parameters.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls, processing_method: str, paramset_idx: int, paramset_desc: str, params: dict\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            processing_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        param_dict = {\n            \"clustering_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set\"\n                    \" already exists - paramset_idx: {}\".format(existing_paramset_idx)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringParamSet.insert_new_params", "title": "<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>", "text": "<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>processing_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> required Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls, processing_method: str, paramset_idx: int, paramset_desc: str, params: dict\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        processing_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    param_dict = {\n        \"clustering_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set\"\n                \" already exists - paramset_idx: {}\".format(existing_paramset_idx)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusterQualityLabel", "title": "<code>ClusterQualityLabel</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code>varchar(4000)</code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description (varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringTask", "title": "<code>ClusteringTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_outdir_dir</code> <code>varchar(255)</code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_outdir_dir (varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; PreCluster\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir: varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering", "title": "<code>Clustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version (varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            _ = kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n            creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        elif task_mode == \"trigger\":\n            raise NotImplementedError(\n                \"Automatic triggering of\" \" clustering analysis is not yet supported\"\n            )\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering.make", "title": "<code>make(key)</code>", "text": "<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        _ = kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    elif task_mode == \"trigger\":\n        raise NotImplementedError(\n            \"Automatic triggering of\" \" clustering analysis is not yet supported\"\n        )\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1({**key, \"clustering_time\": creation_time, \"package_version\": \"\"})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation", "title": "<code>Curation</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Curation procedure table.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> <code>curation_id</code> <code>foreign key, int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time when curation results are generated.</p> <code>curation_output_dir</code> <code>varchar(255)</code> <p>Output directory of the curated results.</p> <code>quality_control</code> <code>bool</code> <p>If True, this clustering result has undergone quality control.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this clustering result.</p> <code>curation_note</code> <code>varchar(2000)</code> <p>Notes about the curation task.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curation procedure table.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n        curation_id (foreign key, int): Unique curation ID.\n        curation_time (datetime): Time when curation results are generated.\n        curation_output_dir (varchar(255) ): Output directory of the curated results.\n        quality_control (bool): If True, this clustering result has undergone quality control.\n        manual_curation (bool): If True, manual curation has been performed on this clustering result.\n        curation_note (varchar(2000) ): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual curation procedure\n    -&gt; Clustering\n    curation_id: int\n    ---\n    curation_time: datetime             # time of generation of this set of curated clustering results\n    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n    quality_control: bool               # has this clustering result undergone quality control?\n    manual_curation: bool               # has manual curation been performed on this clustering result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n        A function to create a new corresponding \"Curation\" for a particular\n        \"ClusteringTask\"\n        \"\"\"\n        if key not in Clustering():\n            raise ValueError(\n                f\"No corresponding entry in Clustering available\"\n                f\" for: {key}; do `Clustering.populate(key)`\"\n            )\n\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n            kilosort_dir\n        )\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": creation_time,\n                \"curation_output_dir\": output_dir,\n                \"quality_control\": is_qc,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation.create1_from_clustering_task", "title": "<code>create1_from_clustering_task(key, curation_note='')</code>", "text": "<p>A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\"</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n    A function to create a new corresponding \"Curation\" for a particular\n    \"ClusteringTask\"\n    \"\"\"\n    if key not in Clustering():\n        raise ValueError(\n            f\"No corresponding entry in Clustering available\"\n            f\" for: {key}; do `Clustering.populate(key)`\"\n        )\n\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n        kilosort_dir\n    )\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": creation_time,\n            \"curation_output_dir\": output_dir,\n            \"quality_control\": is_qc,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering", "title": "<code>CuratedClustering</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Curation primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Curation (foreign key): Curation primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of a curation.\n    -&gt; Curation\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / kilosort_dataset.data[\"params\"][\"sample_rate\"]\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ]\n                        if spike_depths is not None\n                        else None,\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering.Unit", "title": "<code>Unit</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering.make", "title": "<code>make(key)</code>", "text": "<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / kilosort_dataset.data[\"params\"][\"sample_rate\"]\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ]\n                    if spike_depths is not None\n                    else None,\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet", "title": "<code>WaveformSet</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if is_qc:\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.PeakWaveform", "title": "<code>PeakWaveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.make", "title": "<code>make(key)</code>", "text": "<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if is_qc:\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n        self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics", "title": "<code>QualityMetrics</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n        rename_dict = {\n            \"isi_viol\": \"isi_violation\",\n            \"num_viol\": \"number_violation\",\n            \"contam_rate\": \"contamination_rate\",\n        }\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        metrics_df.columns = metrics_df.columns.str.lower()\n        metrics_df.rename(columns=rename_dict, inplace=True)\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.Cluster", "title": "<code>Cluster</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.Waveform", "title": "<code>Waveform</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> <code>velocity_below</code> <code>float</code> <p>inverse velocity of waveform propagation from soma toward the bottom of the probe.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float): inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.make", "title": "<code>make(key)</code>", "text": "<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n    rename_dict = {\n        \"isi_viol\": \"isi_violation\",\n        \"num_viol\": \"number_violation\",\n        \"contam_rate\": \"contamination_rate\",\n    }\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    metrics_df.columns = metrics_df.columns.str.lower()\n    metrics_df.rename(columns=rename_dict, inplace=True)\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_spikeglx_meta_filepath", "title": "<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>", "text": "<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = (\n        EphysRecording.EphysFile &amp; ephys_recording_key &amp; 'file_path LIKE \"%.ap.meta\"'\n    ).fetch1(\"file_path\")\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_neuropixels_channel2electrode_map", "title": "<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>", "text": "<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n        )\n        openephys_dataset = openephys.OpenEphys(session_dir)\n        probe_serial_number = (ProbeInsertion &amp; ephys_recording_key).fetch1(\"probe\")\n        probe_dataset = openephys_dataset.probes[probe_serial_number]\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_ids\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.generate_electrode_config", "title": "<code>generate_electrode_config(probe_type, electrode_keys)</code>", "text": "<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid({k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_report/", "title": "ephys_report.py", "text": ""}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.activate", "title": "<code>activate(schema_name, ephys_schema_name, *, create_schema=True, create_tables=True)</code>", "text": "<p>Activate the current schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>schema name on the database server to activate the <code>ephys_report</code> schema.</p> required <code>ephys_schema_name</code> <code>str</code> <p>schema name of the activated ephys element for which     this ephys_report schema will be downstream from.</p> required <code>create_schema</code> <code>bool</code> <p>If True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True (default), create tables in the database if they do not yet exist.</p> <code>True</code> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>def activate(schema_name, ephys_schema_name, *, create_schema=True, create_tables=True):\n\"\"\"Activate the current schema.\n\n    Args:\n        schema_name (str): schema name on the database server to activate the `ephys_report` schema.\n        ephys_schema_name (str): schema name of the activated ephys element for which\n                this ephys_report schema will be downstream from.\n        create_schema (bool, optional): If True (default), create schema in the database if it does not yet exist.\n        create_tables (bool, optional): If True (default), create tables in the database if they do not yet exist.\n    \"\"\"\n\n    global ephys\n    ephys = dj.create_virtual_module(\"ephys\", ephys_schema_name)\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=ephys.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.ProbeLevelReport", "title": "<code>ProbeLevelReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table for storing probe level figures.</p> <p>Attributes:</p> Name Type Description <code>ephys.CuratedClustering</code> <code>foreign key</code> <p>ephys.CuratedClustering primary key.</p> <code>shank</code> <code>tinyint unsigned</code> <p>Shank of the probe.</p> <code>drift_map_plot</code> <code>attach</code> <p>Figure object for drift map.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass ProbeLevelReport(dj.Computed):\n\"\"\"Table for storing probe level figures.\n\n    Attributes:\n        ephys.CuratedClustering (foreign key): ephys.CuratedClustering primary key.\n        shank (tinyint unsigned): Shank of the probe.\n        drift_map_plot (attach): Figure object for drift map.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.CuratedClustering\n    shank         : tinyint unsigned\n    ---\n    drift_map_plot: attach\n    \"\"\"\n\n    def make(self, key):\n        from .plotting.probe_level import plot_driftmap\n\n        save_dir = _make_save_dir()\n\n        units = ephys.CuratedClustering.Unit &amp; key &amp; \"cluster_quality_label='good'\"\n\n        shanks = set((probe.ProbeType.Electrode &amp; units).fetch(\"shank\"))\n\n        for shank_no in shanks:\n            table = units * ephys.ProbeInsertion * probe.ProbeType.Electrode &amp; {\n                \"shank\": shank_no\n            }\n\n            spike_times, spike_depths = table.fetch(\n                \"spike_times\", \"spike_depths\", order_by=\"unit\"\n            )\n\n            # Get the figure\n            fig = plot_driftmap(spike_times, spike_depths, colormap=\"gist_heat_r\")\n            fig_prefix = (\n                \"-\".join(\n                    [\n                        v.strftime(\"%Y%m%d%H%M%S\")\n                        if isinstance(v, datetime.datetime)\n                        else str(v)\n                        for v in key.values()\n                    ]\n                )\n                + f\"-{shank_no}\"\n            )\n\n            # Save fig and insert\n            fig_dict = _save_figs(\n                figs=(fig,),\n                fig_names=(\"drift_map_plot\",),\n                save_dir=save_dir,\n                fig_prefix=fig_prefix,\n                extension=\".png\",\n            )\n\n            self.insert1({**key, **fig_dict, \"shank\": shank_no})\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.UnitLevelReport", "title": "<code>UnitLevelReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table for storing unit level figures.</p> <p>Attributes:</p> Name Type Description <code>ephys.CuratedClustering.Unit</code> <code>foreign key</code> <p>ephys.CuratedClustering.Unit primary key.</p> <code>ephys.ClusterQualityLabel</code> <code>foreign key</code> <p>ephys.ClusterQualityLabel primary key.</p> <code>waveform_plotly</code> <code>longblob</code> <p>Figure object for unit waveform.</p> <code>autocorrelogram_plotly</code> <code>longblob</code> <p>Figure object for an autocorrelogram.</p> <code>depth_waveform_plotly</code> <code>longblob</code> <p>Figure object for depth waveforms.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass UnitLevelReport(dj.Computed):\n\"\"\"Table for storing unit level figures.\n\n    Attributes:\n        ephys.CuratedClustering.Unit (foreign key): ephys.CuratedClustering.Unit primary key.\n        ephys.ClusterQualityLabel (foreign key): ephys.ClusterQualityLabel primary key.\n        waveform_plotly (longblob): Figure object for unit waveform.\n        autocorrelogram_plotly (longblob): Figure object for an autocorrelogram.\n        depth_waveform_plotly (longblob): Figure object for depth waveforms.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.CuratedClustering.Unit\n    ---\n    -&gt; ephys.ClusterQualityLabel\n    waveform_plotly                 : longblob\n    autocorrelogram_plotly          : longblob\n    depth_waveform_plotly           : longblob\n    \"\"\"\n\n    def make(self, key):\n        from .plotting.unit_level import (\n            plot_auto_correlogram,\n            plot_depth_waveforms,\n            plot_waveform,\n        )\n\n        sampling_rate = (ephys.EphysRecording &amp; key).fetch1(\n            \"sampling_rate\"\n        ) / 1e3  # in kHz\n\n        peak_electrode_waveform, spike_times, cluster_quality_label = (\n            (ephys.CuratedClustering.Unit &amp; key) * ephys.WaveformSet.PeakWaveform\n        ).fetch1(\"peak_electrode_waveform\", \"spike_times\", \"cluster_quality_label\")\n\n        # Get the figure\n        waveform_fig = plot_waveform(\n            waveform=peak_electrode_waveform, sampling_rate=sampling_rate\n        )\n\n        correlogram_fig = plot_auto_correlogram(\n            spike_times=spike_times, bin_size=0.001, window_size=1\n        )\n\n        depth_waveform_fig = plot_depth_waveforms(ephys, unit_key=key, y_range=60)\n\n        self.insert1(\n            {\n                **key,\n                \"cluster_quality_label\": cluster_quality_label,\n                \"waveform_plotly\": waveform_fig.to_plotly_json(),\n                \"autocorrelogram_plotly\": correlogram_fig.to_plotly_json(),\n                \"depth_waveform_plotly\": depth_waveform_fig.to_plotly_json(),\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.QualityMetricCutoffs", "title": "<code>QualityMetricCutoffs</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Cut-off values for unit quality metrics.</p> <p>Attributes:</p> Name Type Description <code>cutoffs_id</code> <code>smallint</code> <p>Unique ID for the cut-off values.</p> <code>amplitude_cutoff_maximum</code> <code>float</code> <p>Optional. Amplitude cut-off.</p> <code>presence_ratio_minimum</code> <code>float</code> <p>Optional. Presence ratio cut-off.</p> <code>isi_violations_maximum</code> <code>float</code> <p>Optional. ISI violation ratio cut-off.</p> <code>cutoffs_hash</code> <code>uuid</code> <p>uuid for the cut-off values.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass QualityMetricCutoffs(dj.Lookup):\n\"\"\"Cut-off values for unit quality metrics.\n\n    Attributes:\n        cutoffs_id (smallint): Unique ID for the cut-off values.\n        amplitude_cutoff_maximum (float): Optional. Amplitude cut-off.\n        presence_ratio_minimum (float): Optional. Presence ratio cut-off.\n        isi_violations_maximum (float): Optional. ISI violation ratio cut-off.\n        cutoffs_hash (uuid): uuid for the cut-off values.\n    \"\"\"\n\n    definition = \"\"\"\n    cutoffs_id                    : smallint\n    ---\n    amplitude_cutoff_maximum=null : float # Defaults to null, no cutoff applied\n    presence_ratio_minimum=null   : float # Defaults to null, no cutoff applied\n    isi_violations_maximum=null   : float # Defaults to null, no cutoff applied\n    cutoffs_hash: uuid\n    unique index (cutoffs_hash)\n    \"\"\"\n\n    contents = [\n        (0, None, None, None, UUID(\"5d835de1-e1af-1871-d81f-d12a9702ff5f\")),\n        (1, 0.1, 0.9, 0.5, UUID(\"f74ccd77-0b3a-2bf8-0bfd-ec9713b5dca8\")),\n    ]\n\n    @classmethod\n    def insert_new_cutoffs(\n        cls,\n        cutoffs_id: int = None,\n        amplitude_cutoff_maximum: float = None,\n        presence_ratio_minimum: float = None,\n        isi_violations_maximum: float = None,\n    ):\n        if cutoffs_id is None:\n            cutoffs_id = (dj.U().aggr(cls, n=\"max(cutoffs_id)\").fetch1(\"n\") or 0) + 1\n\n        param_dict = {\n            \"amplitude_cutoff_maximum\": amplitude_cutoff_maximum,\n            \"presence_ratio_minimum\": presence_ratio_minimum,\n            \"isi_violations_maximum\": isi_violations_maximum,\n        }\n        param_hash = dict_to_uuid(param_dict)\n        param_query = cls &amp; {\"cutoffs_hash\": param_hash}\n\n        if param_query:  # If the specified cutoff set already exists\n            existing_paramset_idx = param_query.fetch1(\"cutoffs_id\")\n            if (\n                existing_paramset_idx == cutoffs_id\n            ):  # If the existing set has the same id: job done\n                return\n            # If not same name: human err, adding the same set with different name\n            else:\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"cutoffs_id\": cutoffs_id} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified cuttoffs_id {cutoffs_id} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(\n                {\"cutoffs_id\": cutoffs_id, **param_dict, \"cutoffs_hash\": param_hash}\n            )\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.QualityMetricSet", "title": "<code>QualityMetricSet</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Set of quality metric values for clusters and its cut-offs.</p> <p>Attributes:</p> Name Type Description <code>ephys.QualityMetrics</code> <code>foreign key</code> <p>ephys.QualityMetrics primary key.</p> <code>QualityMetricCutoffs</code> <code>foreign key</code> <p>QualityMetricCutoffs primary key.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass QualityMetricSet(dj.Manual):\n\"\"\"Set of quality metric values for clusters and its cut-offs.\n\n    Attributes:\n        ephys.QualityMetrics (foreign key): ephys.QualityMetrics primary key.\n        QualityMetricCutoffs (foreign key): QualityMetricCutoffs primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.QualityMetrics\n    -&gt; QualityMetricCutoffs\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.QualityMetricReport", "title": "<code>QualityMetricReport</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Table for storing quality metric figures.</p> <p>Attributes:</p> Name Type Description <code>QualityMetricSet</code> <code>foreign key</code> <p>QualityMetricSet primary key.</p> <code>plot_grid</code> <code>longblob</code> <p>Plotly figure object.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass QualityMetricReport(dj.Computed):\n\"\"\"Table for storing quality metric figures.\n\n    Attributes:\n        QualityMetricSet (foreign key): QualityMetricSet primary key.\n        plot_grid (longblob): Plotly figure object.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; QualityMetricSet\n    ---\n    plot_grid : longblob\n    \"\"\"\n\n    def make(self, key):\n        from .plotting.qc import QualityMetricFigs\n\n        cutoffs = (QualityMetricCutoffs &amp; key).fetch1()\n        qc_key = ephys.QualityMetrics &amp; key\n\n        self.insert1(\n            key.update(\n                dict(plot_grid=QualityMetricFigs(qc_key, **cutoffs).get_grid().to_json)\n            )\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/probe/", "title": "probe.py", "text": "<p>Neuropixels Probes</p>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.activate", "title": "<code>activate(schema_name, *, create_schema=True, create_tables=True)</code>", "text": "<p>Activates the <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>A string containing the name of the probe schema.</p> required <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion.</p> <p>Functions:</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>def activate(\n    schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n):\n\"\"\"Activates the `probe` schemas.\n\n    Args:\n        schema_name (str): A string containing the name of the probe schema.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion.\n\n    Functions:\n    \"\"\"\n    schema.activate(\n        schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n\n    # Add neuropixels probes\n    for probe_type in (\n        \"neuropixels 1.0 - 3A\",\n        \"neuropixels 1.0 - 3B\",\n        \"neuropixels UHD\",\n        \"neuropixels 2.0 - SS\",\n        \"neuropixels 2.0 - MS\",\n    ):\n        if not (ProbeType &amp; {\"probe_type\": probe_type}):\n            try:\n                ProbeType.create_neuropixels_probe(probe_type)\n            except dj.errors.DataJointError as e:\n                print(f\"Unable to create probe-type: {probe_type}\\n{str(e)}\")\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType", "title": "<code>ProbeType</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Type of probe.</p> <p>Attributes:</p> Name Type Description <code>probe_type</code> <code>foreign key, varchar (32) </code> <p>Name of the probe type.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@schema\nclass ProbeType(dj.Lookup):\n\"\"\"Type of probe.\n\n    Attributes:\n        probe_type (foreign key, varchar (32) ): Name of the probe type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Type of probe, with specific electrodes geometry defined\n    probe_type: varchar(32)  # e.g. neuropixels_1.0\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Electrode information for a given probe.\n\n        Attributes:\n            ProbeType (foreign key): ProbeType primary key.\n            electrode (foreign key, int): Electrode index, starting at 0.\n            shank (int): shank index, starting at 0.\n            shank_col (int): column index, starting at 0.\n            shank_row (int): row index, starting at 0.\n            x_coord (float): x-coordinate of the electrode within the probe in micrometers.\n            y_coord (float): y-coordinate of the electrode within the probe in micrometers.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        electrode: int       # electrode index, starts at 0\n        ---\n        shank: int           # shank index, starts at 0, advance left to right\n        shank_col: int       # column index, starts at 0, advance left to right\n        shank_row: int       # row index, starts at 0.\n        x_coord=NULL: float  # (um) x coordinate of the electrode within the probe.\n        y_coord=NULL: float  # (um) y coordinate of the electrode within the probe.\n        \"\"\"\n\n    @staticmethod\n    def create_neuropixels_probe(probe_type: str = \"neuropixels 1.0 - 3A\"):\n\"\"\"\n        Create `ProbeType` and `Electrode` for neuropixels probes:\n        + neuropixels 1.0 - 3A\n        + neuropixels 1.0 - 3B\n        + neuropixels UHD\n        + neuropixels 2.0 - SS\n        + neuropixels 2.0 - MS\n\n        For electrode location, the (0, 0) is the\n         bottom left corner of the probe (ignore the tip portion)\n        Electrode numbering is 1-indexing\n        \"\"\"\n\n        neuropixels_probes_config = {\n            \"neuropixels 1.0 - 3A\": dict(\n                site_count_per_shank=960,\n                col_spacing=32,\n                row_spacing=20,\n                white_spacing=16,\n                col_count_per_shank=2,\n                shank_count=1,\n                shank_spacing=0,\n            ),\n            \"neuropixels 1.0 - 3B\": dict(\n                site_count_per_shank=960,\n                col_spacing=32,\n                row_spacing=20,\n                white_spacing=16,\n                col_count_per_shank=2,\n                shank_count=1,\n                shank_spacing=0,\n            ),\n            \"neuropixels UHD\": dict(\n                site_count_per_shank=384,\n                col_spacing=6,\n                row_spacing=6,\n                white_spacing=0,\n                col_count_per_shank=8,\n                shank_count=1,\n                shank_spacing=0,\n            ),\n            \"neuropixels 2.0 - SS\": dict(\n                site_count_per_shank=1280,\n                col_spacing=32,\n                row_spacing=15,\n                white_spacing=0,\n                col_count_per_shank=2,\n                shank_count=1,\n                shank_spacing=250,\n            ),\n            \"neuropixels 2.0 - MS\": dict(\n                site_count_per_shank=1280,\n                col_spacing=32,\n                row_spacing=15,\n                white_spacing=0,\n                col_count_per_shank=2,\n                shank_count=4,\n                shank_spacing=250,\n            ),\n        }\n\n        probe_type = {\"probe_type\": probe_type}\n        electrode_layouts = build_electrode_layouts(\n            **{**neuropixels_probes_config[probe_type[\"probe_type\"]], **probe_type}\n        )\n        with ProbeType.connection.transaction:\n            ProbeType.insert1(probe_type, skip_duplicates=True)\n            ProbeType.Electrode.insert(electrode_layouts, skip_duplicates=True)\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Electrode information for a given probe.</p> <p>Attributes:</p> Name Type Description <code>ProbeType</code> <code>foreign key</code> <p>ProbeType primary key.</p> <code>electrode</code> <code>foreign key, int</code> <p>Electrode index, starting at 0.</p> <code>shank</code> <code>int</code> <p>shank index, starting at 0.</p> <code>shank_col</code> <code>int</code> <p>column index, starting at 0.</p> <code>shank_row</code> <code>int</code> <p>row index, starting at 0.</p> <code>x_coord</code> <code>float</code> <p>x-coordinate of the electrode within the probe in micrometers.</p> <code>y_coord</code> <code>float</code> <p>y-coordinate of the electrode within the probe in micrometers.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Electrode information for a given probe.\n\n    Attributes:\n        ProbeType (foreign key): ProbeType primary key.\n        electrode (foreign key, int): Electrode index, starting at 0.\n        shank (int): shank index, starting at 0.\n        shank_col (int): column index, starting at 0.\n        shank_row (int): row index, starting at 0.\n        x_coord (float): x-coordinate of the electrode within the probe in micrometers.\n        y_coord (float): y-coordinate of the electrode within the probe in micrometers.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    electrode: int       # electrode index, starts at 0\n    ---\n    shank: int           # shank index, starts at 0, advance left to right\n    shank_col: int       # column index, starts at 0, advance left to right\n    shank_row: int       # row index, starts at 0.\n    x_coord=NULL: float  # (um) x coordinate of the electrode within the probe.\n    y_coord=NULL: float  # (um) y coordinate of the electrode within the probe.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.create_neuropixels_probe", "title": "<code>create_neuropixels_probe(probe_type='neuropixels 1.0 - 3A')</code>  <code>staticmethod</code>", "text": "<p>Create <code>ProbeType</code> and <code>Electrode</code> for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS</p> <p>For electrode location, the (0, 0) is the  bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@staticmethod\ndef create_neuropixels_probe(probe_type: str = \"neuropixels 1.0 - 3A\"):\n\"\"\"\n    Create `ProbeType` and `Electrode` for neuropixels probes:\n    + neuropixels 1.0 - 3A\n    + neuropixels 1.0 - 3B\n    + neuropixels UHD\n    + neuropixels 2.0 - SS\n    + neuropixels 2.0 - MS\n\n    For electrode location, the (0, 0) is the\n     bottom left corner of the probe (ignore the tip portion)\n    Electrode numbering is 1-indexing\n    \"\"\"\n\n    neuropixels_probes_config = {\n        \"neuropixels 1.0 - 3A\": dict(\n            site_count_per_shank=960,\n            col_spacing=32,\n            row_spacing=20,\n            white_spacing=16,\n            col_count_per_shank=2,\n            shank_count=1,\n            shank_spacing=0,\n        ),\n        \"neuropixels 1.0 - 3B\": dict(\n            site_count_per_shank=960,\n            col_spacing=32,\n            row_spacing=20,\n            white_spacing=16,\n            col_count_per_shank=2,\n            shank_count=1,\n            shank_spacing=0,\n        ),\n        \"neuropixels UHD\": dict(\n            site_count_per_shank=384,\n            col_spacing=6,\n            row_spacing=6,\n            white_spacing=0,\n            col_count_per_shank=8,\n            shank_count=1,\n            shank_spacing=0,\n        ),\n        \"neuropixels 2.0 - SS\": dict(\n            site_count_per_shank=1280,\n            col_spacing=32,\n            row_spacing=15,\n            white_spacing=0,\n            col_count_per_shank=2,\n            shank_count=1,\n            shank_spacing=250,\n        ),\n        \"neuropixels 2.0 - MS\": dict(\n            site_count_per_shank=1280,\n            col_spacing=32,\n            row_spacing=15,\n            white_spacing=0,\n            col_count_per_shank=2,\n            shank_count=4,\n            shank_spacing=250,\n        ),\n    }\n\n    probe_type = {\"probe_type\": probe_type}\n    electrode_layouts = build_electrode_layouts(\n        **{**neuropixels_probes_config[probe_type[\"probe_type\"]], **probe_type}\n    )\n    with ProbeType.connection.transaction:\n        ProbeType.insert1(probe_type, skip_duplicates=True)\n        ProbeType.Electrode.insert(electrode_layouts, skip_duplicates=True)\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.Probe", "title": "<code>Probe</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Represent a physical probe with unique ID</p> <p>Attributes:</p> Name Type Description <code>probe</code> <code>foreign key, varchar(32) </code> <p>Unique ID for this model of the probe.</p> <code>ProbeType</code> <code>dict</code> <p>ProbeType entry.</p> <code>probe_comment</code> <code> varchar(1000) </code> <p>Comment about this model of probe.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@schema\nclass Probe(dj.Lookup):\n\"\"\"Represent a physical probe with unique ID\n\n    Attributes:\n        probe (foreign key, varchar(32) ): Unique ID for this model of the probe.\n        ProbeType (dict): ProbeType entry.\n        probe_comment ( varchar(1000) ): Comment about this model of probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Represent a physical probe with unique identification\n    probe: varchar(32)  # unique identifier for this model of probe (e.g. serial number)\n    ---\n    -&gt; ProbeType\n    probe_comment='' :  varchar(1000)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ElectrodeConfig", "title": "<code>ElectrodeConfig</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Electrode configuration setting on a probe.</p> <p>Attributes:</p> Name Type Description <code>electrode_config_hash</code> <code>foreign key, uuid</code> <p>unique index for electrode configuration.</p> <code>ProbeType</code> <code>dict</code> <p>ProbeType entry.</p> <code>electrode_config_name</code> <code> varchar(4000) </code> <p>User-friendly name for this electrode configuration.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@schema\nclass ElectrodeConfig(dj.Lookup):\n\"\"\"Electrode configuration setting on a probe.\n\n    Attributes:\n        electrode_config_hash (foreign key, uuid): unique index for electrode configuration.\n        ProbeType (dict): ProbeType entry.\n        electrode_config_name ( varchar(4000) ): User-friendly name for this electrode configuration.\n    \"\"\"\n\n    definition = \"\"\"\n    # The electrode configuration setting on a given probe\n    electrode_config_hash: uuid\n    ---\n    -&gt; ProbeType\n    electrode_config_name: varchar(4000)  # user friendly name\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Electrode included in the recording.\n\n        Attributes:\n            ElectrodeConfig (foreign key): ElectrodeConfig primary key.\n            ProbeType.Electrode (foreign key): ProbeType.Electrode primary key.\n        \"\"\"\n\n        definition = \"\"\"  # Electrodes selected for recording\n        -&gt; master\n        -&gt; ProbeType.Electrode\n        \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ElectrodeConfig.Electrode", "title": "<code>Electrode</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Electrode included in the recording.</p> <p>Attributes:</p> Name Type Description <code>ElectrodeConfig</code> <code>foreign key</code> <p>ElectrodeConfig primary key.</p> <code>ProbeType.Electrode</code> <code>foreign key</code> <p>ProbeType.Electrode primary key.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Electrode included in the recording.\n\n    Attributes:\n        ElectrodeConfig (foreign key): ElectrodeConfig primary key.\n        ProbeType.Electrode (foreign key): ProbeType.Electrode primary key.\n    \"\"\"\n\n    definition = \"\"\"  # Electrodes selected for recording\n    -&gt; master\n    -&gt; ProbeType.Electrode\n    \"\"\"\n</code></pre>"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.build_electrode_layouts", "title": "<code>build_electrode_layouts(probe_type, site_count_per_shank, col_spacing=None, row_spacing=None, white_spacing=None, col_count_per_shank=1, shank_count=1, shank_spacing=None, y_origin='bottom')</code>", "text": "<p>Builds electrode layouts.</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g., \"neuropixels 1.0 - 3A\").</p> required <code>site_count_per_shank</code> <code>int</code> <p>site count per shank.</p> required <code>col_spacing</code> <code>float</code> <p>(um) horizontal spacing between sites. Defaults to None (single column).</p> <code>None</code> <code>row_spacing</code> <code>float</code> <p>(um) vertical spacing between columns. Defaults to None (single row).</p> <code>None</code> <code>white_spacing</code> <code>float</code> <p>(um) offset spacing. Defaults to None.</p> <code>None</code> <code>col_count_per_shank</code> <code>int</code> <p>number of column per shank. Defaults to 1 (single column).</p> <code>1</code> <code>shank_count</code> <code>int</code> <p>number of shank. Defaults to 1 (single shank).</p> <code>1</code> <code>shank_spacing</code> <code>float</code> <p>(um) spacing between shanks. Defaults to None (single shank).</p> <code>None</code> <code>y_origin</code> <code>str</code> <p>{\"bottom\", \"top\"}. y value decrements if \"top\". Defaults to \"bottom\".</p> <code>'bottom'</code> Source code in <code>element_array_ephys/probe.py</code> <pre><code>def build_electrode_layouts(\n    probe_type: str,\n    site_count_per_shank: int,\n    col_spacing: float = None,\n    row_spacing: float = None,\n    white_spacing: float = None,\n    col_count_per_shank: int = 1,\n    shank_count: int = 1,\n    shank_spacing: float = None,\n    y_origin=\"bottom\",\n) -&gt; list[dict]:\n\"\"\"Builds electrode layouts.\n\n    Args:\n        probe_type (str): probe type (e.g., \"neuropixels 1.0 - 3A\").\n        site_count_per_shank (int): site count per shank.\n        col_spacing (float): (um) horizontal spacing between sites. Defaults to None (single column).\n        row_spacing (float): (um) vertical spacing between columns. Defaults to None (single row).\n        white_spacing (float): (um) offset spacing. Defaults to None.\n        col_count_per_shank (int): number of column per shank. Defaults to 1 (single column).\n        shank_count (int): number of shank. Defaults to 1 (single shank).\n        shank_spacing (float): (um) spacing between shanks. Defaults to None (single shank).\n        y_origin (str): {\"bottom\", \"top\"}. y value decrements if \"top\". Defaults to \"bottom\".\n    \"\"\"\n    row_count = int(site_count_per_shank / col_count_per_shank)\n    x_coords = np.tile(\n        np.arange(0, (col_spacing or 1) * col_count_per_shank, (col_spacing or 1)),\n        row_count,\n    )\n    y_coords = np.repeat(np.arange(row_count) * (row_spacing or 1), col_count_per_shank)\n\n    if white_spacing:\n        x_white_spaces = np.tile(\n            [white_spacing, white_spacing, 0, 0], int(row_count / 2)\n        )\n        x_coords = x_coords + x_white_spaces\n\n    shank_cols = np.tile(range(col_count_per_shank), row_count)\n    shank_rows = np.repeat(range(row_count), col_count_per_shank)\n\n    return [\n        {\n            \"probe_type\": probe_type,\n            \"electrode\": (site_count_per_shank * shank_no) + e_id,\n            \"shank\": shank_no,\n            \"shank_col\": c_id,\n            \"shank_row\": r_id,\n            \"x_coord\": x + (shank_no * (shank_spacing or 1)),\n            \"y_coord\": {\"top\": -y, \"bottom\": y}[y_origin],\n        }\n        for shank_no in range(shank_count)\n        for e_id, (c_id, r_id, x, y) in enumerate(\n            zip(shank_cols, shank_rows, x_coords, y_coords)\n        )\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/version/", "title": "version.py", "text": "<p>Package metadata.</p>"}, {"location": "api/element_array_ephys/export/nwb/nwb/", "title": "nwb.py", "text": ""}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.DecimalEncoder", "title": "<code>DecimalEncoder</code>", "text": "<p>         Bases: <code>json.JSONEncoder</code></p> <p>Extension of json.JSONEncoder class</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>class DecimalEncoder(json.JSONEncoder):\n\"\"\"Extension of json.JSONEncoder class\"\"\"\n\n    def default(self, o):\n        if isinstance(o, decimal.Decimal):\n            return str(o)\n        return super(DecimalEncoder, self).default(o)\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator", "title": "<code>LFPDataChunkIterator</code>", "text": "<p>         Bases: <code>GenericDataChunkIterator</code></p> <p>DataChunkIterator for LFP data that pulls data one channel at a time.</p> <p>Used when reading LFP data from the database (as opposed to directly from source files).</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>class LFPDataChunkIterator(GenericDataChunkIterator):\n\"\"\"DataChunkIterator for LFP data that pulls data one channel at a time.\n\n    Used when reading LFP data from the database (as opposed to directly from source\n    files).\n    \"\"\"\n\n    def __init__(self, lfp_electrodes_query: Table, chunk_length: int = 10000):\n\"\"\"\n        Arguments:\n            lfp_electrodes_query (datajoint table): element_array_ephys.ephys.LFP.Electrode\n            chunk_length (int): Optional. Default 10,000. Chunks are blocks of disk\n                space where data are stored contiguously and compressed.\n        \"\"\"\n        self.lfp_electrodes_query = lfp_electrodes_query\n        self.electrodes = self.lfp_electrodes_query.fetch(\"electrode\")\n\n        first_record = (\n            self.lfp_electrodes_query &amp; dict(electrode=self.electrodes[0])\n        ).fetch1()\n\n        self.n_channels = len(self.electrodes)\n        self.n_tt = len(first_record[\"lfp\"])\n        self._dtype = first_record[\"lfp\"].dtype\n\n        super().__init__(buffer_shape=(self.n_tt, 1), chunk_shape=(chunk_length, 1))\n\n    def _get_data(self, selection):\n        electrode = self.electrodes[selection[1]][0]\n        return (self.lfp_electrodes_query &amp; dict(electrode=electrode)).fetch1(\"lfp\")[\n            selection[0], np.newaxis\n        ]\n\n    def _get_dtype(self):\n        return self._dtype\n\n    def _get_maxshape(self):\n        return self.n_tt, self.n_channels\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__", "title": "<code>__init__(lfp_electrodes_query, chunk_length=10000)</code>", "text": "<p>Parameters:</p> Name Type Description Default <code>lfp_electrodes_query</code> <code>datajoint table</code> <p>element_array_ephys.ephys.LFP.Electrode</p> required <code>chunk_length</code> <code>int</code> <p>Optional. Default 10,000. Chunks are blocks of disk space where data are stored contiguously and compressed.</p> <code>10000</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def __init__(self, lfp_electrodes_query: Table, chunk_length: int = 10000):\n\"\"\"\n    Arguments:\n        lfp_electrodes_query (datajoint table): element_array_ephys.ephys.LFP.Electrode\n        chunk_length (int): Optional. Default 10,000. Chunks are blocks of disk\n            space where data are stored contiguously and compressed.\n    \"\"\"\n    self.lfp_electrodes_query = lfp_electrodes_query\n    self.electrodes = self.lfp_electrodes_query.fetch(\"electrode\")\n\n    first_record = (\n        self.lfp_electrodes_query &amp; dict(electrode=self.electrodes[0])\n    ).fetch1()\n\n    self.n_channels = len(self.electrodes)\n    self.n_tt = len(first_record[\"lfp\"])\n    self._dtype = first_record[\"lfp\"].dtype\n\n    super().__init__(buffer_shape=(self.n_tt, 1), chunk_shape=(chunk_length, 1))\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb", "title": "<code>add_electrodes_to_nwb(session_key, nwbfile)</code>", "text": "<p>Add electrodes table to NWBFile.</p> <p>This is needed for any ElectricalSeries, including raw source data and LFP.</p> Mapping <p>ephys.InsertionLocation -&gt; ElectrodeGroup.location</p> <p>probe.Probe::probe -&gt; device.name probe.Probe::probe_comment -&gt; device.description probe.Probe::probe_type -&gt; device.manufacturer</p> <p>probe.ProbeType.Electrode::electrode -&gt; electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -&gt; electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -&gt; electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -&gt; electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -&gt; electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -&gt; electrodes[\"shank_row\"]</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>nwb file</p> required Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_electrodes_to_nwb(session_key: dict, nwbfile: pynwb.NWBFile):\n\"\"\"Add electrodes table to NWBFile.\n\n    This is needed for any ElectricalSeries, including raw source data and LFP.\n\n    Mapping:\n        ephys.InsertionLocation -&gt; ElectrodeGroup.location\n\n        probe.Probe::probe -&gt; device.name\n        probe.Probe::probe_comment -&gt; device.description\n        probe.Probe::probe_type -&gt; device.manufacturer\n\n        probe.ProbeType.Electrode::electrode -&gt; electrodes[\"id_in_probe\"]\n        probe.ProbeType.Electrode::y_coord -&gt; electrodes[\"rel_y\"]\n        probe.ProbeType.Electrode::x_coord -&gt; electrodes[\"rel_x\"]\n        probe.ProbeType.Electrode::shank -&gt; electrodes[\"shank\"]\n        probe.ProbeType.Electrode::shank_col -&gt; electrodes[\"shank_col\"]\n        probe.ProbeType.Electrode::shank_row -&gt; electrodes[\"shank_row\"]\n\n    Arguments:\n        session_key (dict): key from Session table\n        nwbfile (pynwb.NWBFile): nwb file\n    \"\"\"\n    electrodes_query = probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n\n    for additional_attribute in [\"shank_col\", \"shank_row\", \"shank\"]:\n        nwbfile.add_electrode_column(\n            name=electrodes_query.heading.attributes[additional_attribute].name,\n            description=electrodes_query.heading.attributes[\n                additional_attribute\n            ].comment,\n        )\n\n    nwbfile.add_electrode_column(\n        name=\"id_in_probe\",\n        description=\"electrode id within the probe\",\n    )\n\n    for this_probe in (ephys.ProbeInsertion * probe.Probe &amp; session_key).fetch(\n        as_dict=True\n    ):\n        insertion_record = (ephys.InsertionLocation &amp; this_probe).fetch(as_dict=True)\n        if len(insertion_record) == 1:\n            insert_location = json.dumps(\n                {\n                    k: v\n                    for k, v in insertion_record[0].items()\n                    if k not in ephys.InsertionLocation.primary_key\n                },\n                cls=DecimalEncoder,\n            )\n        elif len(insertion_record) == 0:\n            insert_location = \"unknown\"\n        else:\n            raise dj.DataJointError(\n                f\"Found multiple insertion locations for {this_probe}\"\n            )\n\n        device = nwbfile.create_device(\n            name=this_probe[\"probe\"],\n            description=this_probe.get(\"probe_comment\", None),\n            manufacturer=this_probe.get(\"probe_type\", None),\n        )\n        shank_ids = set((probe.ProbeType.Electrode &amp; this_probe).fetch(\"shank\"))\n        for shank_id in shank_ids:\n            electrode_group = nwbfile.create_electrode_group(\n                name=f\"probe{this_probe['probe']}_shank{shank_id}\",\n                description=f\"probe{this_probe['probe']}_shank{shank_id}\",\n                location=insert_location,\n                device=device,\n            )\n\n            electrodes_query = (\n                probe.ProbeType.Electrode &amp; this_probe &amp; dict(shank=shank_id)\n            ).fetch(as_dict=True)\n            for electrode in electrodes_query:\n                nwbfile.add_electrode(\n                    group=electrode_group,\n                    filtering=\"unknown\",\n                    imp=-1.0,\n                    x=np.nan,\n                    y=np.nan,\n                    z=np.nan,\n                    rel_x=electrode[\"x_coord\"],\n                    rel_y=electrode[\"y_coord\"],\n                    rel_z=np.nan,\n                    shank_col=electrode[\"shank_col\"],\n                    shank_row=electrode[\"shank_row\"],\n                    location=\"unknown\",\n                    id_in_probe=electrode[\"electrode\"],\n                    shank=electrode[\"shank\"],\n                )\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table", "title": "<code>create_units_table(session_key, nwbfile, paramset_record, name='units', desc='data on spiking units')</code>", "text": "Mapping <p>ephys.CuratedClustering.Unit::unit -&gt; units.id ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]</p> <p>ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>nwb file</p> required <code>paramset_record</code> <code>int</code> <p>paramset id from ephys schema</p> required <code>name</code> <code>str</code> <p>Optional table name. Default \"units\"</p> <code>'units'</code> <code>desc</code> <code>str</code> <p>Optional table description. Default \"data on spiking units\"</p> <code>'data on spiking units'</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def create_units_table(\n    session_key: dict,\n    nwbfile: pynwb.NWBFile,\n    paramset_record,\n    name=\"units\",\n    desc=\"data on spiking units\",\n):\n\"\"\"\n    Mapping:\n        ephys.CuratedClustering.Unit::unit -&gt; units.id\n        ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"]\n        ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"]\n        ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]\n\n        ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]\n\n    Arguments:\n        session_key (dict): key from Session table\n        nwbfile (pynwb.NWBFile): nwb file\n        paramset_record (int): paramset id from ephys schema\n        name (str): Optional table name. Default \"units\"\n        desc (str): Optional table description. Default \"data on spiking units\"\n    \"\"\"\n\n    # electrode id mapping\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    units_table = pynwb.misc.Units(name=name, description=desc)\n    # add additional columns to the units table\n    for additional_attribute in [\"cluster_quality_label\", \"spike_depths\"]:\n        # The `index` parameter indicates whether the column is a \"ragged array,\" i.e.\n        # whether each row of this column is a vector with potentially different lengths\n        # for each row.\n        units_table.add_column(\n            name=additional_attribute,\n            description=ephys.CuratedClustering.Unit.heading.attributes[\n                additional_attribute\n            ].comment,\n            index=additional_attribute == \"spike_depths\",\n        )\n\n    clustering_query = (\n        ephys.EphysRecording * ephys.ClusteringTask &amp; session_key &amp; paramset_record\n    )\n\n    for unit in tqdm(\n        (ephys.CuratedClustering.Unit &amp; clustering_query.proj()).fetch(as_dict=True),\n        desc=f\"creating units table for paramset {paramset_record['paramset_idx']}\",\n    ):\n        probe_id, shank_num = (\n            ephys.ProbeInsertion\n            * ephys.CuratedClustering.Unit\n            * probe.ProbeType.Electrode\n            &amp; dict(\n                (k, unit[k])\n                for k in unit.keys()  # excess keys caused errs\n                if k not in [\"spike_times\", \"spike_sites\", \"spike_depths\"]\n            )\n        ).fetch1(\"probe\", \"shank\")\n\n        waveform_mean = (\n            ephys.WaveformSet.PeakWaveform() &amp; clustering_query &amp; unit\n        ).fetch1(\"peak_electrode_waveform\")\n\n        units_table.add_row(\n            id=unit[\"unit\"],\n            electrodes=[mapping[(probe_id, unit[\"electrode\"])]],\n            electrode_group=nwbfile.electrode_groups[\n                f\"probe{probe_id}_shank{shank_num}\"\n            ],\n            cluster_quality_label=unit[\"cluster_quality_label\"],\n            spike_times=unit[\"spike_times\"],\n            spike_depths=unit[\"spike_depths\"],\n            waveform_mean=waveform_mean,\n        )\n\n    return units_table\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb", "title": "<code>add_ephys_units_to_nwb(session_key, nwbfile, primary_clustering_paramset_idx=0)</code>", "text": "<p>Add spiking data to NWBFile.</p> <p>In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations.</p> <p>Use <code>primary_clustering_paramset_idx</code> to indicate which clustering is primary. All others will be stored in /processing/ecephys/.</p> Mapping <p>ephys.CuratedClustering.Unit::unit -&gt; units.id ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]</p> <p>ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>nwbfile</code> <code>pynwb.NWBFile</code> <p>nwb file</p> required <code>primary_clustering_paramset_idx</code> <code>int</code> <p>Optional. Default 0</p> <code>0</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_units_to_nwb(\n    session_key: dict, nwbfile: pynwb.NWBFile, primary_clustering_paramset_idx: int = 0\n):\n\"\"\"Add spiking data to NWBFile.\n\n    In NWB, spiking data is stored in a Units table. The primary Units table is\n    stored at /units. The spiking data in /units is generally the data used in\n    downstream analysis. Only a single Units table can be stored at /units. Other Units\n    tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of\n    Units tables can be stored in this ProcessingModule as long as they have different\n    names, and these Units tables can store intermediate processing steps or\n    alternative curations.\n\n    Use `primary_clustering_paramset_idx` to indicate which clustering is primary. All\n    others will be stored in /processing/ecephys/.\n\n    Mapping:\n        ephys.CuratedClustering.Unit::unit -&gt; units.id\n        ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"]\n        ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"]\n        ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]\n\n        ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]\n\n    Arguments:\n        session_key (dict): key from Session table\n        nwbfile (pynwb.NWBFile): nwb file\n        primary_clustering_paramset_idx (int): Optional. Default 0\n    \"\"\"\n\n    if not ephys.ClusteringTask &amp; session_key:\n        warnings.warn(f\"No unit data exists for session:{session_key}\")\n        return\n\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    for paramset_record in (\n        ephys.ClusteringParamSet &amp; ephys.CuratedClustering &amp; session_key\n    ).fetch(\"paramset_idx\", \"clustering_method\", \"paramset_desc\", as_dict=True):\n        if paramset_record[\"paramset_idx\"] == primary_clustering_paramset_idx:\n            units_table = create_units_table(\n                session_key,\n                nwbfile,\n                paramset_record,\n                desc=paramset_record[\"paramset_desc\"],\n            )\n            nwbfile.units = units_table\n        else:\n            name = f\"units_{paramset_record['clustering_method']}\"\n            units_table = create_units_table(\n                session_key,\n                nwbfile,\n                paramset_record,\n                name=name,\n                desc=paramset_record[\"paramset_desc\"],\n            )\n            ecephys_module = get_module(nwbfile, \"ecephys\")\n            ecephys_module.add(units_table)\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping", "title": "<code>get_electrodes_mapping(electrodes)</code>", "text": "<p>Create mapping from probe and electrode id to row number in the electrodes table</p> <p>This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries.</p> <p>Parameters:</p> Name Type Description Default <code>electrodes</code> <code> hdmf.common.table.DynamicTable </code> <p>hdmf Dynamic Table</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict using tuple (electrodes device name, probe id index) as key and index electrode index as value</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def get_electrodes_mapping(electrodes) -&gt; dict:\n\"\"\"Create mapping from probe and electrode id to row number in the electrodes table\n\n    This is used in the construction of the DynamicTableRegion that indicates what rows\n    of the electrodes table correspond to the data in an ElectricalSeries.\n\n    Arguments:\n        electrodes ( hdmf.common.table.DynamicTable ): hdmf Dynamic Table\n\n    Returns:\n        dict using tuple (electrodes device name, probe id index) as key and index\n            electrode index as value\n\n    \"\"\"\n    return {\n        (\n            electrodes[\"group\"][idx].device.name,\n            electrodes[\"id_in_probe\"][idx],\n        ): idx\n        for idx in range(len(electrodes))\n    }\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper", "title": "<code>gains_helper(gains)</code>", "text": "<p>This handles three different cases for gains. See below</p> Cases <ol> <li>gains are all 1. In this case, return conversion=1e-6, which applies to all     channels and converts from microvolts to volts.</li> <li>Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply     this gain to all channels and convert units to volts.</li> <li>Gains are different for different channels. In this case use the     <code>channel_conversion</code> field in addition to the <code>conversion</code> field so that      each channel can be converted to volts using its own individual gain.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>gains</code> <code> np.ndarray </code> <p>array of gains</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict with conversion float as key and channel_conversion np.ndarray value</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def gains_helper(gains) -&gt; dict:\n\"\"\"This handles three different cases for gains. See below\n\n    Cases:\n        1. gains are all 1. In this case, return conversion=1e-6, which applies to all\n            channels and converts from microvolts to volts.\n        2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply\n            this gain to all channels and convert units to volts.\n        3. Gains are different for different channels. In this case use the\n            `channel_conversion` field in addition to the `conversion` field so that\n             each channel can be converted to volts using its own individual gain.\n\n    Arguments:\n        gains ( np.ndarray ): array of gains\n\n    Returns:\n        dict with conversion float as key and channel_conversion np.ndarray value\n\n    \"\"\"\n    if all(x == 1 for x in gains):\n        return dict(conversion=1e-6, channel_conversion=None)\n    if all(x == gains[0] for x in gains):\n        return dict(conversion=1e-6 * gains[0], channel_conversion=None)\n    return dict(conversion=1e-6, channel_conversion=gains)\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb", "title": "<code>add_ephys_recording_to_nwb(session_key, ephys_root_data_dir, nwbfile, end_frame=None)</code>", "text": "<p>Read voltage data from source files and iteratively transfer to the NWB file.</p> <p>Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data.</p> Mapping <p>source data -&gt; acquisition[\"ElectricalSeries\"]</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>ephys_root_data_dir</code> <code>str</code> <p>root data directory</p> required <code>nwbfile</code> <code>NWBFile</code> <p>nwb file</p> required <code>end_frame</code> <code>int</code> <p>Optional limit on frames for small test conversions</p> <code>None</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_recording_to_nwb(\n    session_key: dict,\n    ephys_root_data_dir: str,\n    nwbfile: pynwb.NWBFile,\n    end_frame: int = None,\n):\n\"\"\"Read voltage data from source files and iteratively transfer to the NWB file.\n\n    Automatically applies lossless compression to the data, so the final file might be\n    smaller than the original, without data loss. Currently supports Neuropixels data\n    acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data.\n\n    Mapping:\n        source data -&gt; acquisition[\"ElectricalSeries\"]\n\n    Arguments:\n        session_key (dict): key from Session table\n        ephys_root_data_dir (str): root data directory\n        nwbfile (NWBFile): nwb file\n        end_frame (int): Optional limit on frames for small test conversions\n    \"\"\"\n\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    for ephys_recording_record in (ephys.EphysRecording &amp; session_key).fetch(\n        as_dict=True\n    ):\n        probe_id = (ephys.ProbeInsertion() &amp; ephys_recording_record).fetch1(\"probe\")\n\n        relative_path = (\n            ephys.EphysRecording.EphysFile &amp; ephys_recording_record\n        ).fetch1(\"file_path\")\n        relative_path = relative_path.replace(\"\\\\\", \"/\")\n        file_path = find_full_path(ephys_root_data_dir, relative_path)\n\n        if ephys_recording_record[\"acq_software\"] == \"SpikeGLX\":\n            extractor = extractors.read_spikeglx(\n                os.path.split(file_path)[0], stream_id=\"imec.ap\"\n            )\n        elif ephys_recording_record[\"acq_software\"] == \"OpenEphys\":\n            extractor = extractors.read_openephys(file_path, stream_id=\"0\")\n        else:\n            raise ValueError(\n                f\"unsupported acq_software type: {ephys_recording_record['acq_software']}\"\n            )\n\n        conversion_kwargs = gains_helper(extractor.get_channel_gains())\n\n        if end_frame is not None:\n            extractor = extractor.frame_slice(0, end_frame)\n\n        recording_channels_by_id = (\n            probe.ElectrodeConfig.Electrode() &amp; ephys_recording_record\n        ).fetch(\"electrode\")\n\n        nwbfile.add_acquisition(\n            pynwb.ecephys.ElectricalSeries(\n                name=f\"ElectricalSeries{ephys_recording_record['insertion_number']}\",\n                description=str(ephys_recording_record),\n                data=SpikeInterfaceRecordingDataChunkIterator(extractor),\n                rate=ephys_recording_record[\"sampling_rate\"],\n                starting_time=(\n                    ephys_recording_record[\"recording_datetime\"]\n                    - ephys_recording_record[\"session_datetime\"]\n                ).total_seconds(),\n                electrodes=nwbfile.create_electrode_table_region(\n                    region=[mapping[(probe_id, x)] for x in recording_channels_by_id],\n                    name=\"electrodes\",\n                    description=\"recorded electrodes\",\n                ),\n                **conversion_kwargs,\n            )\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb", "title": "<code>add_ephys_lfp_from_dj_to_nwb(session_key, nwbfile)</code>", "text": "<p>Read LFP data from the data in element-array-ephys</p> Mapping <p>ephys.LFP.Electrode::lfp -&gt;     processing[\"ecephys\"].lfp.electrical_series[         \"ElectricalSeries{insertion_number}\"     ].data ephys.LFP::lfp_time_stamps -&gt;     processing[\"ecephys\"].lfp.electrical_series[         \"ElectricalSeries{insertion_number}\"     ].timestamps</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>nwbfile</code> <code>NWBFile</code> <p>nwb file</p> required Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_lfp_from_dj_to_nwb(session_key: dict, nwbfile: pynwb.NWBFile):\n\"\"\"Read LFP data from the data in element-array-ephys\n\n    Mapping:\n        ephys.LFP.Electrode::lfp -&gt;\n            processing[\"ecephys\"].lfp.electrical_series[\n                \"ElectricalSeries{insertion_number}\"\n            ].data\n        ephys.LFP::lfp_time_stamps -&gt;\n            processing[\"ecephys\"].lfp.electrical_series[\n                \"ElectricalSeries{insertion_number}\"\n            ].timestamps\n\n    Arguments:\n        session_key (dict): key from Session table\n        nwbfile (NWBFile): nwb file\n    \"\"\"\n\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    ecephys_module = get_module(\n        nwbfile, name=\"ecephys\", description=\"preprocessed ephys data\"\n    )\n\n    nwb_lfp = pynwb.ecephys.LFP(name=\"LFP\")\n    ecephys_module.add(nwb_lfp)\n\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    for lfp_record in (ephys.LFP &amp; session_key).fetch(as_dict=True):\n        probe_id = (ephys.ProbeInsertion &amp; lfp_record).fetch1(\"probe\")\n\n        lfp_electrodes_query = ephys.LFP.Electrode &amp; lfp_record\n        lfp_data = LFPDataChunkIterator(lfp_electrodes_query)\n\n        nwb_lfp.create_electrical_series(\n            name=f\"ElectricalSeries{lfp_record['insertion_number']}\",\n            description=f\"LFP from probe {probe_id}\",\n            data=H5DataIO(lfp_data, compression=True),\n            timestamps=lfp_record[\"lfp_time_stamps\"],\n            electrodes=nwbfile.create_electrode_table_region(\n                name=\"electrodes\",\n                description=\"electrodes used for LFP\",\n                region=[\n                    mapping[(probe_id, x)]\n                    for x in lfp_electrodes_query.fetch(\"electrode\")\n                ],\n            ),\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb", "title": "<code>add_ephys_lfp_from_source_to_nwb(session_key, ephys_root_data_dir, nwbfile, end_frame=None)</code>", "text": "<p>Read the LFP data from the source file. Currently, only works for SpikeGLX data.</p> <p>ephys.EphysRecording::recording_datetime -&gt; acquisition</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>nwbfile</code> <code>NWBFile</code> <p>nwb file</p> required <code>end_frame</code> <code>int</code> <p>Optional limit on frames for small test conversions</p> <code>None</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_lfp_from_source_to_nwb(\n    session_key: dict, ephys_root_data_dir, nwbfile: pynwb.NWBFile, end_frame=None\n):\n\"\"\"\n    Read the LFP data from the source file. Currently, only works for SpikeGLX data.\n\n    ephys.EphysRecording::recording_datetime -&gt; acquisition\n\n    Arguments:\n        session_key (dict): key from Session table\n        nwbfile (NWBFile): nwb file\n        end_frame (int): Optional limit on frames for small test conversions\n    \"\"\"\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    ecephys_module = get_module(\n        nwbfile, name=\"ecephys\", description=\"preprocessed ephys data\"\n    )\n\n    lfp = pynwb.ecephys.LFP()\n    ecephys_module.add(lfp)\n\n    for ephys_recording_record in (ephys.EphysRecording &amp; session_key).fetch(\n        as_dict=True\n    ):\n        probe_id = (ephys.ProbeInsertion() &amp; ephys_recording_record).fetch1(\"probe\")\n\n        relative_path = (\n            ephys.EphysRecording.EphysFile &amp; ephys_recording_record\n        ).fetch1(\"file_path\")\n        relative_path = relative_path.replace(\"\\\\\", \"/\")\n        file_path = find_full_path(ephys_root_data_dir, relative_path)\n\n        if ephys_recording_record[\"acq_software\"] == \"SpikeGLX\":\n            extractor = extractors.read_spikeglx(\n                os.path.split(file_path)[0], stream_id=\"imec.lf\"\n            )\n        else:\n            raise ValueError(\n                \"unsupported acq_software type:\"\n                + f\"{ephys_recording_record['acq_software']}\"\n            )\n\n        if end_frame is not None:\n            extractor = extractor.frame_slice(0, end_frame)\n\n        recording_channels_by_id = (\n            probe.ElectrodeConfig.Electrode() &amp; ephys_recording_record\n        ).fetch(\"electrode\")\n\n        conversion_kwargs = gains_helper(extractor.get_channel_gains())\n\n        lfp.add_electrical_series(\n            pynwb.ecephys.ElectricalSeries(\n                name=f\"ElectricalSeries{ephys_recording_record['insertion_number']}\",\n                description=f\"LFP from probe {probe_id}\",\n                data=SpikeInterfaceRecordingDataChunkIterator(extractor),\n                rate=extractor.get_sampling_frequency(),\n                starting_time=(\n                    ephys_recording_record[\"recording_datetime\"]\n                    - ephys_recording_record[\"session_datetime\"]\n                ).total_seconds(),\n                electrodes=nwbfile.create_electrode_table_region(\n                    region=[mapping[(probe_id, x)] for x in recording_channels_by_id],\n                    name=\"electrodes\",\n                    description=\"recorded electrodes\",\n                ),\n                **conversion_kwargs,\n            )\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb", "title": "<code>ecephys_session_to_nwb(session_key, raw=True, spikes=True, lfp='source', end_frame=None, lab_key=None, project_key=None, protocol_key=None, nwbfile_kwargs=None)</code>", "text": "<p>Main function for converting ephys data to NWB</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>raw</code> <code>bool</code> <p>Optional. Default True. Include the raw data from source. SpikeGLX &amp; OpenEphys are supported</p> <code>True</code> <code>spikes</code> <code>bool</code> <p>Optional. Default True. Whether to include CuratedClustering</p> <code>True</code> <code>lfp</code> <code>str</code> <p>One of the following. \"dj\", read LFP data from ephys.LFP. \"source\", read LFP data from source (SpikeGLX supported). False, do not convert LFP.</p> <code>'source'</code> <code>end_frame</code> <code>int</code> <p>Optional limit on frames for small test conversions.</p> <code>None</code> <code>lab_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>project_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>protocol_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>nwbfile_kwargs</code> <code>dict</code> <p>Optional. If Element Session is not used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required  minimal data for instantiating an NWBFile object. If element-session is  being used, this argument can optionally be used to overwrite NWBFile  fields.</p> <code>None</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def ecephys_session_to_nwb(\n    session_key,\n    raw=True,\n    spikes=True,\n    lfp=\"source\",\n    end_frame=None,\n    lab_key=None,\n    project_key=None,\n    protocol_key=None,\n    nwbfile_kwargs=None,\n):\n\"\"\"Main function for converting ephys data to NWB\n\n    Arguments:\n        session_key (dict): key from Session table\n        raw (bool): Optional. Default True. Include the raw data from source.\n            SpikeGLX &amp; OpenEphys are supported\n        spikes (bool): Optional. Default True. Whether to include CuratedClustering\n        lfp (str): One of the following.\n            \"dj\", read LFP data from ephys.LFP.\n            \"source\", read LFP data from source (SpikeGLX supported).\n            False, do not convert LFP.\n        end_frame (int): Optional limit on frames for small test conversions.\n        lab_key (dict): Optional key to add metadata from other Element Lab.\n        project_key (dict): Optional key to add metadata from other Element Lab.\n        protocol_key (dict): Optional key to add metadata from other Element Lab.\n        nwbfile_kwargs (dict): Optional. If Element Session is not used, this argument\n            is required and must be a dictionary containing 'session_description' (str),\n            'identifier' (str), and 'session_start_time' (datetime), the required\n             minimal data for instantiating an NWBFile object. If element-session is\n             being used, this argument can optionally be used to overwrite NWBFile\n             fields.\n    \"\"\"\n\n    session_to_nwb = getattr(ephys._linking_module, \"session_to_nwb\", False)\n\n    if session_to_nwb:\n        nwbfile = session_to_nwb(\n            session_key,\n            lab_key=lab_key,\n            project_key=project_key,\n            protocol_key=protocol_key,\n            additional_nwbfile_kwargs=nwbfile_kwargs,\n        )\n    else:\n        nwbfile = pynwb.NWBFile(**nwbfile_kwargs)\n\n    ephys_root_data_dir = ephys.get_ephys_root_data_dir()\n\n    if raw:\n        add_ephys_recording_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    if spikes:\n        add_ephys_units_to_nwb(session_key, nwbfile)\n\n    if lfp == \"dj\":\n        add_ephys_lfp_from_dj_to_nwb(session_key, nwbfile)\n\n    if lfp == \"source\":\n        add_ephys_lfp_from_source_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    return nwbfile\n</code></pre>"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb", "title": "<code>write_nwb(nwbfile, fname, check_read=True)</code>", "text": "<p>Export NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>NWBFile</code> <p>nwb file</p> required <code>fname</code> <code>str</code> <p>Absolute path including <code>*.nwb</code> extension.</p> required <code>check_read</code> <code>bool</code> <p>If True, PyNWB will try to read the produced NWB file and ensure that it can be read.</p> <code>True</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def write_nwb(nwbfile, fname, check_read=True):\n\"\"\"Export NWBFile\n\n    Arguments:\n        nwbfile (NWBFile): nwb file\n        fname (str): Absolute path including `*.nwb` extension.\n        check_read (bool): If True, PyNWB will try to read the produced NWB file and\n            ensure that it can be read.\n    \"\"\"\n    with pynwb.NWBHDF5IO(fname, \"w\") as io:\n        io.write(nwbfile)\n\n    if check_read:\n        with pynwb.NWBHDF5IO(fname, \"r\") as io:\n            io.read()\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/corr/", "title": "corr.py", "text": "<p>Code adapted from International Brain Laboratory, T. (2021). ibllib [Computer software]. https://github.com/int-brain-lab/ibllib</p>"}, {"location": "api/element_array_ephys/plotting/corr/#element_array_ephys.plotting.corr.xcorr", "title": "<code>xcorr(spike_times, spike_clusters, bin_size, window_size)</code>", "text": "<p>Compute all pairwise cross-correlograms among the clusters appearing in <code>spike_clusters</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike times in seconds.</p> required <code>spike_clusters</code> <code>np.ndarray</code> <p>Spike-cluster mapping.</p> required <code>bin_size</code> <code>float</code> <p>Size of the time bin in seconds.</p> required <code>window_size</code> <code>int</code> <p>Size of the correlogram window in seconds.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: cross-correlogram array</p> Source code in <code>element_array_ephys/plotting/corr.py</code> <pre><code>def xcorr(\n    spike_times: np.ndarray,\n    spike_clusters: np.ndarray,\n    bin_size: float,\n    window_size: int,\n) -&gt; np.ndarray:\n\"\"\"Compute all pairwise cross-correlograms among the clusters appearing in `spike_clusters`.\n\n    Args:\n        spike_times (np.ndarray): Spike times in seconds.\n        spike_clusters (np.ndarray): Spike-cluster mapping.\n        bin_size (float): Size of the time bin in seconds.\n        window_size (int): Size of the correlogram window in seconds.\n\n    Returns:\n         np.ndarray: cross-correlogram array\n    \"\"\"\n    assert np.all(np.diff(spike_times) &gt;= 0), \"The spike times must be increasing.\"\n    assert spike_times.ndim == 1\n    assert spike_times.shape == spike_clusters.shape\n\n    # Find `binsize`.\n    bin_size = np.clip(bin_size, 1e-5, 1e5)  # in seconds\n\n    # Find `winsize_bins`.\n    window_size = np.clip(window_size, 1e-5, 1e5)  # in seconds\n    winsize_bins = 2 * int(0.5 * window_size / bin_size) + 1\n\n    # Take the cluster order into account.\n    clusters = np.unique(spike_clusters)\n    n_clusters = len(clusters)\n\n    # Like spike_clusters, but with 0..n_clusters-1 indices.\n    spike_clusters_i = _index_of(spike_clusters, clusters)\n\n    # Shift between the two copies of the spike trains.\n    shift = 1\n\n    # At a given shift, the mask precises which spikes have matching spikes\n    # within the correlogram time window.\n    mask = np.ones_like(spike_times, dtype=bool)\n\n    correlograms = _create_correlograms_array(n_clusters, winsize_bins)\n\n    # The loop continues as long as there is at least one spike with\n    # a matching spike.\n    while mask[:-shift].any():\n        # Interval between spike i and spike i+shift.\n        spike_diff = _diff_shifted(spike_times, shift)\n\n        # Binarize the delays between spike i and spike i+shift.\n        spike_diff_b = np.round(spike_diff / bin_size).astype(np.int64)\n\n        # Spikes with no matching spikes are masked.\n        mask[:-shift][spike_diff_b &gt; (winsize_bins / 2)] = False\n\n        # Cache the masked spike delays.\n        m = mask[:-shift].copy()\n        d = spike_diff_b[m]\n\n        # Find the indices in the raveled correlograms array that need\n        # to be incremented, taking into account the spike clusters.\n        indices = np.ravel_multi_index(\n            (spike_clusters_i[:-shift][m], spike_clusters_i[+shift:][m], d),\n            correlograms.shape,\n        )\n\n        # Increment the matching spikes in the correlograms array.\n        _increment(correlograms.ravel(), indices)\n\n        shift += 1\n\n    return _symmetrize_correlograms(correlograms)\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/corr/#element_array_ephys.plotting.corr.acorr", "title": "<code>acorr(spike_times, bin_size, window_size)</code>", "text": "<p>Compute the auto-correlogram of a unit.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike times in seconds.</p> required <code>bin_size</code> <code>float</code> <p>Size of the time bin in seconds.</p> required <code>window_size</code> <code>int</code> <p>Size of the correlogram window in seconds.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: auto-correlogram array (winsize_samples,)</p> Source code in <code>element_array_ephys/plotting/corr.py</code> <pre><code>def acorr(spike_times: np.ndarray, bin_size: float, window_size: int) -&gt; np.ndarray:\n\"\"\"Compute the auto-correlogram of a unit.\n\n    Args:\n        spike_times (np.ndarray): Spike times in seconds.\n        bin_size (float, optional): Size of the time bin in seconds.\n        window_size (int, optional): Size of the correlogram window in seconds.\n\n    Returns:\n        np.ndarray: auto-correlogram array (winsize_samples,)\n    \"\"\"\n    xc = xcorr(\n        spike_times,\n        np.zeros_like(spike_times, dtype=np.int32),\n        bin_size=bin_size,\n        window_size=window_size,\n    )\n    return xc[0, 0, :]\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/probe_level/", "title": "probe_level.py", "text": ""}, {"location": "api/element_array_ephys/plotting/probe_level/#element_array_ephys.plotting.probe_level.plot_raster", "title": "<code>plot_raster(units, spike_times)</code>", "text": "<p>Population raster plots.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>np.ndarray</code> <p>Recorded units.</p> required <code>spike_times</code> <code>np.ndarray</code> <p>Spike timestamps in seconds.</p> required <p>Returns:</p> Type Description <code>matplotlib.figure.Figure</code> <p>matplotlib.figure.Figure: matplotlib figure object showing spikes rasters over time (x-axis in seconds). Each row (y-axis) indicates a single unit.</p> Source code in <code>element_array_ephys/plotting/probe_level.py</code> <pre><code>def plot_raster(units: np.ndarray, spike_times: np.ndarray) -&gt; matplotlib.figure.Figure:\n\"\"\"Population raster plots.\n\n    Args:\n        units (np.ndarray): Recorded units.\n        spike_times (np.ndarray): Spike timestamps in seconds.\n\n    Returns:\n        matplotlib.figure.Figure: matplotlib figure object showing spikes rasters over time (x-axis in seconds). Each row (y-axis) indicates a single unit.\n    \"\"\"\n    units = np.arange(1, len(units) + 1)\n    x = np.hstack(spike_times)\n    y = np.hstack([np.full_like(s, u) for u, s in zip(units, spike_times)])\n    fig, ax = plt.subplots(1, 1, figsize=(32, 8), dpi=100)\n    ax.plot(x, y, \"|\")\n    ax.set(\n        xlabel=\"Time (s)\",\n        ylabel=\"Unit\",\n        xlim=[0 - 0.5, x[-1] + 0.5],\n        ylim=(1, len(units)),\n    )\n    sns.despine()\n    fig.tight_layout()\n\n    return fig\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/probe_level/#element_array_ephys.plotting.probe_level.plot_driftmap", "title": "<code>plot_driftmap(spike_times, spike_depths, colormap='gist_heat_r')</code>", "text": "<p>Plot drift map of unit activity for all units recorded in a given shank of a probe.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike timestamps in seconds.</p> required <code>spike_depths</code> <code>np.ndarray</code> <p>The depth of the electrode where the spike was found in \u03bcm.</p> required <code>colormap</code> <code>str</code> <p>Colormap. Defaults to \"gist_heat_r\".</p> <code>'gist_heat_r'</code> <p>Returns:</p> Type Description <code>matplotlib.figure.Figure</code> <p>matplotlib.figure.Figure: matplotlib figure object for showing population activity for all units over time (x-axis in seconds) according to the spatial depths of the spikes (y-axis in \u03bcm).</p> Source code in <code>element_array_ephys/plotting/probe_level.py</code> <pre><code>def plot_driftmap(\n    spike_times: np.ndarray, spike_depths: np.ndarray, colormap=\"gist_heat_r\"\n) -&gt; matplotlib.figure.Figure:\n\"\"\"Plot drift map of unit activity for all units recorded in a given shank of a probe.\n\n    Args:\n        spike_times (np.ndarray): Spike timestamps in seconds.\n        spike_depths (np.ndarray): The depth of the electrode where the spike was found in \u03bcm.\n        colormap (str, optional): Colormap. Defaults to \"gist_heat_r\".\n\n    Returns:\n        matplotlib.figure.Figure: matplotlib figure object for showing population activity for all units over time (x-axis in seconds) according to the spatial depths of the spikes (y-axis in \u03bcm).\n    \"\"\"\n\n    spike_times = np.hstack(spike_times)\n    spike_depths = np.hstack(spike_depths)\n\n    # Time-depth 2D histogram\n    time_bin_count = 1000\n    depth_bin_count = 200\n\n    spike_bins = np.linspace(0, spike_times.max(), time_bin_count)\n    depth_bins = np.linspace(0, np.nanmax(spike_depths), depth_bin_count)\n\n    spk_count, spk_edges, depth_edges = np.histogram2d(\n        spike_times, spike_depths, bins=[spike_bins, depth_bins]\n    )\n    spk_rates = spk_count / np.mean(np.diff(spike_bins))\n    spk_edges = spk_edges[:-1]\n    depth_edges = depth_edges[:-1]\n\n    # Canvas setup\n    fig = plt.figure(figsize=(12, 5), dpi=200)\n    grid = plt.GridSpec(15, 12)\n\n    ax_cbar = plt.subplot(grid[0, 0:10])\n    ax_driftmap = plt.subplot(grid[2:, 0:10])\n    ax_spkcount = plt.subplot(grid[2:, 10:])\n\n    # Plot main\n    im = ax_driftmap.imshow(\n        spk_rates.T,\n        aspect=\"auto\",\n        cmap=colormap,\n        extent=[spike_bins[0], spike_bins[-1], depth_bins[-1], depth_bins[0]],\n    )\n    # Cosmetic\n    ax_driftmap.invert_yaxis()\n    ax_driftmap.set(\n        xlabel=\"Time (s)\",\n        ylabel=\"Distance from the probe tip ($\\mu$m)\",\n        ylim=[depth_edges[0], depth_edges[-1]],\n    )\n\n    # Colorbar for firing rates\n    cb = fig.colorbar(im, cax=ax_cbar, orientation=\"horizontal\")\n    cb.outline.set_visible(False)\n    cb.ax.xaxis.tick_top()\n    cb.set_label(\"Firing rate (Hz)\")\n    cb.ax.xaxis.set_label_position(\"top\")\n\n    # Plot spike count\n    ax_spkcount.plot(spk_count.sum(axis=0) / 10e3, depth_edges, \"k\")\n    ax_spkcount.set_xlabel(\"Spike count (x$10^3$)\")\n    ax_spkcount.set_yticks([])\n    ax_spkcount.set_ylim(depth_edges[0], depth_edges[-1])\n    sns.despine()\n\n    return fig\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/qc/", "title": "qc.py", "text": ""}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs", "title": "<code>QualityMetricFigs</code>", "text": "<p>         Bases: <code>object</code></p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>class QualityMetricFigs(object):\n    def __init__(\n        self,\n        ephys: types.ModuleType,\n        key: dict = None,\n        scale: float = 1,\n        fig_width=800,\n        amplitude_cutoff_maximum: float = None,\n        presence_ratio_minimum: float = None,\n        isi_violations_maximum: float = None,\n        dark_mode: bool = False,\n    ):\n\"\"\"Initialize QC metric class\n\n        Args:\n            ephys (module): datajoint module with a QualityMetric table\n            key (dict, optional): key from ephys.QualityMetric table. Defaults to None.\n            scale (float, optional): Scale at which to render figure. Defaults to 1.4.\n            fig_width (int, optional): Figure width in pixels. Defaults to 800.\n            amplitude_cutoff_maximum (float, optional): Cutoff for unit amplitude in visualizations. Defaults to None.\n            presence_ratio_minimum (float, optional): Cutoff for presence ratio in visualizations. Defaults to None.\n            isi_violations_maximum (float, optional): Cutoff for isi violations in visualizations. Defaults to None.\n            dark_mode (bool, optional): Set background to black, foreground white. Default False, black on white.\n        \"\"\"\n        self._ephys = ephys\n        self._key = key\n        self._scale = scale\n        self._plots = {}  # Empty default to defer set to dict property below\n        self._fig_width = fig_width\n        self._amplitude_cutoff_max = amplitude_cutoff_maximum\n        self._presence_ratio_min = presence_ratio_minimum\n        self._isi_violations_max = isi_violations_maximum\n        self._dark_mode = dark_mode\n        self._units = pd.DataFrame()  # Empty default\n        self._x_fmt = dict(showgrid=False, zeroline=False, linewidth=2, ticks=\"outside\")\n        self._y_fmt = dict(showgrid=False, linewidth=0, zeroline=True, visible=False)\n        self._no_data_text = \"No data available\"  # What to show when no data in table\n        self._null_series = pd.Series(np.nan)  # What to substitute when no data\n\n    @property\n    def key(self) -&gt; dict:\n\"\"\"Key in ephys.QualityMetrics table\"\"\"\n        return self._key\n\n    @key.setter  # Allows `cls.property = new_item` notation\n    def key(self, key: dict):\n\"\"\"Use class_instance.key = your_key to reset key\"\"\"\n        if key not in self._ephys.QualityMetrics.fetch(\"KEY\"):\n            # If not already full key, check if uniquely identifies entry\n            key = (self._ephys.QualityMetrics &amp; key).fetch1(\"KEY\")\n        self._key = key\n\n    @key.deleter  # Allows `del cls.property` to clear key\n    def key(self):\n\"\"\"Use del class_instance.key to clear key\"\"\"\n        logger.info(\"Cleared key\")\n        self._key = None\n\n    @property\n    def cutoffs(self) -&gt; dict:\n\"\"\"Amplitude, presence ratio, isi violation cutoffs\"\"\"\n        return dict(\n            amplitude_cutoff_maximum=self._amplitude_cutoff_max,\n            presence_ratio_minimum=self._presence_ratio_min,\n            isi_violations_maximum=self._isi_violations_max,\n        )\n\n    @cutoffs.setter\n    def cutoffs(self, cutoff_dict):\n\"\"\"Use class_instance.cutoffs = dict(var=cutoff) to adjust cutoffs\n\n        Args:\n            cutoff_dict (kwargs): Cutoffs to adjust: amplitude_cutoff_maximum,\n                presence_ratio_minimum, and/or isi_violations_maximum\n        \"\"\"\n        self._amplitude_cutoff_max = cutoff_dict.get(\n            \"amplitude_cutoff_maximum\", self._amplitude_cutoff_max\n        )\n        self._presence_ratio_min = cutoff_dict.get(\n            \"presence_ratio_minimum\", self._presence_ratio_min\n        )\n        self._isi_violations_max = cutoff_dict.get(\n            \"isi_violations_maximum\", self._isi_violations_max\n        )\n        _ = self.units\n\n    @property\n    def units(self) -&gt; pd.DataFrame:\n\"\"\"Pandas dataframe of QC metrics\"\"\"\n        if not self._key:\n            return self._null_series\n\n        if self._units.empty:\n            restrictions = [\"TRUE\"]\n            if self._amplitude_cutoff_max:\n                restrictions.append(f\"amplitude_cutoff &lt; {self._amplitude_cutoff_max}\")\n            if self._presence_ratio_min:\n                restrictions.append(f\"presence_ratio &gt; {self._presence_ratio_min}\")\n            if self._isi_violations_max:\n                restrictions.append(f\"isi_violation &lt; {self._isi_violations_max}\")\n            \" AND \".join(restrictions)  # Build restriction from cutoffs\n            return (\n                self._ephys.QualityMetrics\n                * self._ephys.QualityMetrics.Cluster\n                * self._ephys.QualityMetrics.Waveform\n                &amp; self._key\n                &amp; restrictions\n            ).fetch(format=\"frame\")\n\n        return self._units\n\n    def _format_fig(\n        self, fig: go.Figure = None, scale: float = None, ratio: float = 1.0\n    ) -&gt; go.Figure:\n\"\"\"Return formatted figure or apply formatting to existing figure\n\n        Args:\n            fig (go.Figure, optional): Apply formatting to this plotly graph object\n                Figure to apply formatting. Defaults to empty.\n            scale (float, optional): Scale to render figure. Defaults to scale from\n                class init, 1.\n            ratio (float, optional): Figure aspect ratio width/height. Defaults to 1.\n\n        Returns:\n            go.Figure: Formatted figure\n        \"\"\"\n        if not fig:\n            fig = go.Figure()\n        if not scale:\n            scale = self._scale\n\n        width = self._fig_width * scale\n\n        return fig.update_layout(\n            template=\"plotly_dark\" if self._dark_mode else \"simple_white\",\n            width=width,\n            height=width / ratio,\n            margin=dict(l=20 * scale, r=20 * scale, t=40 * scale, b=40 * scale),\n            showlegend=False,\n        )\n\n    def _empty_fig(\n        self, text=\"Select a key to visualize QC metrics\", scale=None\n    ) -&gt; go.Figure:\n\"\"\"Return figure object for when no key is provided\"\"\"\n        if not scale:\n            scale = self._scale\n\n        return (\n            self._format_fig(scale=scale)\n            .add_annotation(text=text, showarrow=False)\n            .update_layout(xaxis=self._y_fmt, yaxis=self._y_fmt)\n        )\n\n    def _plot_metric(\n        self,\n        data: pd.DataFrame,\n        bins: np.ndarray,\n        scale: float = None,\n        fig: go.Figure = None,\n        **trace_kwargs,\n    ) -&gt; go.Figure:\n\"\"\"Plot histogram using bins provided\n\n        Args:\n            data (pd.DataFrame): Data to be plotted, from QC metric\n            bins (np.ndarray): Array of bins to use for histogram\n            scale (float, optional): Scale to render figure. Defaults to scale from\n                class initialization.\n            fig (go.Figure, optional): Add trace to this figure. Defaults to empty\n                formatted figure.\n\n        Returns:\n            go.Figure: Histogram plot\n        \"\"\"\n        if not scale:\n            scale = self._scale\n        if not fig:\n            fig = self._format_fig(scale=scale)\n\n        if not data.isnull().all():\n            histogram, histogram_bins = np.histogram(data, bins=bins, density=True)\n        else:\n            # To quiet divide by zero error when no data\n            histogram, histogram_bins = np.ndarray(0), np.ndarray(0)\n\n        return fig.add_trace(\n            go.Scatter(\n                x=histogram_bins[:-1],\n                y=gaussian_filter1d(histogram, 1),\n                mode=\"lines\",\n                line=dict(color=\"rgb(0, 160, 223)\", width=2 * scale),  # DataJoint Blue\n                hovertemplate=\"%{x:.2f}&lt;br&gt;%{y:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n            ),\n            **trace_kwargs,\n        )\n\n    def get_single_fig(self, fig_name: str, scale: float = None) -&gt; go.Figure:\n\"\"\"Return a single figure of the plots listed in the plot_list property\n\n        Args:\n            fig_name (str): Name of figure to be rendered\n            scale (float, optional): Scale to render fig. Defaults to scale at class init, 1.\n\n        Returns:\n            go.Figure: Histogram plot\n        \"\"\"\n        if not self._key:\n            return self._empty_fig()\n        if not scale:\n            scale = self._scale\n\n        fig_dict = self.plots.get(fig_name, dict()) if self._key else dict()\n        data = fig_dict.get(\"data\", self._null_series)\n        bins = fig_dict.get(\"bins\", np.linspace(0, 0, 0))\n        vline = fig_dict.get(\"vline\", None)\n\n        if data.isnull().all():\n            return self._empty_fig(text=self._no_data_text)\n\n        fig = (\n            self._plot_metric(data=data, bins=bins, scale=scale)\n            .update_layout(xaxis=self._x_fmt, yaxis=self._y_fmt)\n            .update_layout(  # Add title\n                title=dict(text=fig_dict.get(\"xaxis\", \" \"), xanchor=\"center\", x=0.5),\n                font=dict(size=12 * scale),\n            )\n        )\n\n        if vline:\n            fig.add_vline(x=vline, line_width=2 * scale, line_dash=\"dash\")\n\n        return fig\n\n    def get_grid(self, n_columns: int = 4, scale: float = 1.0) -&gt; go.Figure:\n\"\"\"Plot grid of histograms as subplots in go.Figure using n_columns\n\n        Args:\n            n_columns (int, optional): Number of column in grid. Defaults to 4.\n            scale (float, optional): Scale to render fig. Defaults to scale at class init, 1.\n\n        Returns:\n            go.Figure: grid of available plots\n        \"\"\"\n        from plotly.subplots import make_subplots\n\n        if not self._key:\n            return self._empty_fig()\n        if not scale:\n            scale = self._scale\n\n        n_rows = int(np.ceil(len(self.plots) / n_columns))\n\n        fig = self._format_fig(\n            fig=make_subplots(\n                rows=n_rows,\n                cols=n_columns,\n                shared_xaxes=False,\n                shared_yaxes=False,\n                vertical_spacing=(0.5 / n_rows),\n            ),\n            scale=scale,\n            ratio=(n_columns / n_rows),\n        ).update_layout(  # Global title\n            title=dict(text=\"Histograms of Quality Metrics\", xanchor=\"center\", x=0.5),\n            font=dict(size=12 * scale),\n        )\n\n        for idx, plot in enumerate(self._plots.values()):  # Each subplot\n            this_row = int(np.floor(idx / n_columns) + 1)\n            this_col = idx % n_columns + 1\n            data = plot.get(\"data\", self._null_series)\n            vline = plot.get(\"vline\", None)\n            if data.isnull().all():\n                vline = None  # If no data, don't want vline either\n                fig[\"layout\"].update(\n                    annotations=[\n                        dict(\n                            xref=f\"x{idx+1}\",\n                            yref=f\"y{idx+1}\",\n                            text=self._no_data_text,\n                            showarrow=False,\n                        ),\n                    ]\n                )\n            fig = self._plot_metric(  # still need to plot empty to cal y_vals min/max\n                data=data,\n                bins=plot[\"bins\"],\n                fig=fig,\n                row=this_row,\n                col=this_col,\n                scale=scale,\n            )\n            fig.update_xaxes(\n                title=dict(text=plot[\"xaxis\"], font_size=11 * scale),\n                row=this_row,\n                col=this_col,\n            )\n            if vline:\n                y_vals = fig.to_dict()[\"data\"][idx][\"y\"]\n                fig.add_shape(  # Add overlay WRT whole fig\n                    go.layout.Shape(\n                        type=\"line\",\n                        yref=\"paper\",\n                        xref=\"x\",  # relative to subplot x\n                        x0=vline,\n                        y0=min(y_vals),\n                        x1=vline,\n                        y1=max(y_vals),\n                        line=dict(width=2 * scale),\n                    ),\n                    row=this_row,\n                    col=this_col,\n                )\n\n        return fig.update_xaxes(**self._x_fmt).update_yaxes(**self._y_fmt)\n\n    @property\n    def plot_list(self):\n\"\"\"List of plots that can be rendered individually by name or as grid\"\"\"\n        if not self._plots:\n            _ = self.plots\n        return [plot for plot in self._plots]\n\n    @property\n    def plots(self):\n        if not self._plots:\n            self._plots = {\n                \"firing_rate\": {  # If linear, use np.linspace(0, 50, 100)\n                    \"xaxis\": \"Firing rate (log&lt;sub&gt;10&lt;/sub&gt; Hz)\",\n                    \"data\": np.log10(self.units.get(\"firing_rate\", self._null_series)),\n                    \"bins\": np.linspace(-3, 2, 100),\n                },\n                \"presence_ratio\": {\n                    \"xaxis\": \"Presence ratio\",\n                    \"data\": self.units.get(\"presence_ratio\", self._null_series),\n                    \"bins\": np.linspace(0, 1, 100),\n                    \"vline\": 0.9,\n                },\n                \"amp_cutoff\": {\n                    \"xaxis\": \"Amplitude cutoff\",\n                    \"data\": self.units.get(\"amplitude_cutoff\", self._null_series),\n                    \"bins\": np.linspace(0, 0.5, 200),\n                    \"vline\": 0.1,\n                },\n                \"isi_violation\": {  # If linear bins(0, 10, 200). Offset b/c log(0) null\n                    \"xaxis\": \"ISI violations (log&lt;sub&gt;10&lt;/sub&gt;)\",\n                    \"data\": np.log10(\n                        self.units.get(\"isi_violation\", self._null_series) + 1e-5\n                    ),\n                    \"bins\": np.linspace(-6, 2.5, 100),\n                    \"vline\": np.log10(0.5),\n                },\n                \"snr\": {\n                    \"xaxis\": \"SNR\",\n                    \"data\": self.units.get(\"snr\", self._null_series),\n                    \"bins\": np.linspace(0, 10, 100),\n                },\n                \"iso_dist\": {\n                    \"xaxis\": \"Isolation distance\",\n                    \"data\": self.units.get(\"isolation_distance\", self._null_series),\n                    \"bins\": np.linspace(0, 170, 50),\n                },\n                \"d_prime\": {\n                    \"xaxis\": \"d-prime\",\n                    \"data\": self.units.get(\"d_prime\", self._null_series),\n                    \"bins\": np.linspace(0, 15, 50),\n                },\n                \"nn_hit\": {\n                    \"xaxis\": \"Nearest-neighbors hit rate\",\n                    \"data\": self.units.get(\"nn_hit_rate\", self._null_series),\n                    \"bins\": np.linspace(0, 1, 100),\n                },\n            }\n        return self._plots\n\n    @plots.setter\n    def plots(self, new_plot_dict: dict):\n\"\"\"Adds or updates plot item in the set to be rendered.\n\n        plot items are structured as followed: dict with name key, embedded dict with\n            xaxis: string x-axis label\n            data: pandas dataframe to be plotted\n            bins: numpy ndarray of bin cutoffs for histogram\n        \"\"\"\n        _ = self.plots\n        [self._plots.update({k: v}) for k, v in new_plot_dict.items()]\n\n    def remove_plot(self, plot_name):\n\"\"\"Removes an item from the set of plots\"\"\"\n        _ = self._plots.pop(plot_name)\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.__init__", "title": "<code>__init__(ephys, key=None, scale=1, fig_width=800, amplitude_cutoff_maximum=None, presence_ratio_minimum=None, isi_violations_maximum=None, dark_mode=False)</code>", "text": "<p>Initialize QC metric class</p> <p>Parameters:</p> Name Type Description Default <code>ephys</code> <code>module</code> <p>datajoint module with a QualityMetric table</p> required <code>key</code> <code>dict</code> <p>key from ephys.QualityMetric table. Defaults to None.</p> <code>None</code> <code>scale</code> <code>float</code> <p>Scale at which to render figure. Defaults to 1.4.</p> <code>1</code> <code>fig_width</code> <code>int</code> <p>Figure width in pixels. Defaults to 800.</p> <code>800</code> <code>amplitude_cutoff_maximum</code> <code>float</code> <p>Cutoff for unit amplitude in visualizations. Defaults to None.</p> <code>None</code> <code>presence_ratio_minimum</code> <code>float</code> <p>Cutoff for presence ratio in visualizations. Defaults to None.</p> <code>None</code> <code>isi_violations_maximum</code> <code>float</code> <p>Cutoff for isi violations in visualizations. Defaults to None.</p> <code>None</code> <code>dark_mode</code> <code>bool</code> <p>Set background to black, foreground white. Default False, black on white.</p> <code>False</code> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def __init__(\n    self,\n    ephys: types.ModuleType,\n    key: dict = None,\n    scale: float = 1,\n    fig_width=800,\n    amplitude_cutoff_maximum: float = None,\n    presence_ratio_minimum: float = None,\n    isi_violations_maximum: float = None,\n    dark_mode: bool = False,\n):\n\"\"\"Initialize QC metric class\n\n    Args:\n        ephys (module): datajoint module with a QualityMetric table\n        key (dict, optional): key from ephys.QualityMetric table. Defaults to None.\n        scale (float, optional): Scale at which to render figure. Defaults to 1.4.\n        fig_width (int, optional): Figure width in pixels. Defaults to 800.\n        amplitude_cutoff_maximum (float, optional): Cutoff for unit amplitude in visualizations. Defaults to None.\n        presence_ratio_minimum (float, optional): Cutoff for presence ratio in visualizations. Defaults to None.\n        isi_violations_maximum (float, optional): Cutoff for isi violations in visualizations. Defaults to None.\n        dark_mode (bool, optional): Set background to black, foreground white. Default False, black on white.\n    \"\"\"\n    self._ephys = ephys\n    self._key = key\n    self._scale = scale\n    self._plots = {}  # Empty default to defer set to dict property below\n    self._fig_width = fig_width\n    self._amplitude_cutoff_max = amplitude_cutoff_maximum\n    self._presence_ratio_min = presence_ratio_minimum\n    self._isi_violations_max = isi_violations_maximum\n    self._dark_mode = dark_mode\n    self._units = pd.DataFrame()  # Empty default\n    self._x_fmt = dict(showgrid=False, zeroline=False, linewidth=2, ticks=\"outside\")\n    self._y_fmt = dict(showgrid=False, linewidth=0, zeroline=True, visible=False)\n    self._no_data_text = \"No data available\"  # What to show when no data in table\n    self._null_series = pd.Series(np.nan)  # What to substitute when no data\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.key", "title": "<code>key: dict</code>  <code>writable</code> <code>property</code> <code>deletable</code>", "text": "<p>Key in ephys.QualityMetrics table</p>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.cutoffs", "title": "<code>cutoffs: dict</code>  <code>writable</code> <code>property</code>", "text": "<p>Amplitude, presence ratio, isi violation cutoffs</p>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.units", "title": "<code>units: pd.DataFrame</code>  <code>property</code>", "text": "<p>Pandas dataframe of QC metrics</p>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.get_single_fig", "title": "<code>get_single_fig(fig_name, scale=None)</code>", "text": "<p>Return a single figure of the plots listed in the plot_list property</p> <p>Parameters:</p> Name Type Description Default <code>fig_name</code> <code>str</code> <p>Name of figure to be rendered</p> required <code>scale</code> <code>float</code> <p>Scale to render fig. Defaults to scale at class init, 1.</p> <code>None</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Histogram plot</p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def get_single_fig(self, fig_name: str, scale: float = None) -&gt; go.Figure:\n\"\"\"Return a single figure of the plots listed in the plot_list property\n\n    Args:\n        fig_name (str): Name of figure to be rendered\n        scale (float, optional): Scale to render fig. Defaults to scale at class init, 1.\n\n    Returns:\n        go.Figure: Histogram plot\n    \"\"\"\n    if not self._key:\n        return self._empty_fig()\n    if not scale:\n        scale = self._scale\n\n    fig_dict = self.plots.get(fig_name, dict()) if self._key else dict()\n    data = fig_dict.get(\"data\", self._null_series)\n    bins = fig_dict.get(\"bins\", np.linspace(0, 0, 0))\n    vline = fig_dict.get(\"vline\", None)\n\n    if data.isnull().all():\n        return self._empty_fig(text=self._no_data_text)\n\n    fig = (\n        self._plot_metric(data=data, bins=bins, scale=scale)\n        .update_layout(xaxis=self._x_fmt, yaxis=self._y_fmt)\n        .update_layout(  # Add title\n            title=dict(text=fig_dict.get(\"xaxis\", \" \"), xanchor=\"center\", x=0.5),\n            font=dict(size=12 * scale),\n        )\n    )\n\n    if vline:\n        fig.add_vline(x=vline, line_width=2 * scale, line_dash=\"dash\")\n\n    return fig\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.get_grid", "title": "<code>get_grid(n_columns=4, scale=1.0)</code>", "text": "<p>Plot grid of histograms as subplots in go.Figure using n_columns</p> <p>Parameters:</p> Name Type Description Default <code>n_columns</code> <code>int</code> <p>Number of column in grid. Defaults to 4.</p> <code>4</code> <code>scale</code> <code>float</code> <p>Scale to render fig. Defaults to scale at class init, 1.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: grid of available plots</p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def get_grid(self, n_columns: int = 4, scale: float = 1.0) -&gt; go.Figure:\n\"\"\"Plot grid of histograms as subplots in go.Figure using n_columns\n\n    Args:\n        n_columns (int, optional): Number of column in grid. Defaults to 4.\n        scale (float, optional): Scale to render fig. Defaults to scale at class init, 1.\n\n    Returns:\n        go.Figure: grid of available plots\n    \"\"\"\n    from plotly.subplots import make_subplots\n\n    if not self._key:\n        return self._empty_fig()\n    if not scale:\n        scale = self._scale\n\n    n_rows = int(np.ceil(len(self.plots) / n_columns))\n\n    fig = self._format_fig(\n        fig=make_subplots(\n            rows=n_rows,\n            cols=n_columns,\n            shared_xaxes=False,\n            shared_yaxes=False,\n            vertical_spacing=(0.5 / n_rows),\n        ),\n        scale=scale,\n        ratio=(n_columns / n_rows),\n    ).update_layout(  # Global title\n        title=dict(text=\"Histograms of Quality Metrics\", xanchor=\"center\", x=0.5),\n        font=dict(size=12 * scale),\n    )\n\n    for idx, plot in enumerate(self._plots.values()):  # Each subplot\n        this_row = int(np.floor(idx / n_columns) + 1)\n        this_col = idx % n_columns + 1\n        data = plot.get(\"data\", self._null_series)\n        vline = plot.get(\"vline\", None)\n        if data.isnull().all():\n            vline = None  # If no data, don't want vline either\n            fig[\"layout\"].update(\n                annotations=[\n                    dict(\n                        xref=f\"x{idx+1}\",\n                        yref=f\"y{idx+1}\",\n                        text=self._no_data_text,\n                        showarrow=False,\n                    ),\n                ]\n            )\n        fig = self._plot_metric(  # still need to plot empty to cal y_vals min/max\n            data=data,\n            bins=plot[\"bins\"],\n            fig=fig,\n            row=this_row,\n            col=this_col,\n            scale=scale,\n        )\n        fig.update_xaxes(\n            title=dict(text=plot[\"xaxis\"], font_size=11 * scale),\n            row=this_row,\n            col=this_col,\n        )\n        if vline:\n            y_vals = fig.to_dict()[\"data\"][idx][\"y\"]\n            fig.add_shape(  # Add overlay WRT whole fig\n                go.layout.Shape(\n                    type=\"line\",\n                    yref=\"paper\",\n                    xref=\"x\",  # relative to subplot x\n                    x0=vline,\n                    y0=min(y_vals),\n                    x1=vline,\n                    y1=max(y_vals),\n                    line=dict(width=2 * scale),\n                ),\n                row=this_row,\n                col=this_col,\n            )\n\n    return fig.update_xaxes(**self._x_fmt).update_yaxes(**self._y_fmt)\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.plot_list", "title": "<code>plot_list</code>  <code>property</code>", "text": "<p>List of plots that can be rendered individually by name or as grid</p>"}, {"location": "api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.remove_plot", "title": "<code>remove_plot(plot_name)</code>", "text": "<p>Removes an item from the set of plots</p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def remove_plot(self, plot_name):\n\"\"\"Removes an item from the set of plots\"\"\"\n    _ = self._plots.pop(plot_name)\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/unit_level/", "title": "unit_level.py", "text": ""}, {"location": "api/element_array_ephys/plotting/unit_level/#element_array_ephys.plotting.unit_level.plot_waveform", "title": "<code>plot_waveform(waveform, sampling_rate)</code>", "text": "<p>Plot unit waveform.</p> <p>Parameters:</p> Name Type Description Default <code>waveform</code> <code>np.ndarray</code> <p>Amplitude of a spike waveform in \u03bcV.</p> required <code>sampling_rate</code> <code>float</code> <p>Sampling rate in kHz.</p> required <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Plotly figure object for showing the amplitude of a waveform (y-axis in \u03bcV) over time (x-axis).</p> Source code in <code>element_array_ephys/plotting/unit_level.py</code> <pre><code>def plot_waveform(waveform: np.ndarray, sampling_rate: float) -&gt; go.Figure:\n\"\"\"Plot unit waveform.\n\n    Args:\n        waveform (np.ndarray): Amplitude of a spike waveform in \u03bcV.\n        sampling_rate (float): Sampling rate in kHz.\n\n    Returns:\n        go.Figure: Plotly figure object for showing the amplitude of a waveform (y-axis in \u03bcV) over time (x-axis).\n    \"\"\"\n    waveform_df = pd.DataFrame(data={\"waveform\": waveform})\n    waveform_df[\"timestamp\"] = waveform_df.index / sampling_rate\n\n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=waveform_df[\"timestamp\"],\n            y=waveform_df[\"waveform\"],\n            mode=\"lines\",\n            line=dict(color=\"rgb(0, 160, 223)\", width=2),  # DataJoint Blue\n            hovertemplate=\"%{y:.2f} \u03bcV&lt;br&gt;\" + \"%{x:.2f} ms&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    fig.update_layout(\n        title=\"Avg. waveform\",\n        xaxis_title=\"Time (ms)\",\n        yaxis_title=\"Voltage (\u03bcV)\",\n        template=\"simple_white\",\n        width=350,\n        height=350,\n    )\n    return fig\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/unit_level/#element_array_ephys.plotting.unit_level.plot_auto_correlogram", "title": "<code>plot_auto_correlogram(spike_times, bin_size=0.001, window_size=1)</code>", "text": "<p>Plot the auto-correlogram of a unit.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike timestamps in seconds</p> required <code>bin_size</code> <code>float</code> <p>Size of the time bin (lag) in seconds. Defaults to 0.001.</p> <code>0.001</code> <code>window_size</code> <code>int</code> <p>Size of the correlogram window in seconds. Defaults to 1 (\u00b1 500ms)</p> <code>1</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Plotly figure object for showing</p> <code>go.Figure</code> <p>counts (y-axis) over time lags (x-axis).</p> Source code in <code>element_array_ephys/plotting/unit_level.py</code> <pre><code>def plot_auto_correlogram(\n    spike_times: np.ndarray, bin_size: float = 0.001, window_size: int = 1\n) -&gt; go.Figure:\n\"\"\"Plot the auto-correlogram of a unit.\n\n    Args:\n        spike_times (np.ndarray): Spike timestamps in seconds\n        bin_size (float, optional): Size of the time bin (lag) in seconds. Defaults to 0.001.\n        window_size (int, optional): Size of the correlogram window in seconds. Defaults to 1 (\u00b1 500ms)\n\n    Returns:\n        go.Figure: Plotly figure object for showing\n        counts (y-axis) over time lags (x-axis).\n    \"\"\"\n    from .corr import acorr\n\n    correlogram = acorr(\n        spike_times=spike_times, bin_size=bin_size, window_size=window_size\n    )\n    df = pd.DataFrame(\n        data={\"correlogram\": correlogram},\n        index=pd.RangeIndex(\n            start=-(window_size * 1e3) / 2,\n            stop=(window_size * 1e3) / 2 + bin_size * 1e3,\n            step=bin_size * 1e3,\n        ),\n    )\n    df[\"lags\"] = df.index  # in ms\n\n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=df[\"lags\"],\n            y=df[\"correlogram\"],\n            mode=\"lines\",\n            line=dict(color=\"black\", width=1),\n            hovertemplate=\"%{y}&lt;br&gt;\" + \"%{x:.2f} ms&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    fig.update_layout(\n        title=\"Auto Correlogram\",\n        xaxis_title=\"Lags (ms)\",\n        yaxis_title=\"Count\",\n        template=\"simple_white\",\n        width=350,\n        height=350,\n        yaxis_range=[0, None],\n    )\n    return fig\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/unit_level/#element_array_ephys.plotting.unit_level.plot_depth_waveforms", "title": "<code>plot_depth_waveforms(ephys, unit_key, y_range=60)</code>", "text": "<p>Plot the peak waveform (in red) and waveforms from its neighboring sites on a spatial coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>ephys</code> <code>Module</code> <p>Imported ephys module object.</p> required <code>unit_key</code> <code>dict[str, Any]</code> <p>Key dictionary from ephys.CuratedClustering.Unit table.</p> required <code>y_range</code> <code>float</code> <p>Vertical range to show waveforms relative to the peak waveform in \u03bcm. Defaults to 60.</p> <code>60</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>element_array_ephys/plotting/unit_level.py</code> <pre><code>def plot_depth_waveforms(\n    ephys: Module,\n    unit_key: dict[str, Any],\n    y_range: float = 60,\n) -&gt; go.Figure:\n\"\"\"Plot the peak waveform (in red) and waveforms from its neighboring sites on a spatial coordinate.\n\n    Args:\n        ephys (Module): Imported ephys module object.\n        unit_key (dict[str, Any]): Key dictionary from ephys.CuratedClustering.Unit table.\n        y_range (float, optional): Vertical range to show waveforms relative to the peak waveform in \u03bcm. Defaults to 60.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n\n    sampling_rate = (ephys.EphysRecording &amp; unit_key).fetch1(\n        \"sampling_rate\"\n    ) / 1e3  # in kHz\n\n    probe_type, peak_electrode = (ephys.CuratedClustering.Unit &amp; unit_key).fetch1(\n        \"probe_type\", \"electrode\"\n    )  # electrode where the peak waveform was found\n\n    electrodes, coord_y = (\n        probe.ProbeType.Electrode &amp; f\"probe_type='{probe_type}'\"\n    ).fetch(\"electrode\", \"y_coord\")\n\n    peak_electrode_shank = (\n        probe.ProbeType.Electrode\n        &amp; f\"probe_type='{probe_type}'\"\n        &amp; f\"electrode={peak_electrode}\"\n    ).fetch1(\"shank\")\n\n    peak_coord_y = coord_y[electrodes == peak_electrode][0]\n\n    coord_ylim_low = (\n        coord_y.min()\n        if (peak_coord_y - y_range) &lt;= coord_y.min()\n        else peak_coord_y - y_range\n    )\n    coord_ylim_high = (\n        coord_y.max()\n        if (peak_coord_y + y_range) &gt;= coord_y.max()\n        else peak_coord_y + y_range\n    )\n\n    tbl = (\n        (probe.ProbeType.Electrode)\n        &amp; f\"probe_type = '{probe_type}'\"\n        &amp; f\"y_coord BETWEEN {coord_ylim_low} AND {coord_ylim_high}\"\n        &amp; f\"shank={peak_electrode_shank}\"\n    )\n    electrodes_to_plot, x_coords, y_coords = tbl.fetch(\n        \"electrode\", \"x_coord\", \"y_coord\"\n    )\n\n    coords = np.array([x_coords, y_coords]).T  # x, y coordinates\n\n    waveforms = (\n        ephys.WaveformSet.Waveform\n        &amp; unit_key\n        &amp; f\"electrode IN {tuple(electrodes_to_plot)}\"\n    ).fetch(\"waveform_mean\")\n    waveforms = np.stack(waveforms)  # all mean waveforms of a given neuron\n\n    x_min, x_max = np.min(coords[:, 0]), np.max(coords[:, 0])\n    y_min, y_max = np.min(coords[:, 1]), np.max(coords[:, 1])\n\n    # Spacing between recording sites (in um)\n    x_inc = np.diff(np.sort((coords[coords[:, 1] == coords[0, 1]][:, 0]))).mean() / 2\n    y_inc = np.diff(np.sort((coords[coords[:, 0] == coords[0, 0]][:, 1]))).mean() / 2\n    time = np.arange(waveforms.shape[1]) / sampling_rate\n\n    x_scale_factor = x_inc / (time[-1] + 1 / sampling_rate)  # correspond to 1 ms\n    time_scaled = time * x_scale_factor\n\n    wf_amps = waveforms.max(axis=1) - waveforms.min(axis=1)\n    max_amp = wf_amps.max()\n    y_scale_factor = y_inc / max_amp\n\n    unique_x_loc = np.sort(np.unique(coords[:, 0]))\n    xtick_label = [str(int(x)) for x in unique_x_loc]\n    xtick_loc = time_scaled[len(time_scaled) // 2 + 1] + unique_x_loc\n\n    # Plot figure\n    fig = go.Figure()\n    for electrode, wf, coord in zip(electrodes_to_plot, waveforms, coords):\n        wf_scaled = wf * y_scale_factor\n        wf_scaled -= wf_scaled.mean()\n        color = \"red\" if electrode == peak_electrode else \"rgb(51, 76.5, 204)\"\n\n        fig.add_trace(\n            go.Scatter(\n                x=time_scaled + coord[0],\n                y=wf_scaled + coord[1],\n                mode=\"lines\",\n                line=dict(color=color, width=1.5),\n                hovertemplate=f\"electrode {electrode}&lt;br&gt;\"\n                + f\"x ={coord[0]: .0f} \u03bcm&lt;br&gt;\"\n                + f\"y ={coord[1]: .0f} \u03bcm&lt;extra&gt;&lt;/extra&gt;\",\n            )\n        )\n        fig.update_layout(\n            title=\"Depth Waveforms\",\n            xaxis_title=\"Electrode position (\u03bcm)\",\n            yaxis_title=\"Distance from the probe tip (\u03bcm)\",\n            template=\"simple_white\",\n            width=400,\n            height=600,\n            xaxis_range=[x_min - x_inc / 2, x_max + x_inc * 1.2],\n            yaxis_range=[y_min - y_inc * 2, y_max + y_inc * 2],\n        )\n\n    fig.update_layout(showlegend=False)\n    fig.update_xaxes(tickvals=xtick_loc, ticktext=xtick_label)\n\n    # Add a scale bar\n    x0 = xtick_loc[0] - (x_scale_factor * 1.5)\n    y0 = y_min - (y_inc * 1.5)\n\n    fig.add_trace(\n        go.Scatter(\n            x=[x0, x0 + x_scale_factor],\n            y=[y0, y0],\n            mode=\"lines\",\n            line=dict(color=\"black\", width=2),\n            hovertemplate=\"1 ms&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=[x0, x0],\n            y=[y0, y0 + y_inc],\n            mode=\"lines\",\n            line=dict(color=\"black\", width=2),\n            hovertemplate=f\"{max_amp: .2f} \u03bcV&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    return fig\n</code></pre>"}, {"location": "api/element_array_ephys/plotting/widget/", "title": "widget.py", "text": ""}, {"location": "api/element_array_ephys/readers/kilosort/", "title": "kilosort.py", "text": ""}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort", "title": "<code>Kilosort</code>", "text": "Source code in <code>element_array_ephys/readers/kilosort.py</code> <pre><code>class Kilosort:\n    _kilosort_core_files = [\n        \"params.py\",\n        \"amplitudes.npy\",\n        \"channel_map.npy\",\n        \"channel_positions.npy\",\n        \"pc_features.npy\",\n        \"pc_feature_ind.npy\",\n        \"similar_templates.npy\",\n        \"spike_templates.npy\",\n        \"spike_times.npy\",\n        \"template_features.npy\",\n        \"template_feature_ind.npy\",\n        \"templates.npy\",\n        \"templates_ind.npy\",\n        \"whitening_mat.npy\",\n        \"whitening_mat_inv.npy\",\n        \"spike_clusters.npy\",\n    ]\n\n    _kilosort_additional_files = [\n        \"spike_times_sec.npy\",\n        \"spike_times_sec_adj.npy\",\n        \"cluster_groups.csv\",\n        \"cluster_KSLabel.tsv\",\n    ]\n\n    kilosort_files = _kilosort_core_files + _kilosort_additional_files\n\n    def __init__(self, kilosort_dir):\n        self._kilosort_dir = pathlib.Path(kilosort_dir)\n        self._files = {}\n        self._data = None\n        self._clusters = None\n\n        self.validate()\n\n        params_filepath = kilosort_dir / \"params.py\"\n        self._info = {\n            \"time_created\": datetime.fromtimestamp(params_filepath.stat().st_ctime),\n            \"time_modified\": datetime.fromtimestamp(params_filepath.stat().st_mtime),\n        }\n\n    @property\n    def data(self):\n        if self._data is None:\n            self._load()\n        return self._data\n\n    @property\n    def info(self):\n        return self._info\n\n    def validate(self):\n\"\"\"\n        Check if this is a valid set of kilosort outputs - i.e. all crucial files exist\n        \"\"\"\n        missing_files = []\n        for f in Kilosort._kilosort_core_files:\n            full_path = self._kilosort_dir / f\n            if not full_path.exists():\n                missing_files.append(f)\n        if missing_files:\n            raise FileNotFoundError(\n                f\"Kilosort files missing in ({self._kilosort_dir}):\" f\" {missing_files}\"\n            )\n\n    def _load(self):\n        self._data = {}\n        for kilosort_filename in Kilosort.kilosort_files:\n            kilosort_filepath = self._kilosort_dir / kilosort_filename\n\n            if not kilosort_filepath.exists():\n                log.debug(\"skipping {} - does not exist\".format(kilosort_filepath))\n                continue\n\n            base, ext = path.splitext(kilosort_filename)\n            self._files[base] = kilosort_filepath\n\n            if kilosort_filename == \"params.py\":\n                log.debug(\"loading params.py {}\".format(kilosort_filepath))\n                # params.py is a 'key = val' file\n                params = {}\n                for line in open(kilosort_filepath, \"r\").readlines():\n                    k, v = line.strip(\"\\n\").split(\"=\")\n                    params[k.strip()] = convert_to_number(v.strip())\n                log.debug(\"params: {}\".format(params))\n                self._data[base] = params\n\n            if ext == \".npy\":\n                log.debug(\"loading npy {}\".format(kilosort_filepath))\n                d = np.load(\n                    kilosort_filepath,\n                    mmap_mode=\"r\",\n                    allow_pickle=False,\n                    fix_imports=False,\n                )\n                self._data[base] = (\n                    np.reshape(d, d.shape[0]) if d.ndim == 2 and d.shape[1] == 1 else d\n                )\n\n        self._data[\"channel_map\"] = self._data[\"channel_map\"].flatten()\n\n        # Read the Cluster Groups\n        for cluster_pattern, cluster_col_name in zip(\n            [\"cluster_group.*\", \"cluster_KSLabel.*\"], [\"group\", \"KSLabel\"]\n        ):\n            try:\n                cluster_file = next(self._kilosort_dir.glob(cluster_pattern))\n            except StopIteration:\n                pass\n            else:\n                cluster_file_suffix = cluster_file.suffix\n                assert cluster_file_suffix in (\".tsv\", \".xlsx\")\n                break\n        else:\n            raise FileNotFoundError(\n                'Neither \"cluster_groups\" nor \"cluster_KSLabel\" file found!'\n            )\n\n        if cluster_file_suffix == \".tsv\":\n            df = pd.read_csv(cluster_file, sep=\"\\t\", header=0)\n        elif cluster_file_suffix == \".xlsx\":\n            df = pd.read_excel(cluster_file, engine=\"openpyxl\")\n        else:\n            df = pd.read_csv(cluster_file, delimiter=\"\\t\")\n\n        self._data[\"cluster_groups\"] = np.array(df[cluster_col_name].values)\n        self._data[\"cluster_ids\"] = np.array(df[\"cluster_id\"].values)\n\n    def get_best_channel(self, unit):\n        template_idx = self.data[\"spike_templates\"][\n            np.where(self.data[\"spike_clusters\"] == unit)[0][0]\n        ]\n        channel_templates = self.data[\"templates\"][template_idx, :, :]\n        max_channel_idx = np.abs(channel_templates).max(axis=0).argmax()\n        max_channel = self.data[\"channel_map\"][max_channel_idx]\n\n        return max_channel, max_channel_idx\n\n    def extract_spike_depths(self):\n\"\"\"Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m\"\"\"\n\n        if \"pc_features\" in self.data:\n            ycoords = self.data[\"channel_positions\"][:, 1]\n            pc_features = self.data[\"pc_features\"][:, 0, :]  # 1st PC only\n            pc_features = np.where(pc_features &lt; 0, 0, pc_features)\n\n            # ---- compute center of mass of these features (spike depths) ----\n\n            # which channels for each spike?\n            spk_feature_ind = self.data[\"pc_feature_ind\"][\n                self.data[\"spike_templates\"], :\n            ]\n            # ycoords of those channels?\n            spk_feature_ycoord = ycoords[spk_feature_ind]\n            # center of mass is sum(coords.*features)/sum(features)\n            self._data[\"spike_depths\"] = np.sum(\n                spk_feature_ycoord * pc_features**2, axis=1\n            ) / np.sum(pc_features**2, axis=1)\n        else:\n            self._data[\"spike_depths\"] = None\n\n        # ---- extract spike sites ----\n        max_site_ind = np.argmax(np.abs(self.data[\"templates\"]).max(axis=1), axis=1)\n        spike_site_ind = max_site_ind[self.data[\"spike_templates\"]]\n        self._data[\"spike_sites\"] = self.data[\"channel_map\"][spike_site_ind]\n</code></pre>"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.validate", "title": "<code>validate()</code>", "text": "<p>Check if this is a valid set of kilosort outputs - i.e. all crucial files exist</p> Source code in <code>element_array_ephys/readers/kilosort.py</code> <pre><code>def validate(self):\n\"\"\"\n    Check if this is a valid set of kilosort outputs - i.e. all crucial files exist\n    \"\"\"\n    missing_files = []\n    for f in Kilosort._kilosort_core_files:\n        full_path = self._kilosort_dir / f\n        if not full_path.exists():\n            missing_files.append(f)\n    if missing_files:\n        raise FileNotFoundError(\n            f\"Kilosort files missing in ({self._kilosort_dir}):\" f\" {missing_files}\"\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.extract_spike_depths", "title": "<code>extract_spike_depths()</code>", "text": "<p>Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m</p> Source code in <code>element_array_ephys/readers/kilosort.py</code> <pre><code>def extract_spike_depths(self):\n\"\"\"Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m\"\"\"\n\n    if \"pc_features\" in self.data:\n        ycoords = self.data[\"channel_positions\"][:, 1]\n        pc_features = self.data[\"pc_features\"][:, 0, :]  # 1st PC only\n        pc_features = np.where(pc_features &lt; 0, 0, pc_features)\n\n        # ---- compute center of mass of these features (spike depths) ----\n\n        # which channels for each spike?\n        spk_feature_ind = self.data[\"pc_feature_ind\"][\n            self.data[\"spike_templates\"], :\n        ]\n        # ycoords of those channels?\n        spk_feature_ycoord = ycoords[spk_feature_ind]\n        # center of mass is sum(coords.*features)/sum(features)\n        self._data[\"spike_depths\"] = np.sum(\n            spk_feature_ycoord * pc_features**2, axis=1\n        ) / np.sum(pc_features**2, axis=1)\n    else:\n        self._data[\"spike_depths\"] = None\n\n    # ---- extract spike sites ----\n    max_site_ind = np.argmax(np.abs(self.data[\"templates\"]).max(axis=1), axis=1)\n    spike_site_ind = max_site_ind[self.data[\"spike_templates\"]]\n    self._data[\"spike_sites\"] = self.data[\"channel_map\"][spike_site_ind]\n</code></pre>"}, {"location": "api/element_array_ephys/readers/kilosort_triggering/", "title": "kilosort_triggering.py", "text": ""}, {"location": "api/element_array_ephys/readers/kilosort_triggering/#element_array_ephys.readers.kilosort_triggering.SGLXKilosortPipeline", "title": "<code>SGLXKilosortPipeline</code>", "text": "<p>An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline  for one Neuropixels probe in one recording session using the Spike GLX acquisition software.</p> <p>Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting</p> Source code in <code>element_array_ephys/readers/kilosort_triggering.py</code> <pre><code>class SGLXKilosortPipeline:\n\"\"\"\n    An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline\n     for one Neuropixels probe in one recording session using the Spike GLX acquisition software.\n\n    Primarily calling routines specified from:\n    https://github.com/jenniferColonell/ecephys_spike_sorting\n    \"\"\"\n\n    _modules = [\n        \"kilosort_helper\",\n        \"kilosort_postprocessing\",\n        \"noise_templates\",\n        \"mean_waveforms\",\n        \"quality_metrics\",\n    ]\n\n    _default_catgt_params = {\n        \"catGT_car_mode\": \"gblcar\",\n        \"catGT_loccar_min_um\": 40,\n        \"catGT_loccar_max_um\": 160,\n        \"catGT_cmd_string\": \"-prb_fld -out_prb_fld -gfix=0.4,0.10,0.02\",\n        \"ni_present\": False,\n        \"ni_extract_string\": \"-XA=0,1,3,500 -iXA=1,3,3,0  -XD=-1,1,50 -XD=-1,2,1.7 -XD=-1,3,5 -iXD=-1,3,5\",\n    }\n\n    _input_json_args = list(inspect.signature(createInputJson).parameters)\n\n    def __init__(\n        self,\n        npx_input_dir: str,\n        ks_output_dir: str,\n        params: dict,\n        KS2ver: str,\n        run_CatGT=False,\n        ni_present=False,\n        ni_extract_string=None,\n    ):\n        self._npx_input_dir = pathlib.Path(npx_input_dir)\n\n        self._ks_output_dir = pathlib.Path(ks_output_dir)\n        self._ks_output_dir.mkdir(parents=True, exist_ok=True)\n\n        self._params = params\n        self._KS2ver = KS2ver\n        self._run_CatGT = run_CatGT\n        self._run_CatGT = run_CatGT\n        self._default_catgt_params[\"ni_present\"] = ni_present\n        self._default_catgt_params[\"ni_extract_string\"] = (\n            ni_extract_string or self._default_catgt_params[\"ni_extract_string\"]\n        )\n\n        self._json_directory = self._ks_output_dir / \"json_configs\"\n        self._json_directory.mkdir(parents=True, exist_ok=True)\n\n        self._module_input_json = (\n            self._json_directory / f\"{self._npx_input_dir.name}-input.json\"\n        )\n        self._module_logfile = (\n            self._json_directory / f\"{self._npx_input_dir.name}-run_modules-log.txt\"\n        )\n\n        self._CatGT_finished = False\n        self.ks_input_params = None\n        self._modules_input_hash = None\n        self._modules_input_hash_fp = None\n\n    def parse_input_filename(self):\n        meta_filename = next(self._npx_input_dir.glob(\"*.ap.meta\")).name\n        match = re.search(\"(.*)_g(\\d)_t(\\d+|cat)\\.imec(\\d?)\\.ap\\.meta\", meta_filename)\n        session_str, gate_str, trigger_str, probe_str = match.groups()\n        return session_str, gate_str, trigger_str, probe_str or \"0\"\n\n    def generate_CatGT_input_json(self):\n        if not self._run_CatGT:\n            print(\"run_CatGT is set to False, skipping...\")\n            return\n\n        session_str, gate_str, trig_str, probe_str = self.parse_input_filename()\n\n        first_trig, last_trig = SpikeGLX_utils.ParseTrigStr(\n            \"start,end\", probe_str, gate_str, self._npx_input_dir.as_posix()\n        )\n        trigger_str = repr(first_trig) + \",\" + repr(last_trig)\n\n        self._catGT_input_json = (\n            self._json_directory / f\"{session_str}{probe_str}_CatGT-input.json\"\n        )\n\n        catgt_params = {\n            k: self._params.get(k, v) for k, v in self._default_catgt_params.items()\n        }\n\n        ni_present = catgt_params.pop(\"ni_present\")\n        ni_extract_string = catgt_params.pop(\"ni_extract_string\")\n\n        catgt_params[\"catGT_stream_string\"] = \"-ap -ni\" if ni_present else \"-ap\"\n        sync_extract = \"-SY=\" + probe_str + \",-1,6,500\"\n        extract_string = sync_extract + (f\" {ni_extract_string}\" if ni_present else \"\")\n        catgt_params[\"catGT_cmd_string\"] += f\" {extract_string}\"\n\n        input_meta_fullpath, continuous_file = self._get_raw_data_filepaths()\n\n        # create symbolic link to the actual data files - as CatGT expects files to follow a certain naming convention\n        continuous_file_symlink = (\n            continuous_file.parent\n            / f\"{session_str}_g{gate_str}\"\n            / f\"{session_str}_g{gate_str}_imec{probe_str}\"\n            / f\"{session_str}_g{gate_str}_t{trig_str}.imec{probe_str}.ap.bin\"\n        )\n        continuous_file_symlink.parent.mkdir(parents=True, exist_ok=True)\n        if continuous_file_symlink.exists():\n            continuous_file_symlink.unlink()\n        continuous_file_symlink.symlink_to(continuous_file)\n        input_meta_fullpath_symlink = (\n            input_meta_fullpath.parent\n            / f\"{session_str}_g{gate_str}\"\n            / f\"{session_str}_g{gate_str}_imec{probe_str}\"\n            / f\"{session_str}_g{gate_str}_t{trig_str}.imec{probe_str}.ap.meta\"\n        )\n        input_meta_fullpath_symlink.parent.mkdir(parents=True, exist_ok=True)\n        if input_meta_fullpath_symlink.exists():\n            input_meta_fullpath_symlink.unlink()\n        input_meta_fullpath_symlink.symlink_to(input_meta_fullpath)\n\n        createInputJson(\n            self._catGT_input_json.as_posix(),\n            KS2ver=self._KS2ver,\n            npx_directory=self._npx_input_dir.as_posix(),\n            spikeGLX_data=True,\n            catGT_run_name=session_str,\n            gate_string=gate_str,\n            trigger_string=trigger_str,\n            probe_string=probe_str,\n            continuous_file=continuous_file.as_posix(),\n            input_meta_path=input_meta_fullpath.as_posix(),\n            extracted_data_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_tmp=self._ks_output_dir.as_posix(),\n            kilosort_repository=_get_kilosort_repository(self._KS2ver),\n            **{k: v for k, v in catgt_params.items() if k in self._input_json_args},\n        )\n\n    def run_CatGT(self, force_rerun=False):\n        if self._run_CatGT and (not self._CatGT_finished or force_rerun):\n            self.generate_CatGT_input_json()\n\n            print(\"---- Running CatGT ----\")\n            catGT_input_json = self._catGT_input_json.as_posix()\n            catGT_output_json = catGT_input_json.replace(\n                \"CatGT-input.json\", \"CatGT-output.json\"\n            )\n\n            command = (\n                sys.executable\n                + \" -W ignore -m ecephys_spike_sorting.modules.\"\n                + \"catGT_helper\"\n                + \" --input_json \"\n                + catGT_input_json\n                + \" --output_json \"\n                + catGT_output_json\n            )\n            subprocess.check_call(command.split(\" \"))\n\n            self._CatGT_finished = True\n\n    def generate_modules_input_json(self):\n        session_str, _, _, probe_str = self.parse_input_filename()\n        self._module_input_json = (\n            self._json_directory / f\"{session_str}_imec{probe_str}-input.json\"\n        )\n\n        input_meta_fullpath, continuous_file = self._get_raw_data_filepaths()\n\n        params = {}\n        for k, v in self._params.items():\n            value = str(v) if isinstance(v, list) else v\n            if f\"ks_{k}\" in self._input_json_args:\n                params[f\"ks_{k}\"] = value\n            if k in self._input_json_args:\n                params[k] = value\n\n        self.ks_input_params = createInputJson(\n            self._module_input_json.as_posix(),\n            KS2ver=self._KS2ver,\n            npx_directory=self._npx_input_dir.as_posix(),\n            spikeGLX_data=True,\n            continuous_file=continuous_file.as_posix(),\n            input_meta_path=input_meta_fullpath.as_posix(),\n            extracted_data_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_tmp=self._ks_output_dir.as_posix(),\n            ks_make_copy=True,\n            noise_template_use_rf=self._params.get(\"noise_template_use_rf\", False),\n            c_Waves_snr_um=self._params.get(\"c_Waves_snr_um\", 160),\n            qm_isi_thresh=self._params.get(\"refPerMS\", 2.0) / 1000,\n            kilosort_repository=_get_kilosort_repository(self._KS2ver),\n            **params,\n        )\n\n        self._modules_input_hash = dict_to_uuid(dict(self._params, KS2ver=self._KS2ver))\n\n    def run_modules(self, modules_to_run=None):\n        if self._run_CatGT and not self._CatGT_finished:\n            self.run_CatGT()\n\n        print(\"---- Running Modules ----\")\n        self.generate_modules_input_json()\n        module_input_json = self._module_input_json.as_posix()\n        module_logfile = self._module_logfile.as_posix()\n\n        modules = modules_to_run or self._modules\n\n        for module in modules:\n            module_status = self._get_module_status(module)\n            if module_status[\"completion_time\"] is not None:\n                continue\n\n            module_output_json = self._get_module_output_json_filename(module)\n            command = (\n                sys.executable\n                + \" -W ignore -m ecephys_spike_sorting.modules.\"\n                + module\n                + \" --input_json \"\n                + module_input_json\n                + \" --output_json \"\n                + module_output_json\n            )\n\n            start_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": None,\n                        \"duration\": None,\n                    }\n                }\n            )\n            with open(module_logfile, \"a\") as f:\n                subprocess.check_call(command.split(\" \"), stdout=f)\n            completion_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": completion_time,\n                        \"duration\": (completion_time - start_time).total_seconds(),\n                    }\n                }\n            )\n\n        self._update_total_duration()\n\n    def _get_raw_data_filepaths(self):\n        session_str, gate_str, _, probe_str = self.parse_input_filename()\n\n        if self._CatGT_finished:\n            catGT_dest = self._ks_output_dir\n            run_str = session_str + \"_g\" + gate_str\n            run_folder = \"catgt_\" + run_str\n            prb_folder = run_str + \"_imec\" + probe_str\n            data_directory = catGT_dest / run_folder / prb_folder\n        else:\n            data_directory = self._npx_input_dir\n        try:\n            meta_fp = next(data_directory.glob(f\"{session_str}*.ap.meta\"))\n            bin_fp = next(data_directory.glob(f\"{session_str}*.ap.bin\"))\n        except StopIteration:\n            raise RuntimeError(\n                f\"No ap meta/bin files found in {data_directory} - CatGT error?\"\n            )\n\n        return meta_fp, bin_fp\n\n    def _update_module_status(self, updated_module_status={}):\n        if self._modules_input_hash is None:\n            raise RuntimeError('\"generate_modules_input_json()\" not yet performed!')\n\n        self._modules_input_hash_fp = (\n            self._json_directory / f\".{self._modules_input_hash}.json\"\n        )\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            modules_status = {**modules_status, **updated_module_status}\n        else:\n            # handle cases of processing rerun on different parameters (the hash changes)\n            # delete outdated files\n            [\n                f.unlink()\n                for f in self._json_directory.glob(\"*\")\n                if f.is_file() and f.name != self._module_input_json.name\n            ]\n\n            modules_status = {\n                module: {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n                for module in self._modules\n            }\n        with open(self._modules_input_hash_fp, \"w\") as f:\n            json.dump(modules_status, f, default=str)\n\n    def _get_module_status(self, module):\n        if self._modules_input_hash_fp is None:\n            self._update_module_status()\n\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            if modules_status[module][\"completion_time\"] is None:\n                # additional logic to read from the \"-output.json\" file for this module as well\n                # handle cases where the module has finished successfully,\n                # but the \"_modules_input_hash_fp\" is not updated (for whatever reason),\n                # resulting in this module not registered as completed in the \"_modules_input_hash_fp\"\n                module_output_json_fp = pathlib.Path(\n                    self._get_module_output_json_filename(module)\n                )\n                if module_output_json_fp.exists():\n                    with open(module_output_json_fp) as f:\n                        module_run_output = json.load(f)\n                    modules_status[module][\"duration\"] = module_run_output[\n                        \"execution_time\"\n                    ]\n                    modules_status[module][\"completion_time\"] = datetime.strptime(\n                        modules_status[module][\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n                    ) + timedelta(seconds=module_run_output[\"execution_time\"])\n            return modules_status[module]\n\n        return {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n\n    def _get_module_output_json_filename(self, module):\n        module_input_json = self._module_input_json.as_posix()\n        module_output_json = module_input_json.replace(\n            \"-input.json\",\n            \"-\" + module + \"-\" + str(self._modules_input_hash) + \"-output.json\",\n        )\n        return module_output_json\n\n    def _update_total_duration(self):\n        with open(self._modules_input_hash_fp) as f:\n            modules_status = json.load(f)\n        cumulative_execution_duration = sum(\n            v[\"duration\"] or 0\n            for k, v in modules_status.items()\n            if k not in (\"cumulative_execution_duration\", \"total_duration\")\n        )\n\n        for m in self._modules:\n            first_start_time = modules_status[m][\"start_time\"]\n            if first_start_time is not None:\n                break\n\n        for m in self._modules[::-1]:\n            last_completion_time = modules_status[m][\"completion_time\"]\n            if last_completion_time is not None:\n                break\n\n        if first_start_time is None or last_completion_time is None:\n            return\n\n        total_duration = (\n            datetime.strptime(\n                last_completion_time,\n                \"%Y-%m-%d %H:%M:%S.%f\",\n            )\n            - datetime.strptime(first_start_time, \"%Y-%m-%d %H:%M:%S.%f\")\n        ).total_seconds()\n        self._update_module_status(\n            {\n                \"cumulative_execution_duration\": cumulative_execution_duration,\n                \"total_duration\": total_duration,\n            }\n        )\n</code></pre>"}, {"location": "api/element_array_ephys/readers/kilosort_triggering/#element_array_ephys.readers.kilosort_triggering.OpenEphysKilosortPipeline", "title": "<code>OpenEphysKilosortPipeline</code>", "text": "<p>An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline  for one Neuropixels probe in one recording session using the Open Ephys acquisition software.</p> <p>Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Which is based on <code>ecephys_spike_sorting</code> routines from Allen Institute https://github.com/AllenInstitute/ecephys_spike_sorting</p> Source code in <code>element_array_ephys/readers/kilosort_triggering.py</code> <pre><code>class OpenEphysKilosortPipeline:\n\"\"\"\n    An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline\n     for one Neuropixels probe in one recording session using the Open Ephys acquisition software.\n\n    Primarily calling routines specified from:\n    https://github.com/jenniferColonell/ecephys_spike_sorting\n    Which is based on `ecephys_spike_sorting` routines from Allen Institute\n    https://github.com/AllenInstitute/ecephys_spike_sorting\n    \"\"\"\n\n    _modules = [\n        \"depth_estimation\",\n        \"median_subtraction\",\n        \"kilosort_helper\",\n        \"kilosort_postprocessing\",\n        \"noise_templates\",\n        \"mean_waveforms\",\n        \"quality_metrics\",\n    ]\n\n    _input_json_args = list(inspect.signature(createInputJson).parameters)\n\n    def __init__(\n        self, npx_input_dir: str, ks_output_dir: str, params: dict, KS2ver: str\n    ):\n        self._npx_input_dir = pathlib.Path(npx_input_dir)\n\n        self._ks_output_dir = pathlib.Path(ks_output_dir)\n        self._ks_output_dir.mkdir(parents=True, exist_ok=True)\n\n        self._params = params\n        self._KS2ver = KS2ver\n\n        self._json_directory = self._ks_output_dir / \"json_configs\"\n        self._json_directory.mkdir(parents=True, exist_ok=True)\n\n        self._module_input_json = (\n            self._json_directory / f\"{self._npx_input_dir.name}-input.json\"\n        )\n        self._module_logfile = (\n            self._json_directory / f\"{self._npx_input_dir.name}-run_modules-log.txt\"\n        )\n\n        self.ks_input_params = None\n        self._modules_input_hash = None\n        self._modules_input_hash_fp = None\n\n    def make_chanmap_file(self):\n        continuous_file = self._npx_input_dir / \"continuous.dat\"\n        self._chanmap_filepath = self._ks_output_dir / \"chanMap.mat\"\n\n        _write_channel_map_file(\n            channel_ind=self._params[\"channel_ind\"],\n            x_coords=self._params[\"x_coords\"],\n            y_coords=self._params[\"y_coords\"],\n            shank_ind=self._params[\"shank_ind\"],\n            connected=self._params[\"connected\"],\n            probe_name=self._params[\"probe_type\"],\n            ap_band_file=continuous_file.as_posix(),\n            bit_volts=self._params[\"uVPerBit\"],\n            sample_rate=self._params[\"sample_rate\"],\n            save_path=self._chanmap_filepath.as_posix(),\n            is_0_based=True,\n        )\n\n    def generate_modules_input_json(self):\n        self.make_chanmap_file()\n\n        continuous_file = self._get_raw_data_filepaths()\n\n        lf_dir = self._npx_input_dir.as_posix()\n        try:\n            # old probe folder convention with 100.0, 100.1, 100.2, 100.3, etc.\n            name, num = re.search(r\"(.+\\.)(\\d)+$\", lf_dir).groups()\n        except AttributeError:\n            # new probe folder convention with -AP or -LFP\n            assert lf_dir.endswith(\"AP\")\n            lf_dir = re.sub(\"-AP$\", \"-LFP\", lf_dir)\n        else:\n            lf_dir = f\"{name}{int(num) + 1}\"\n        lf_file = pathlib.Path(lf_dir) / \"continuous.dat\"\n\n        params = {}\n        for k, v in self._params.items():\n            value = str(v) if isinstance(v, list) else v\n            if f\"ks_{k}\" in self._input_json_args:\n                params[f\"ks_{k}\"] = value\n            if k in self._input_json_args:\n                params[k] = value\n\n        self.ks_input_params = createInputJson(\n            self._module_input_json.as_posix(),\n            KS2ver=self._KS2ver,\n            npx_directory=self._npx_input_dir.as_posix(),\n            spikeGLX_data=False,\n            continuous_file=continuous_file.as_posix(),\n            lf_file=lf_file.as_posix(),\n            extracted_data_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_tmp=self._ks_output_dir.as_posix(),\n            ks_make_copy=True,\n            noise_template_use_rf=self._params.get(\"noise_template_use_rf\", False),\n            use_C_Waves=False,\n            c_Waves_snr_um=self._params.get(\"c_Waves_snr_um\", 160),\n            qm_isi_thresh=self._params.get(\"refPerMS\", 2.0) / 1000,\n            kilosort_repository=_get_kilosort_repository(self._KS2ver),\n            chanMap_path=self._chanmap_filepath.as_posix(),\n            **params,\n        )\n\n        self._modules_input_hash = dict_to_uuid(dict(self._params, KS2ver=self._KS2ver))\n\n    def run_modules(self, modules_to_run=None):\n        print(\"---- Running Modules ----\")\n        self.generate_modules_input_json()\n        module_input_json = self._module_input_json.as_posix()\n        module_logfile = self._module_logfile.as_posix()\n\n        modules = modules_to_run or self._modules\n\n        for module in modules:\n            module_status = self._get_module_status(module)\n            if module_status[\"completion_time\"] is not None:\n                continue\n\n            if module == \"median_subtraction\":\n                median_subtraction_duration = (\n                    self._get_median_subtraction_duration_from_log()\n                )\n                if median_subtraction_duration is not None:\n                    median_subtraction_status = self._get_module_status(\n                        \"median_subtraction\"\n                    )\n                    median_subtraction_status[\"duration\"] = median_subtraction_duration\n                    median_subtraction_status[\"completion_time\"] = datetime.strptime(\n                        median_subtraction_status[\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n                    ) + timedelta(seconds=median_subtraction_status[\"duration\"])\n                    self._update_module_status(\n                        {\"median_subtraction\": median_subtraction_status}\n                    )\n                    continue\n\n            module_output_json = self._get_module_output_json_filename(module)\n            command = [\n                sys.executable,\n                \"-W\",\n                \"ignore\",\n                \"-m\",\n                \"ecephys_spike_sorting.modules.\" + module,\n                \"--input_json\",\n                module_input_json,\n                \"--output_json\",\n                module_output_json,\n            ]\n\n            start_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": None,\n                        \"duration\": None,\n                    }\n                }\n            )\n            with open(module_logfile, \"a\") as f:\n                subprocess.check_call(command, stdout=f)\n            completion_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": completion_time,\n                        \"duration\": (completion_time - start_time).total_seconds(),\n                    }\n                }\n            )\n\n        self._update_total_duration()\n\n    def _get_raw_data_filepaths(self):\n        raw_ap_fp = self._npx_input_dir / \"continuous.dat\"\n\n        if \"median_subtraction\" not in self._modules:\n            return raw_ap_fp\n\n        # median subtraction step will overwrite original continuous.dat file with the corrected version\n        # to preserve the original raw data - make a copy here and work on the copied version\n        assert \"depth_estimation\" in self._modules\n        continuous_file = self._ks_output_dir / \"continuous.dat\"\n        if continuous_file.exists():\n            if raw_ap_fp.stat().st_mtime == continuous_file.stat().st_mtime:\n                return continuous_file\n            else:\n                if self._module_logfile.exists():\n                    return continuous_file\n\n        shutil.copy2(raw_ap_fp, continuous_file)\n        return continuous_file\n\n    def _update_module_status(self, updated_module_status={}):\n        if self._modules_input_hash is None:\n            raise RuntimeError('\"generate_modules_input_json()\" not yet performed!')\n\n        self._modules_input_hash_fp = (\n            self._json_directory / f\".{self._modules_input_hash}.json\"\n        )\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            modules_status = {**modules_status, **updated_module_status}\n        else:\n            # handle cases of processing rerun on different parameters (the hash changes)\n            # delete outdated files\n            [\n                f.unlink()\n                for f in self._json_directory.glob(\"*\")\n                if f.is_file() and f.name != self._module_input_json.name\n            ]\n\n            modules_status = {\n                module: {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n                for module in self._modules\n            }\n        with open(self._modules_input_hash_fp, \"w\") as f:\n            json.dump(modules_status, f, default=str)\n\n    def _get_module_status(self, module):\n        if self._modules_input_hash_fp is None:\n            self._update_module_status()\n\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            if modules_status[module][\"completion_time\"] is None:\n                # additional logic to read from the \"-output.json\" file for this module as well\n                # handle cases where the module has finished successfully,\n                # but the \"_modules_input_hash_fp\" is not updated (for whatever reason),\n                # resulting in this module not registered as completed in the \"_modules_input_hash_fp\"\n                module_output_json_fp = pathlib.Path(\n                    self._get_module_output_json_filename(module)\n                )\n                if module_output_json_fp.exists():\n                    with open(module_output_json_fp) as f:\n                        module_run_output = json.load(f)\n                    modules_status[module][\"duration\"] = module_run_output[\n                        \"execution_time\"\n                    ]\n                    modules_status[module][\"completion_time\"] = datetime.strptime(\n                        modules_status[module][\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n                    ) + timedelta(seconds=module_run_output[\"execution_time\"])\n            return modules_status[module]\n\n        return {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n\n    def _get_module_output_json_filename(self, module):\n        module_input_json = self._module_input_json.as_posix()\n        module_output_json = module_input_json.replace(\n            \"-input.json\",\n            \"-\" + module + \"-\" + str(self._modules_input_hash) + \"-output.json\",\n        )\n        return module_output_json\n\n    def _update_total_duration(self):\n        with open(self._modules_input_hash_fp) as f:\n            modules_status = json.load(f)\n        cumulative_execution_duration = sum(\n            v[\"duration\"] or 0\n            for k, v in modules_status.items()\n            if k not in (\"cumulative_execution_duration\", \"total_duration\")\n        )\n\n        for m in self._modules:\n            first_start_time = modules_status[m][\"start_time\"]\n            if first_start_time is not None:\n                break\n\n        for m in self._modules[::-1]:\n            last_completion_time = modules_status[m][\"completion_time\"]\n            if last_completion_time is not None:\n                break\n\n        if first_start_time is None or last_completion_time is None:\n            return\n\n        total_duration = (\n            datetime.strptime(\n                last_completion_time,\n                \"%Y-%m-%d %H:%M:%S.%f\",\n            )\n            - datetime.strptime(first_start_time, \"%Y-%m-%d %H:%M:%S.%f\")\n        ).total_seconds()\n        self._update_module_status(\n            {\n                \"cumulative_execution_duration\": cumulative_execution_duration,\n                \"total_duration\": total_duration,\n            }\n        )\n\n    def _get_median_subtraction_duration_from_log(self):\n        raw_ap_fp = self._npx_input_dir / \"continuous.dat\"\n        continuous_file = self._ks_output_dir / \"continuous.dat\"\n        if raw_ap_fp.stat().st_mtime &lt; continuous_file.stat().st_mtime:\n            # if the copied continuous.dat was actually modified,\n            # median_subtraction may have been completed - let's check\n            if self._module_logfile.exists():\n                with open(self._module_logfile, \"r\") as f:\n                    previous_line = \"\"\n                    for line in f.readlines():\n                        if line.startswith(\n                            \"ecephys spike sorting: median subtraction module\"\n                        ) and previous_line.startswith(\"Total processing time:\"):\n                            # regex to search for the processing duration - a float value\n                            duration = int(\n                                re.search(\"\\d+\\.?\\d+\", previous_line).group()\n                            )\n                            return duration\n                        previous_line = line\n</code></pre>"}, {"location": "api/element_array_ephys/readers/openephys/", "title": "openephys.py", "text": ""}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.logger", "title": "<code>logger = logging.getLogger('datajoint')</code>  <code>module-attribute</code>", "text": "<p>The Open Ephys Record Node saves Neuropixels data in binary format according to the following the directory structure: (https://open-ephys.github.io/gui-docs/User-Manual/Recording-data/Binary-format.html)</p> <p>Record Node 102 -- experiment1 (equivalent to one experimental session - multi probes, multi recordings per probe)    -- recording1    -- recording2       -- continuous          -- Neuropix-PXI-100.0 (probe0 ap)          -- Neuropix-PXI-100.1 (probe0 lf)          -- Neuropix-PXI-100.2 (probe1 ap)          -- Neuropix-PXI-100.3 (probe1 lf)          ...       -- events       -- spikes       -- structure.oebin -- experiment 2    ... -- settings.xml -- settings2.xml ...</p>"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys", "title": "<code>OpenEphys</code>", "text": "Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>class OpenEphys:\n    def __init__(self, experiment_dir):\n        self.session_dir = pathlib.Path(experiment_dir)\n\n        if self.session_dir.name.startswith(\"recording\"):\n            openephys_file = pyopenephys.File(\n                self.session_dir.parent.parent\n            )  # this is on the Record Node level\n            self._is_recording_folder = True\n        else:\n            openephys_file = pyopenephys.File(\n                self.session_dir.parent\n            )  # this is on the Record Node level\n            self._is_recording_folder = False\n\n        # extract the \"recordings\" for this session\n        self.experiment = next(\n            experiment\n            for experiment in openephys_file.experiments\n            if pathlib.Path(experiment.absolute_foldername)\n            == (\n                self.session_dir.parent\n                if self._is_recording_folder\n                else self.session_dir\n            )\n        )\n\n        # extract probe data\n        self.probes = self.load_probe_data()\n\n        #\n        self._recording_time = None\n\n    @property\n    def recording_time(self):\n        if self._recording_time is None:\n            recording_datetimes = []\n            for probe in self.probes.values():\n                recording_datetimes.extend(probe.recording_info[\"recording_datetimes\"])\n            self._recording_time = sorted(recording_datetimes)[0]\n        return self._recording_time\n\n    def load_probe_data(self):  # noqa: C901\n\"\"\"\n        Loop through all Open Ephys \"signalchains/processors\", identify the processor for\n         the Neuropixels probe(s), extract probe info\n            Loop through all recordings, associate recordings to\n            the matching probes, extract recording info\n\n        Yielding multiple \"Probe\" objects, each containing meta information\n         and timeseries data associated with each probe\n        \"\"\"\n\n        probes = {}\n        sigchain_iter = (\n            self.experiment.settings[\"SIGNALCHAIN\"]\n            if isinstance(self.experiment.settings[\"SIGNALCHAIN\"], list)\n            else [self.experiment.settings[\"SIGNALCHAIN\"]]\n        )\n        for sigchain in sigchain_iter:\n            processor_iter = (\n                sigchain[\"PROCESSOR\"]\n                if isinstance(sigchain[\"PROCESSOR\"], list)\n                else [sigchain[\"PROCESSOR\"]]\n            )\n            for processor in processor_iter:\n                if processor[\"@pluginName\"] in (\"Neuropix-3a\", \"Neuropix-PXI\"):\n                    if \"STREAM\" in processor:  # only on version &gt;= 0.6.0\n                        ap_streams = [\n                            stream\n                            for stream in processor[\"STREAM\"]\n                            if not stream[\"@name\"].endswith(\"LFP\")\n                        ]\n                    else:\n                        ap_streams = None\n\n                    if (\n                        processor[\"@pluginName\"] == \"Neuropix-3a\"\n                        or \"NP_PROBE\" not in processor[\"EDITOR\"]\n                    ):\n                        editor_probe_key = \"PROBE\"\n                    elif processor[\"@pluginName\"] == \"Neuropix-PXI\":\n                        editor_probe_key = \"NP_PROBE\"\n                    else:\n                        raise NotImplementedError\n\n                    probe_indices = (\n                        (0,)\n                        if isinstance(processor[\"EDITOR\"][editor_probe_key], dict)\n                        else range(len(processor[\"EDITOR\"][editor_probe_key]))\n                    )\n\n                else:  # not a processor for Neuropixels probe\n                    continue\n\n                for probe_index in probe_indices:\n                    probe = Probe(processor, probe_index)\n                    if ap_streams:\n                        probe.probe_info[\"ap_stream\"] = ap_streams[probe_index]\n                    probes[probe.probe_SN] = probe\n\n        for probe_index, probe_SN in enumerate(probes):\n            probe = probes[probe_SN]\n\n            for rec in self.experiment.recordings:\n                if (\n                    self._is_recording_folder\n                    and rec.absolute_foldername != self.session_dir\n                ):\n                    continue\n\n                assert len(rec._oebin[\"continuous\"]) == len(rec.analog_signals), (\n                    f\"Mismatch in the number of continuous data\"\n                    f' - expecting {len(rec._oebin[\"continuous\"])} (from structure.oebin file),'\n                    f\" found {len(rec.analog_signals)} (in continuous folder)\"\n                )\n\n                for continuous_info, analog_signal in zip(\n                    rec._oebin[\"continuous\"], rec.analog_signals\n                ):\n                    if continuous_info[\"source_processor_id\"] != probe.processor_id:\n                        continue\n\n                    # determine if this is continuous data for AP or LFP for the current probe\n                    if \"ap_stream\" in probe.probe_info:\n                        if (\n                            probe.probe_info[\"ap_stream\"][\"@name\"].split(\"-\")[0]\n                            != continuous_info[\"stream_name\"].split(\"-\")[0]\n                        ):\n                            continue  # not continuous data for the current probe\n                        match = re.search(\"-(AP|LFP)$\", continuous_info[\"stream_name\"])\n                        if match:\n                            continuous_type = match.groups()[0].lower()\n                        else:\n                            continuous_type = \"ap\"\n                    elif \"source_processor_sub_idx\" in continuous_info:\n                        if (\n                            continuous_info[\"source_processor_sub_idx\"]\n                            == probe_index * 2\n                        ):  # ap data\n                            assert (\n                                continuous_info[\"sample_rate\"]\n                                == analog_signal.sample_rate\n                                == 30000\n                            )\n                            continuous_type = \"ap\"\n                        elif (\n                            continuous_info[\"source_processor_sub_idx\"]\n                            == probe_index * 2 + 1\n                        ):  # lfp data\n                            assert (\n                                continuous_info[\"sample_rate\"]\n                                == analog_signal.sample_rate\n                                == 2500\n                            )\n                            continuous_type = \"lfp\"\n                        else:\n                            continue  # not continuous data for the current probe\n                    else:\n                        raise ValueError(\n                            f'Unable to infer type (AP or LFP) for the continuous data from:\\n\\t{continuous_info[\"folder_name\"]}'\n                        )\n\n                    if continuous_type == \"ap\":\n                        probe.recording_info[\"recording_count\"] += 1\n                        probe.recording_info[\"recording_datetimes\"].append(\n                            rec.datetime\n                            + datetime.timedelta(seconds=float(rec.start_time))\n                        )\n                        probe.recording_info[\"recording_durations\"].append(\n                            float(rec.duration)\n                        )\n                        probe.recording_info[\"recording_files\"].append(\n                            rec.absolute_foldername\n                            / \"continuous\"\n                            / continuous_info[\"folder_name\"]\n                        )\n                    elif continuous_type == \"lfp\":\n                        probe.recording_info[\"recording_lfp_files\"].append(\n                            rec.absolute_foldername\n                            / \"continuous\"\n                            / continuous_info[\"folder_name\"]\n                        )\n\n                    meta = getattr(probe, continuous_type + \"_meta\")\n                    if not meta:\n                        # channel indices - 0-based indexing\n                        channels_indices = [\n                            int(re.search(r\"\\d+$\", chn_name).group()) - 1\n                            for chn_name in analog_signal.channel_names\n                        ]\n\n                        meta.update(\n                            **continuous_info,\n                            channels_indices=channels_indices,\n                            channels_ids=analog_signal.channel_ids,\n                            channels_names=analog_signal.channel_names,\n                            channels_gains=analog_signal.gains,\n                        )\n\n                    signal = getattr(probe, continuous_type + \"_analog_signals\")\n                    signal.append(analog_signal)\n\n        return probes\n</code></pre>"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys.load_probe_data", "title": "<code>load_probe_data()</code>", "text": "<p>Loop through all Open Ephys \"signalchains/processors\", identify the processor for  the Neuropixels probe(s), extract probe info     Loop through all recordings, associate recordings to     the matching probes, extract recording info</p> <p>Yielding multiple \"Probe\" objects, each containing meta information  and timeseries data associated with each probe</p> Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>def load_probe_data(self):  # noqa: C901\n\"\"\"\n    Loop through all Open Ephys \"signalchains/processors\", identify the processor for\n     the Neuropixels probe(s), extract probe info\n        Loop through all recordings, associate recordings to\n        the matching probes, extract recording info\n\n    Yielding multiple \"Probe\" objects, each containing meta information\n     and timeseries data associated with each probe\n    \"\"\"\n\n    probes = {}\n    sigchain_iter = (\n        self.experiment.settings[\"SIGNALCHAIN\"]\n        if isinstance(self.experiment.settings[\"SIGNALCHAIN\"], list)\n        else [self.experiment.settings[\"SIGNALCHAIN\"]]\n    )\n    for sigchain in sigchain_iter:\n        processor_iter = (\n            sigchain[\"PROCESSOR\"]\n            if isinstance(sigchain[\"PROCESSOR\"], list)\n            else [sigchain[\"PROCESSOR\"]]\n        )\n        for processor in processor_iter:\n            if processor[\"@pluginName\"] in (\"Neuropix-3a\", \"Neuropix-PXI\"):\n                if \"STREAM\" in processor:  # only on version &gt;= 0.6.0\n                    ap_streams = [\n                        stream\n                        for stream in processor[\"STREAM\"]\n                        if not stream[\"@name\"].endswith(\"LFP\")\n                    ]\n                else:\n                    ap_streams = None\n\n                if (\n                    processor[\"@pluginName\"] == \"Neuropix-3a\"\n                    or \"NP_PROBE\" not in processor[\"EDITOR\"]\n                ):\n                    editor_probe_key = \"PROBE\"\n                elif processor[\"@pluginName\"] == \"Neuropix-PXI\":\n                    editor_probe_key = \"NP_PROBE\"\n                else:\n                    raise NotImplementedError\n\n                probe_indices = (\n                    (0,)\n                    if isinstance(processor[\"EDITOR\"][editor_probe_key], dict)\n                    else range(len(processor[\"EDITOR\"][editor_probe_key]))\n                )\n\n            else:  # not a processor for Neuropixels probe\n                continue\n\n            for probe_index in probe_indices:\n                probe = Probe(processor, probe_index)\n                if ap_streams:\n                    probe.probe_info[\"ap_stream\"] = ap_streams[probe_index]\n                probes[probe.probe_SN] = probe\n\n    for probe_index, probe_SN in enumerate(probes):\n        probe = probes[probe_SN]\n\n        for rec in self.experiment.recordings:\n            if (\n                self._is_recording_folder\n                and rec.absolute_foldername != self.session_dir\n            ):\n                continue\n\n            assert len(rec._oebin[\"continuous\"]) == len(rec.analog_signals), (\n                f\"Mismatch in the number of continuous data\"\n                f' - expecting {len(rec._oebin[\"continuous\"])} (from structure.oebin file),'\n                f\" found {len(rec.analog_signals)} (in continuous folder)\"\n            )\n\n            for continuous_info, analog_signal in zip(\n                rec._oebin[\"continuous\"], rec.analog_signals\n            ):\n                if continuous_info[\"source_processor_id\"] != probe.processor_id:\n                    continue\n\n                # determine if this is continuous data for AP or LFP for the current probe\n                if \"ap_stream\" in probe.probe_info:\n                    if (\n                        probe.probe_info[\"ap_stream\"][\"@name\"].split(\"-\")[0]\n                        != continuous_info[\"stream_name\"].split(\"-\")[0]\n                    ):\n                        continue  # not continuous data for the current probe\n                    match = re.search(\"-(AP|LFP)$\", continuous_info[\"stream_name\"])\n                    if match:\n                        continuous_type = match.groups()[0].lower()\n                    else:\n                        continuous_type = \"ap\"\n                elif \"source_processor_sub_idx\" in continuous_info:\n                    if (\n                        continuous_info[\"source_processor_sub_idx\"]\n                        == probe_index * 2\n                    ):  # ap data\n                        assert (\n                            continuous_info[\"sample_rate\"]\n                            == analog_signal.sample_rate\n                            == 30000\n                        )\n                        continuous_type = \"ap\"\n                    elif (\n                        continuous_info[\"source_processor_sub_idx\"]\n                        == probe_index * 2 + 1\n                    ):  # lfp data\n                        assert (\n                            continuous_info[\"sample_rate\"]\n                            == analog_signal.sample_rate\n                            == 2500\n                        )\n                        continuous_type = \"lfp\"\n                    else:\n                        continue  # not continuous data for the current probe\n                else:\n                    raise ValueError(\n                        f'Unable to infer type (AP or LFP) for the continuous data from:\\n\\t{continuous_info[\"folder_name\"]}'\n                    )\n\n                if continuous_type == \"ap\":\n                    probe.recording_info[\"recording_count\"] += 1\n                    probe.recording_info[\"recording_datetimes\"].append(\n                        rec.datetime\n                        + datetime.timedelta(seconds=float(rec.start_time))\n                    )\n                    probe.recording_info[\"recording_durations\"].append(\n                        float(rec.duration)\n                    )\n                    probe.recording_info[\"recording_files\"].append(\n                        rec.absolute_foldername\n                        / \"continuous\"\n                        / continuous_info[\"folder_name\"]\n                    )\n                elif continuous_type == \"lfp\":\n                    probe.recording_info[\"recording_lfp_files\"].append(\n                        rec.absolute_foldername\n                        / \"continuous\"\n                        / continuous_info[\"folder_name\"]\n                    )\n\n                meta = getattr(probe, continuous_type + \"_meta\")\n                if not meta:\n                    # channel indices - 0-based indexing\n                    channels_indices = [\n                        int(re.search(r\"\\d+$\", chn_name).group()) - 1\n                        for chn_name in analog_signal.channel_names\n                    ]\n\n                    meta.update(\n                        **continuous_info,\n                        channels_indices=channels_indices,\n                        channels_ids=analog_signal.channel_ids,\n                        channels_names=analog_signal.channel_names,\n                        channels_gains=analog_signal.gains,\n                    )\n\n                signal = getattr(probe, continuous_type + \"_analog_signals\")\n                signal.append(analog_signal)\n\n    return probes\n</code></pre>"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe", "title": "<code>Probe</code>", "text": "Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>class Probe:\n    def __init__(self, processor, probe_index=0):\n        processor_node_id = processor.get(\"@nodeId\", processor.get(\"@NodeId\"))\n        if processor_node_id is None:\n            raise KeyError('Neither \"@nodeId\" nor \"@NodeId\" key found')\n\n        self.processor_id = int(processor_node_id)\n\n        if (\n            processor[\"@pluginName\"] == \"Neuropix-3a\"\n            or \"NP_PROBE\" not in processor[\"EDITOR\"]\n        ):\n            self.probe_info = (\n                processor[\"EDITOR\"][\"PROBE\"]\n                if isinstance(processor[\"EDITOR\"][\"PROBE\"], dict)\n                else processor[\"EDITOR\"][\"PROBE\"][probe_index]\n            )\n            self.probe_SN = self.probe_info[\"@probe_serial_number\"]\n            self.probe_model = _probe_model_name_mapping[processor[\"@pluginName\"]]\n            self._channels_connected = {\n                int(re.search(r\"\\d+$\", k).group()): int(v)\n                for k, v in self.probe_info.pop(\"CHANNELSTATUS\").items()\n            }\n        else:  # Neuropix-PXI\n            self.probe_info = (\n                processor[\"EDITOR\"][\"NP_PROBE\"]\n                if isinstance(processor[\"EDITOR\"][\"NP_PROBE\"], dict)\n                else processor[\"EDITOR\"][\"NP_PROBE\"][probe_index]\n            )\n            self.probe_SN = self.probe_info[\"@probe_serial_number\"]\n            self.probe_model = _probe_model_name_mapping[self.probe_info[\"@probe_name\"]]\n\n            if \"ELECTRODE_XPOS\" in self.probe_info:\n                self.probe_info[\"ELECTRODE_XPOS\"] = {\n                    int(re.search(r\"\\d+$\", k).group()): int(v)\n                    for k, v in self.probe_info.pop(\"ELECTRODE_XPOS\").items()\n                }\n                self.probe_info[\"ELECTRODE_YPOS\"] = {\n                    int(re.search(r\"\\d+$\", k).group()): int(v)\n                    for k, v in self.probe_info.pop(\"ELECTRODE_YPOS\").items()\n                }\n                self.probe_info[\"ELECTRODE_SHANK\"] = {\n                    int(re.search(r\"\\d+$\", k).group()): int(v)\n                    for k, v in self.probe_info[\"CHANNELS\"].items()\n                }\n\n            self._channels_connected = {\n                int(re.search(r\"\\d+$\", k).group()): 1\n                for k in self.probe_info.pop(\"CHANNELS\")\n            }\n\n        self.ap_meta = {}\n        self.lfp_meta = {}\n\n        self.ap_analog_signals = []\n        self.lfp_analog_signals = []\n\n        self.recording_info = {\n            \"recording_count\": 0,\n            \"recording_datetimes\": [],\n            \"recording_durations\": [],\n            \"recording_files\": [],\n            \"recording_lfp_files\": [],\n        }\n\n        self._ap_timeseries = None\n        self._ap_timestamps = None\n        self._lfp_timeseries = None\n        self._lfp_timestamps = None\n\n    @property\n    def channels_connected(self):\n        return {\n            chn_idx: self._channels_connected.get(chn_idx, 0)\n            for chn_idx in self.ap_meta[\"channels_indices\"]\n        }\n\n    @property\n    def ap_timeseries(self):\n\"\"\"\n        AP data concatenated across recordings. Shape: (sample x channel)\n        Data are stored as int16 - to convert to microvolts,\n         multiply with self.ap_meta['channels_gains']\n        \"\"\"\n        if self._ap_timeseries is None:\n            self._ap_timeseries = np.hstack(\n                [s.signal for s in self.ap_analog_signals]\n            ).T\n        return self._ap_timeseries\n\n    @property\n    def ap_timestamps(self):\n        if self._ap_timestamps is None:\n            self._ap_timestamps = np.hstack([s.times for s in self.ap_analog_signals])\n        return self._ap_timestamps\n\n    @property\n    def lfp_timeseries(self):\n\"\"\"\n        LFP data concatenated across recordings. Shape: (sample x channel)\n        Data are stored as int16 - to convert to microvolts,\n         multiply with self.lfp_meta['channels_gains']\n        \"\"\"\n        if self._lfp_timeseries is None:\n            self._lfp_timeseries = np.hstack(\n                [s.signal for s in self.lfp_analog_signals]\n            ).T\n        return self._lfp_timeseries\n\n    @property\n    def lfp_timestamps(self):\n        if self._lfp_timestamps is None:\n            self._lfp_timestamps = np.hstack([s.times for s in self.lfp_analog_signals])\n        return self._lfp_timestamps\n\n    def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n        :param spikes: spike times (in second) to extract waveforms\n        :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms\n        :param n_wf: number of spikes per unit to extract the waveforms\n        :param wf_win: number of sample pre and post a spike\n        :return: waveforms (sample x channel x spike)\n        \"\"\"\n        channel_bit_volts = np.array(self.ap_meta[\"channels_gains\"])[channel_ind]\n\n        # ignore spikes at the beginning or end of raw data\n        spikes = spikes[\n            np.logical_and(\n                spikes &gt; (-wf_win[0] / self.ap_meta[\"sample_rate\"]),\n                spikes\n                &lt; (self.ap_timestamps.max() - wf_win[-1] / self.ap_meta[\"sample_rate\"]),\n            )\n        ]\n        # select a randomized set of \"n_wf\" spikes\n        np.random.shuffle(spikes)\n        spikes = spikes[:n_wf]\n        # extract waveforms\n        if len(spikes) &gt; 0:\n            spike_indices = np.searchsorted(self.ap_timestamps, spikes, side=\"left\")\n            # waveform at each spike: (sample x channel x spike)\n            spike_wfs = np.dstack(\n                [\n                    self.ap_timeseries[\n                        int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind\n                    ]\n                    * channel_bit_volts\n                    for spk in spike_indices\n                ]\n            )\n            return spike_wfs\n        else:  # if no spike found, return NaN of size (sample x channel x 1)\n            return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n\n    def compress(self):\n        from mtscomp import compress as mts_compress\n\n        ap_dirs = self.recording_info[\"recording_files\"]\n        lfp_dirs = self.recording_info[\"recording_lfp_files\"]\n\n        meta_mapping = {\"ap\": self.ap_meta, \"lfp\": self.lfp_meta}\n\n        compressed_files = []\n        for continuous_dir, continuous_type in zip(\n            ap_dirs + lfp_dirs, [\"ap\"] * len(ap_dirs) + [\"lfp\"] * len(lfp_dirs)\n        ):\n            dat_fp = continuous_dir / \"continuous.dat\"\n            if not dat_fp.exists():\n                raise FileNotFoundError(\n                    f'Compression error - \"{dat_fp}\" does not exist'\n                )\n            cdat_fp = continuous_dir / \"continuous.cdat\"\n            ch_fp = continuous_dir / \"continuous.ch\"\n\n            if cdat_fp.exists():\n                assert ch_fp.exists()\n                logger.info(f\"Compressed file exists ({cdat_fp}), skipping...\")\n                continue\n\n            try:\n                mts_compress(\n                    dat_fp,\n                    cdat_fp,\n                    ch_fp,\n                    sample_rate=meta_mapping[continuous_type][\"sample_rate\"],\n                    n_channels=meta_mapping[continuous_type][\"num_channels\"],\n                    dtype=np.memmap(dat_fp).dtype,\n                )\n            except Exception as e:\n                cdat_fp.unlink(missing_ok=True)\n                ch_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                compressed_files.append((cdat_fp, ch_fp))\n\n        return compressed_files\n\n    def decompress(self):\n        from mtscomp import decompress as mts_decompress\n\n        ap_dirs = self.recording_info[\"recording_files\"]\n        lfp_dirs = self.recording_info[\"recording_lfp_files\"]\n\n        decompressed_files = []\n        for continuous_dir, continuous_type in zip(\n            ap_dirs + lfp_dirs, [\"ap\"] * len(ap_dirs) + [\"lfp\"] * len(lfp_dirs)\n        ):\n            dat_fp = continuous_dir / \"continuous.dat\"\n\n            if dat_fp.exists():\n                logger.info(f\"Decompressed file exists ({dat_fp}), skipping...\")\n                continue\n\n            cdat_fp = continuous_dir / \"continuous.cdat\"\n            ch_fp = continuous_dir / \"continuous.ch\"\n\n            if not cdat_fp.exists():\n                raise FileNotFoundError(\n                    f'Decompression error - \"{cdat_fp}\" does not exist'\n                )\n\n            try:\n                decomp_arr = mts_decompress(cdat_fp, ch_fp)\n                decomp_arr.tofile(dat_fp)\n            except Exception as e:\n                dat_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                decompressed_files.append(dat_fp)\n\n        return decompressed_files\n</code></pre>"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.ap_timeseries", "title": "<code>ap_timeseries</code>  <code>property</code>", "text": "<p>AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts,  multiply with self.ap_meta['channels_gains']</p>"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.lfp_timeseries", "title": "<code>lfp_timeseries</code>  <code>property</code>", "text": "<p>LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts,  multiply with self.lfp_meta['channels_gains']</p>"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.extract_spike_waveforms", "title": "<code>extract_spike_waveforms(spikes, channel_ind, n_wf=500, wf_win=(-32, 32))</code>", "text": "<p>:param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike)</p> Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n    :param spikes: spike times (in second) to extract waveforms\n    :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms\n    :param n_wf: number of spikes per unit to extract the waveforms\n    :param wf_win: number of sample pre and post a spike\n    :return: waveforms (sample x channel x spike)\n    \"\"\"\n    channel_bit_volts = np.array(self.ap_meta[\"channels_gains\"])[channel_ind]\n\n    # ignore spikes at the beginning or end of raw data\n    spikes = spikes[\n        np.logical_and(\n            spikes &gt; (-wf_win[0] / self.ap_meta[\"sample_rate\"]),\n            spikes\n            &lt; (self.ap_timestamps.max() - wf_win[-1] / self.ap_meta[\"sample_rate\"]),\n        )\n    ]\n    # select a randomized set of \"n_wf\" spikes\n    np.random.shuffle(spikes)\n    spikes = spikes[:n_wf]\n    # extract waveforms\n    if len(spikes) &gt; 0:\n        spike_indices = np.searchsorted(self.ap_timestamps, spikes, side=\"left\")\n        # waveform at each spike: (sample x channel x spike)\n        spike_wfs = np.dstack(\n            [\n                self.ap_timeseries[\n                    int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind\n                ]\n                * channel_bit_volts\n                for spk in spike_indices\n            ]\n        )\n        return spike_wfs\n    else:  # if no spike found, return NaN of size (sample x channel x 1)\n        return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/", "title": "spikeglx.py", "text": ""}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX", "title": "<code>SpikeGLX</code>", "text": "Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>class SpikeGLX:\n    def __init__(self, root_dir):\n\"\"\"\n        create neuropixels reader from 'root name' - e.g. the recording:\n\n            /data/rec_1/npx_g0_t0.imec.ap.meta\n            /data/rec_1/npx_g0_t0.imec.ap.bin\n            /data/rec_1/npx_g0_t0.imec.lf.meta\n            /data/rec_1/npx_g0_t0.imec.lf.bin\n\n        would have a 'root name' of:\n\n            /data/rec_1/npx_g0_t0.imec\n\n        only a single recording is read/loaded via the root\n        name &amp; associated meta - no interpretation of g0_t0.imec, etc is\n        performed at this layer.\n        \"\"\"\n        self._apmeta, self._ap_timeseries = None, None\n        self._lfmeta, self._lf_timeseries = None, None\n\n        self.root_dir = pathlib.Path(root_dir)\n\n        try:\n            meta_filepath = next(pathlib.Path(root_dir).glob(\"*.ap.meta\"))\n        except StopIteration:\n            raise FileNotFoundError(f\"No SpikeGLX file (.ap.meta) found at: {root_dir}\")\n\n        self.root_name = meta_filepath.name.replace(\".ap.meta\", \"\")\n\n    @property\n    def apmeta(self):\n        if self._apmeta is None:\n            self._apmeta = SpikeGLXMeta(self.root_dir / (self.root_name + \".ap.meta\"))\n        return self._apmeta\n\n    @property\n    def ap_timeseries(self):\n\"\"\"\n        AP data: (sample x channel)\n        Data are stored as np.memmap with dtype: int16\n        - to convert to microvolts, multiply with self.get_channel_bit_volts('ap')\n        \"\"\"\n        if self._ap_timeseries is None:\n            self.validate_file(\"ap\")\n            self._ap_timeseries = self._read_bin(\n                self.root_dir / (self.root_name + \".ap.bin\")\n            )\n        return self._ap_timeseries\n\n    @property\n    def lfmeta(self):\n        if self._lfmeta is None:\n            self._lfmeta = SpikeGLXMeta(self.root_dir / (self.root_name + \".lf.meta\"))\n        return self._lfmeta\n\n    @property\n    def lf_timeseries(self):\n\"\"\"\n        LFP data: (sample x channel)\n        Data are stored as np.memmap with dtype: int16\n        - to convert to microvolts, multiply with self.get_channel_bit_volts('lf')\n        \"\"\"\n        if self._lf_timeseries is None:\n            self.validate_file(\"lf\")\n            self._lf_timeseries = self._read_bin(\n                self.root_dir / (self.root_name + \".lf.bin\")\n            )\n        return self._lf_timeseries\n\n    def get_channel_bit_volts(self, band=\"ap\"):\n\"\"\"\n        Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels\n        Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n                dataVolts = dataInt * Vmax / Imax / gain\n        \"\"\"\n        vmax = float(self.apmeta.meta[\"imAiRangeMax\"])\n\n        if band == \"ap\":\n            imax = IMAX[self.apmeta.probe_model]\n            imroTbl_data = self.apmeta.imroTbl[\"data\"]\n            imroTbl_idx = 3\n            chn_ind = self.apmeta.get_recording_channels_indices(exclude_sync=True)\n\n        elif band == \"lf\":\n            imax = IMAX[self.lfmeta.probe_model]\n            imroTbl_data = self.lfmeta.imroTbl[\"data\"]\n            imroTbl_idx = 4\n            chn_ind = self.lfmeta.get_recording_channels_indices(exclude_sync=True)\n        else:\n            raise ValueError(f'Unsupported band: {band} - Must be \"ap\" or \"lf\"')\n\n        # extract channels' gains\n        if \"imDatPrb_dock\" in self.apmeta.meta:\n            # NP 2.0; APGain = 80 for all AP (LF is computed from AP)\n            chn_gains = [AP_GAIN] * len(imroTbl_data)\n        else:\n            # 3A, 3B1, 3B2 (NP 1.0)\n            chn_gains = [c[imroTbl_idx] for c in imroTbl_data]\n\n        chn_gains = np.array(chn_gains)[chn_ind]\n\n        return vmax / imax / chn_gains * 1e6  # convert to uV as well\n\n    def _read_bin(self, fname):\n        nchan = self.apmeta.meta[\"nSavedChans\"]\n        dtype = np.dtype((np.int16, nchan))\n        return np.memmap(fname, dtype, \"r\")\n\n    def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n        :param spikes: spike times (in second) to extract waveforms\n        :param channel_ind: channel indices (of shankmap) to extract the waveforms from\n        :param n_wf: number of spikes per unit to extract the waveforms\n        :param wf_win: number of sample pre and post a spike\n        :return: waveforms (in uV) - shape: (sample x channel x spike)\n        \"\"\"\n        channel_bit_volts = self.get_channel_bit_volts(\"ap\")[channel_ind]\n\n        data = self.ap_timeseries\n\n        spikes = np.round(spikes * self.apmeta.meta[\"imSampRate\"]).astype(\n            int\n        )  # convert to sample\n        # ignore spikes at the beginning or end of raw data\n        spikes = spikes[\n            np.logical_and(spikes &gt; -wf_win[0], spikes &lt; data.shape[0] - wf_win[-1])\n        ]\n\n        np.random.shuffle(spikes)\n        spikes = spikes[:n_wf]\n        if len(spikes) &gt; 0:\n            # waveform at each spike: (sample x channel x spike)\n            spike_wfs = np.dstack(\n                [\n                    data[int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind]\n                    * channel_bit_volts\n                    for spk in spikes\n                ]\n            )\n            return spike_wfs\n        else:  # if no spike found, return NaN of size (sample x channel x 1)\n            return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n\n    def validate_file(self, file_type=\"ap\"):\n        file_path = self.root_dir / (self.root_name + f\".{file_type}.bin\")\n        file_size = file_path.stat().st_size\n\n        if file_type == \"ap\":\n            meta = self.apmeta\n        elif file_type == \"lf\":\n            meta = self.lfmeta\n        else:\n            raise KeyError(f\"Unknown file_type {file_type} - must be 'ap' or 'lf'\")\n\n        if file_size != meta.meta[\"fileSizeBytes\"]:\n            raise IOError(\n                f\"File size error! {file_path} may be corrupted or in transfer?\"\n            )\n\n    def compress(self):\n        from mtscomp import compress as mts_compress\n\n        ap_file = self.root_dir / (self.root_name + \".ap.bin\")\n        lfp_file = self.root_dir / (self.root_name + \".lf.bin\")\n\n        meta_mapping = {\"ap\": self.apmeta, \"lfp\": self.lfmeta}\n\n        compressed_files = []\n        for bin_fp, band_type in zip([ap_file, lfp_file], [\"ap\", \"lfp\"]):\n            if not bin_fp.exists():\n                raise FileNotFoundError(\n                    f'Compression error - \"{bin_fp}\" does not exist'\n                )\n            cbin_fp = bin_fp.parent / f\"{bin_fp.stem}.cbin\"\n            ch_fp = bin_fp.parent / f\"{bin_fp.stem}.ch\"\n\n            if cbin_fp.exists():\n                assert ch_fp.exists()\n                logger.info(f\"Compressed file exists ({cbin_fp}), skipping...\")\n                continue\n\n            try:\n                mts_compress(\n                    bin_fp,\n                    cbin_fp,\n                    ch_fp,\n                    sample_rate=meta_mapping[band_type][\"sample_rate\"],\n                    n_channels=meta_mapping[band_type][\"num_channels\"],\n                    dtype=np.memmap(bin_fp).dtype,\n                )\n            except Exception as e:\n                cbin_fp.unlink(missing_ok=True)\n                ch_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                compressed_files.append((cbin_fp, ch_fp))\n\n        return compressed_files\n\n    def decompress(self):\n        from mtscomp import decompress as mts_decompress\n\n        ap_file = self.root_dir / (self.root_name + \".ap.bin\")\n        lfp_file = self.root_dir / (self.root_name + \".lf.bin\")\n\n        decompressed_files = []\n        for bin_fp, band_type in zip([ap_file, lfp_file], [\"ap\", \"lfp\"]):\n            if bin_fp.exists():\n                logger.info(f\"Decompressed file exists ({bin_fp}), skipping...\")\n                continue\n\n            cbin_fp = bin_fp.parent / f\"{bin_fp.stem}.cbin\"\n            ch_fp = bin_fp.parent / f\"{bin_fp.stem}.ch\"\n\n            if not cbin_fp.exists():\n                raise FileNotFoundError(\n                    f'Decompression error - \"{cbin_fp}\" does not exist'\n                )\n\n            try:\n                decomp_arr = mts_decompress(cbin_fp, ch_fp)\n                decomp_arr.tofile(bin_fp)\n            except Exception as e:\n                bin_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                decompressed_files.append(bin_fp)\n\n        return decompressed_files\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.__init__", "title": "<code>__init__(root_dir)</code>", "text": "<p>create neuropixels reader from 'root name' - e.g. the recording:</p> <pre><code>/data/rec_1/npx_g0_t0.imec.ap.meta\n/data/rec_1/npx_g0_t0.imec.ap.bin\n/data/rec_1/npx_g0_t0.imec.lf.meta\n/data/rec_1/npx_g0_t0.imec.lf.bin\n</code></pre> <p>would have a 'root name' of:</p> <pre><code>/data/rec_1/npx_g0_t0.imec\n</code></pre> <p>only a single recording is read/loaded via the root name &amp; associated meta - no interpretation of g0_t0.imec, etc is performed at this layer.</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def __init__(self, root_dir):\n\"\"\"\n    create neuropixels reader from 'root name' - e.g. the recording:\n\n        /data/rec_1/npx_g0_t0.imec.ap.meta\n        /data/rec_1/npx_g0_t0.imec.ap.bin\n        /data/rec_1/npx_g0_t0.imec.lf.meta\n        /data/rec_1/npx_g0_t0.imec.lf.bin\n\n    would have a 'root name' of:\n\n        /data/rec_1/npx_g0_t0.imec\n\n    only a single recording is read/loaded via the root\n    name &amp; associated meta - no interpretation of g0_t0.imec, etc is\n    performed at this layer.\n    \"\"\"\n    self._apmeta, self._ap_timeseries = None, None\n    self._lfmeta, self._lf_timeseries = None, None\n\n    self.root_dir = pathlib.Path(root_dir)\n\n    try:\n        meta_filepath = next(pathlib.Path(root_dir).glob(\"*.ap.meta\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No SpikeGLX file (.ap.meta) found at: {root_dir}\")\n\n    self.root_name = meta_filepath.name.replace(\".ap.meta\", \"\")\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.ap_timeseries", "title": "<code>ap_timeseries</code>  <code>property</code>", "text": "<p>AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap')</p>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.lf_timeseries", "title": "<code>lf_timeseries</code>  <code>property</code>", "text": "<p>LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf')</p>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.get_channel_bit_volts", "title": "<code>get_channel_bit_volts(band='ap')</code>", "text": "<p>Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels</p> https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip <p>dataVolts = dataInt * Vmax / Imax / gain</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def get_channel_bit_volts(self, band=\"ap\"):\n\"\"\"\n    Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels\n    Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n            dataVolts = dataInt * Vmax / Imax / gain\n    \"\"\"\n    vmax = float(self.apmeta.meta[\"imAiRangeMax\"])\n\n    if band == \"ap\":\n        imax = IMAX[self.apmeta.probe_model]\n        imroTbl_data = self.apmeta.imroTbl[\"data\"]\n        imroTbl_idx = 3\n        chn_ind = self.apmeta.get_recording_channels_indices(exclude_sync=True)\n\n    elif band == \"lf\":\n        imax = IMAX[self.lfmeta.probe_model]\n        imroTbl_data = self.lfmeta.imroTbl[\"data\"]\n        imroTbl_idx = 4\n        chn_ind = self.lfmeta.get_recording_channels_indices(exclude_sync=True)\n    else:\n        raise ValueError(f'Unsupported band: {band} - Must be \"ap\" or \"lf\"')\n\n    # extract channels' gains\n    if \"imDatPrb_dock\" in self.apmeta.meta:\n        # NP 2.0; APGain = 80 for all AP (LF is computed from AP)\n        chn_gains = [AP_GAIN] * len(imroTbl_data)\n    else:\n        # 3A, 3B1, 3B2 (NP 1.0)\n        chn_gains = [c[imroTbl_idx] for c in imroTbl_data]\n\n    chn_gains = np.array(chn_gains)[chn_ind]\n\n    return vmax / imax / chn_gains * 1e6  # convert to uV as well\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.extract_spike_waveforms", "title": "<code>extract_spike_waveforms(spikes, channel_ind, n_wf=500, wf_win=(-32, 32))</code>", "text": "<p>:param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike)</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n    :param spikes: spike times (in second) to extract waveforms\n    :param channel_ind: channel indices (of shankmap) to extract the waveforms from\n    :param n_wf: number of spikes per unit to extract the waveforms\n    :param wf_win: number of sample pre and post a spike\n    :return: waveforms (in uV) - shape: (sample x channel x spike)\n    \"\"\"\n    channel_bit_volts = self.get_channel_bit_volts(\"ap\")[channel_ind]\n\n    data = self.ap_timeseries\n\n    spikes = np.round(spikes * self.apmeta.meta[\"imSampRate\"]).astype(\n        int\n    )  # convert to sample\n    # ignore spikes at the beginning or end of raw data\n    spikes = spikes[\n        np.logical_and(spikes &gt; -wf_win[0], spikes &lt; data.shape[0] - wf_win[-1])\n    ]\n\n    np.random.shuffle(spikes)\n    spikes = spikes[:n_wf]\n    if len(spikes) &gt; 0:\n        # waveform at each spike: (sample x channel x spike)\n        spike_wfs = np.dstack(\n            [\n                data[int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind]\n                * channel_bit_volts\n                for spk in spikes\n            ]\n        )\n        return spike_wfs\n    else:  # if no spike found, return NaN of size (sample x channel x 1)\n        return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta", "title": "<code>SpikeGLXMeta</code>", "text": "Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>class SpikeGLXMeta:\n    def __init__(self, meta_filepath):\n\"\"\"\n        Some good processing references:\n            https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n            https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m\n        \"\"\"\n\n        self.fname = meta_filepath\n        self.meta = _read_meta(meta_filepath)\n\n        # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0)\n        probe_model = self.meta.get(\"imDatPrb_type\", 1)\n        if probe_model &lt;= 1:\n            if \"typeEnabled\" in self.meta:\n                self.probe_model = \"neuropixels 1.0 - 3A\"\n            elif \"typeImEnabled\" in self.meta:\n                self.probe_model = \"neuropixels 1.0 - 3B\"\n        elif probe_model == 1100:\n            self.probe_model = \"neuropixels UHD\"\n        elif probe_model == 21:\n            self.probe_model = \"neuropixels 2.0 - SS\"\n        elif probe_model == 24:\n            self.probe_model = \"neuropixels 2.0 - MS\"\n        else:\n            self.probe_model = str(probe_model)\n\n        # Get recording time\n        self.recording_time = datetime.strptime(\n            self.meta.get(\"fileCreateTime_original\", self.meta[\"fileCreateTime\"]),\n            \"%Y-%m-%dT%H:%M:%S\",\n        )\n        self.recording_duration = self.meta.get(\"fileTimeSecs\")\n\n        # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B\n        try:\n            self.probe_SN = self.meta.get(\"imProbeSN\", self.meta.get(\"imDatPrb_sn\"))\n        except KeyError:\n            raise KeyError(\n                \"Probe Serial Number not found in\"\n                ' either \"imProbeSN\" or \"imDatPrb_sn\"'\n            )\n\n        self.chanmap = (\n            self._parse_chanmap(self.meta[\"~snsChanMap\"])\n            if \"~snsChanMap\" in self.meta\n            else None\n        )\n        self.shankmap = (\n            self._parse_shankmap(self.meta[\"~snsShankMap\"])\n            if \"~snsShankMap\" in self.meta\n            else None\n        )\n        self.imroTbl = (\n            self._parse_imrotbl(self.meta[\"~imroTbl\"])\n            if \"~imroTbl\" in self.meta\n            else None\n        )\n\n        # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap\n        self.recording_channels = np.arange(len(self.imroTbl[\"data\"]))[\n            self.get_recording_channels_indices(exclude_sync=True)\n        ]\n\n    @staticmethod\n    def _parse_chanmap(raw):\n\"\"\"\n        https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#channel-map\n        Parse channel map header structure. Converts:\n\n            '(x,y,z)(c0,x:y)...(cI,x:y),(sy0;x:y)'\n\n        e.g:\n\n            '(384,384,1)(AP0;0:0)...(AP383;383:383)(SY0;768:768)'\n\n        into dict of form:\n\n            {'shape': [x,y,z], 'c0': [x,y], ... }\n        \"\"\"\n\n        res = {}\n        for u in (i.rstrip(\")\").split(\";\") for i in raw.split(\"(\") if i != \"\"):\n            if (len(u)) == 1:\n                res[\"shape\"] = u[0].split(\",\")\n            else:\n                res[u[0]] = u[1].split(\":\")\n\n        return res\n\n    @staticmethod\n    def _parse_shankmap(raw):\n\"\"\"\n        The shankmap contains details on the shank info\n            for each electrode sites of the sites being recorded only\n\n        https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#shank-map\n        Parse shank map header structure. Converts:\n\n            '(x,y,z)(a:b:c:d)...(a:b:c:d)'\n\n        e.g:\n\n            '(1,2,480)(0:0:192:1)...(0:1:191:1)'\n\n        into dict of form:\n\n            {'shape': [x,y,z], 'data': [[a,b,c,d],...]}\n        \"\"\"\n        res = {\"shape\": None, \"data\": []}\n\n        for u in (i.rstrip(\")\") for i in raw.split(\"(\") if i != \"\"):\n            if \",\" in u:\n                res[\"shape\"] = [int(d) for d in u.split(\",\")]\n            else:\n                res[\"data\"].append([int(d) for d in u.split(\":\")])\n\n        return res\n\n    @staticmethod\n    def _parse_imrotbl(raw):\n\"\"\"\n        The imro table contains info for all electrode sites (no sync)\n            for a particular electrode configuration (all 384 sites)\n        Note: not all of these 384 sites are necessarily recorded\n\n        https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#imro-per-channel-settings\n        Parse imro tbl structure. Converts:\n\n            '(X,Y,Z)(A B C D E)...(A B C D E)'\n\n        e.g.:\n\n            '(641251209,3,384)(0 1 0 500 250)...(383 0 0 500 250)'\n\n        into dict of form:\n\n            {'shape': (x,y,z), 'data': []}\n        \"\"\"\n        res = {\"shape\": None, \"data\": []}\n\n        for u in (i.rstrip(\")\") for i in raw.split(\"(\") if i != \"\"):\n            if \",\" in u:\n                res[\"shape\"] = [int(d) for d in u.split(\",\")]\n            else:\n                res[\"data\"].append([int(d) for d in u.split(\" \")])\n\n        return res\n\n    def get_recording_channels_indices(self, exclude_sync=False):\n\"\"\"\n        The indices of recorded channels (in chanmap)\n         with respect to the channels listed in the imro table\n        \"\"\"\n        recorded_chns_ind = [\n            int(v[0])\n            for k, v in self.chanmap.items()\n            if k != \"shape\" and (not k.startswith(\"SY\") if exclude_sync else True)\n        ]\n        orig_chns_ind = self.get_original_chans()\n        _, _, chns_ind = np.intersect1d(\n            orig_chns_ind, recorded_chns_ind, return_indices=True\n        )\n        return chns_ind\n\n    def get_original_chans(self):\n\"\"\"\n        Because you can selectively save channels, the\n        ith channel in the file isn't necessarily the ith acquired channel.\n        Use this function to convert from ith stored to original index.\n\n        Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n            OriginalChans() function\n        \"\"\"\n        if self.meta[\"snsSaveChanSubset\"] == \"all\":\n            # output = int32, 0 to nSavedChans - 1\n            channels = np.arange(0, int(self.meta[\"nSavedChans\"]))\n        else:\n            # parse the channel list self.meta['snsSaveChanSubset']\n            channels = np.arange(0)  # empty array\n            for channel_range in self.meta[\"snsSaveChanSubset\"].split(\",\"):\n                # a block of contiguous channels specified as chan or chan1:chan2 inclusive\n                ix = [int(r) for r in channel_range.split(\":\")]\n                assert len(ix) in (\n                    1,\n                    2,\n                ), f\"Invalid channel range spec '{channel_range}'\"\n                channels = np.append(channels, np.r_[ix[0] : ix[-1] + 1])\n        return channels\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.__init__", "title": "<code>__init__(meta_filepath)</code>", "text": "Some good processing references <p>https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def __init__(self, meta_filepath):\n\"\"\"\n    Some good processing references:\n        https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n        https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m\n    \"\"\"\n\n    self.fname = meta_filepath\n    self.meta = _read_meta(meta_filepath)\n\n    # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0)\n    probe_model = self.meta.get(\"imDatPrb_type\", 1)\n    if probe_model &lt;= 1:\n        if \"typeEnabled\" in self.meta:\n            self.probe_model = \"neuropixels 1.0 - 3A\"\n        elif \"typeImEnabled\" in self.meta:\n            self.probe_model = \"neuropixels 1.0 - 3B\"\n    elif probe_model == 1100:\n        self.probe_model = \"neuropixels UHD\"\n    elif probe_model == 21:\n        self.probe_model = \"neuropixels 2.0 - SS\"\n    elif probe_model == 24:\n        self.probe_model = \"neuropixels 2.0 - MS\"\n    else:\n        self.probe_model = str(probe_model)\n\n    # Get recording time\n    self.recording_time = datetime.strptime(\n        self.meta.get(\"fileCreateTime_original\", self.meta[\"fileCreateTime\"]),\n        \"%Y-%m-%dT%H:%M:%S\",\n    )\n    self.recording_duration = self.meta.get(\"fileTimeSecs\")\n\n    # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B\n    try:\n        self.probe_SN = self.meta.get(\"imProbeSN\", self.meta.get(\"imDatPrb_sn\"))\n    except KeyError:\n        raise KeyError(\n            \"Probe Serial Number not found in\"\n            ' either \"imProbeSN\" or \"imDatPrb_sn\"'\n        )\n\n    self.chanmap = (\n        self._parse_chanmap(self.meta[\"~snsChanMap\"])\n        if \"~snsChanMap\" in self.meta\n        else None\n    )\n    self.shankmap = (\n        self._parse_shankmap(self.meta[\"~snsShankMap\"])\n        if \"~snsShankMap\" in self.meta\n        else None\n    )\n    self.imroTbl = (\n        self._parse_imrotbl(self.meta[\"~imroTbl\"])\n        if \"~imroTbl\" in self.meta\n        else None\n    )\n\n    # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap\n    self.recording_channels = np.arange(len(self.imroTbl[\"data\"]))[\n        self.get_recording_channels_indices(exclude_sync=True)\n    ]\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_recording_channels_indices", "title": "<code>get_recording_channels_indices(exclude_sync=False)</code>", "text": "<p>The indices of recorded channels (in chanmap)  with respect to the channels listed in the imro table</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def get_recording_channels_indices(self, exclude_sync=False):\n\"\"\"\n    The indices of recorded channels (in chanmap)\n     with respect to the channels listed in the imro table\n    \"\"\"\n    recorded_chns_ind = [\n        int(v[0])\n        for k, v in self.chanmap.items()\n        if k != \"shape\" and (not k.startswith(\"SY\") if exclude_sync else True)\n    ]\n    orig_chns_ind = self.get_original_chans()\n    _, _, chns_ind = np.intersect1d(\n        orig_chns_ind, recorded_chns_ind, return_indices=True\n    )\n    return chns_ind\n</code></pre>"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_original_chans", "title": "<code>get_original_chans()</code>", "text": "<p>Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index.</p> <p>Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip     OriginalChans() function</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def get_original_chans(self):\n\"\"\"\n    Because you can selectively save channels, the\n    ith channel in the file isn't necessarily the ith acquired channel.\n    Use this function to convert from ith stored to original index.\n\n    Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n        OriginalChans() function\n    \"\"\"\n    if self.meta[\"snsSaveChanSubset\"] == \"all\":\n        # output = int32, 0 to nSavedChans - 1\n        channels = np.arange(0, int(self.meta[\"nSavedChans\"]))\n    else:\n        # parse the channel list self.meta['snsSaveChanSubset']\n        channels = np.arange(0)  # empty array\n        for channel_range in self.meta[\"snsSaveChanSubset\"].split(\",\"):\n            # a block of contiguous channels specified as chan or chan1:chan2 inclusive\n            ix = [int(r) for r in channel_range.split(\":\")]\n            assert len(ix) in (\n                1,\n                2,\n            ), f\"Invalid channel range spec '{channel_range}'\"\n            channels = np.append(channels, np.r_[ix[0] : ix[-1] + 1])\n    return channels\n</code></pre>"}, {"location": "api/element_array_ephys/readers/utils/", "title": "utils.py", "text": ""}, {"location": "api/workflow_array_ephys/analysis/", "title": "analysis.py", "text": ""}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.activate", "title": "<code>activate(schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>schema name on the database server</p> required <code>create_schema</code> <code>bool</code> <p>when True (default), create schema in the database if it                 does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>str</code> <p>when True (default), create schema tables in the database                  if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>a module (or name) containing the required dependencies.</p> <code>None</code> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>def activate(\n    schema_name, *, create_schema=True, create_tables=True, linking_module=None\n):\n\"\"\"Activate this schema.\n\n    Args:\n        schema_name (str): schema name on the database server\n        create_schema (bool): when True (default), create schema in the database if it\n                            does not yet exist.\n        create_tables (str): when True (default), create schema tables in the database\n                             if they do not yet exist.\n        linking_module (str): a module (or name) containing the required dependencies.\n    \"\"\"\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(linking_module), (\n        \"The argument 'dependency' must \" + \"be a module's name or a module\"\n    )\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignmentCondition", "title": "<code>SpikesAlignmentCondition</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Alignment activity table</p> <p>Attributes:</p> Name Type Description <code>ephys.CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering foreign key</p> <code>event.AlignmentEvent</code> <code>foreign key</code> <p>AlignmentEvent foreign key</p> <code>trial_condition</code> <code>foreign key</code> <p>varchar(128) # user-friendly name of condition</p> <code>condition_description</code> <code> varchar(1000), nullable</code> <p>condition description</p> <code>bin_size</code> <code>float</code> <p>Bin-size (in second) used to compute the PSTH Default 0.04</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>@schema\nclass SpikesAlignmentCondition(dj.Manual):\n\"\"\"Alignment activity table\n\n    Attributes:\n        ephys.CuratedClustering (foreign key): CuratedClustering foreign key\n        event.AlignmentEvent (foreign key): AlignmentEvent foreign key\n        trial_condition: varchar(128) # user-friendly name of condition\n        condition_description ( varchar(1000), nullable): condition description\n        bin_size (float, optional): Bin-size (in second) used to compute the PSTH\n            Default 0.04\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.CuratedClustering\n    -&gt; event.AlignmentEvent\n    trial_condition: varchar(128) # user-friendly name of condition\n    ---\n    condition_description='': varchar(1000)\n    bin_size=0.04: float # bin-size (in second) used to compute the PSTH\n    \"\"\"\n\n    class Trial(dj.Part):\n\"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\"\n\n        definition = \"\"\"  # Trials on which to compute event-aligned spikes and PSTH\n        -&gt; master\n        -&gt; trial.Trial\n        \"\"\"\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignmentCondition.Trial", "title": "<code>Trial</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Trials on which to compute event-aligned spikes and PSTH</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>class Trial(dj.Part):\n\"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\"\n\n    definition = \"\"\"  # Trials on which to compute event-aligned spikes and PSTH\n    -&gt; master\n    -&gt; trial.Trial\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment", "title": "<code>SpikesAlignment</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>@schema\nclass SpikesAlignment(dj.Computed):\n\"\"\"Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH\"\"\"\n\n    definition = \"\"\"\n    -&gt; SpikesAlignmentCondition\n    \"\"\"\n\n    class AlignedTrialSpikes(dj.Part):\n\"\"\"Spike activity aligned to the event time within the designated window\n\n        Attributes:\n            SpikesAlignmentCondition.Trial (foreign key): Trial foreign key\n            ephys.CuratedClustering.Unit (foreign key): Unit foreign key\n            SpikesAlignmentCondition.Trial (foreign key): Trial foreign key\n            aligned_spike_times (longblob): (s) spike times relative to alignment event\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; ephys.CuratedClustering.Unit\n        -&gt; SpikesAlignmentCondition.Trial\n        ---\n        aligned_spike_times: longblob # (s) spike times relative to alignment event time\n        \"\"\"\n\n    class UnitPSTH(dj.Part):\n\"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit\n\n        Attributes:\n            SpikesAlignment (foreign key): SpikesAlignment foreign key\n            ephys.CuratedClustering.Unit (foreign key): Unit foreign key\n            psth (longblob): event-aligned spike peristimulus time histogram (PSTH)\n            psth_edges (longblob): set of PSTH edges\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; ephys.CuratedClustering.Unit\n        ---\n        psth: longblob  # event-aligned spike peristimulus time histogram (PSTH)\n        psth_edges: longblob\n        \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH\n\n        Args:\n            key (dict): Dict uniquely identifying one SpikesAlignmentCondition\n        \"\"\"\n        unit_keys, unit_spike_times = (\n            _linking_module.ephys.CuratedClustering.Unit &amp; key\n        ).fetch(\"KEY\", \"spike_times\", order_by=\"unit\")\n        bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n\n        trialized_event_times = (\n            _linking_module.trial.get_trialized_alignment_event_times(\n                key,\n                _linking_module.trial.Trial &amp; (SpikesAlignmentCondition.Trial &amp; key),\n            )\n        )\n\n        min_limit = (trialized_event_times.event - trialized_event_times.start).max()\n        max_limit = (trialized_event_times.end - trialized_event_times.event).max()\n\n        # Spike raster\n        aligned_trial_spikes = []\n        units_spike_raster = {\n            u[\"unit\"]: {**key, **u, \"aligned_spikes\": []} for u in unit_keys\n        }\n        for _, r in trialized_event_times.iterrows():\n            if np.isnan(r.event):\n                continue\n            alignment_start_time = r.event - min_limit\n            alignment_end_time = r.event + max_limit\n            for unit_key, spikes in zip(unit_keys, unit_spike_times):\n                aligned_spikes = (\n                    spikes[\n                        (alignment_start_time &lt;= spikes) &amp; (spikes &lt; alignment_end_time)\n                    ]\n                    - r.event\n                )\n                aligned_trial_spikes.append(\n                    {\n                        **key,\n                        **unit_key,\n                        **r.trial_key,\n                        \"aligned_spike_times\": aligned_spikes,\n                    }\n                )\n                units_spike_raster[unit_key[\"unit\"]][\"aligned_spikes\"].append(\n                    aligned_spikes\n                )\n\n        # PSTH\n        for unit_spike_raster in units_spike_raster.values():\n            spikes = np.concatenate(unit_spike_raster[\"aligned_spikes\"])\n\n            psth, edges = np.histogram(\n                spikes, bins=np.arange(-min_limit, max_limit, bin_size)\n            )\n            unit_spike_raster[\"psth\"] = (\n                psth / len(unit_spike_raster.pop(\"aligned_spikes\")) / bin_size\n            )\n            unit_spike_raster[\"psth_edges\"] = edges[1:]\n\n        self.insert1(key)\n        self.AlignedTrialSpikes.insert(aligned_trial_spikes)\n        self.UnitPSTH.insert(list(units_spike_raster.values()))\n\n    def plot(self, key: dict, unit: int, axs: tuple = None) -&gt; Figure:\n\"\"\"Plot event-aligned and trial-averaged spiking\n\n        Args:\n            key (dict): key of SpikesAlignmentCondition master table\n            unit (int): ID of ephys.CuratedClustering.Unit table\n            axs (tuple, optional): Definition of axes for plot.\n                Default is plt.subplots(2, 1, figsize=(12, 8))\n\n        Returns:\n            fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes\n        \"\"\"\n        from .plotting import plot_psth\n\n        fig = None\n        if axs is None:\n            fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n\n        bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n        trial_ids, aligned_spikes = (\n            self.AlignedTrialSpikes &amp; key &amp; {\"unit\": unit}\n        ).fetch(\"trial_id\", \"aligned_spike_times\")\n        psth, psth_edges = (self.UnitPSTH &amp; key &amp; {\"unit\": unit}).fetch1(\n            \"psth\", \"psth_edges\"\n        )\n\n        xlim = psth_edges[0], psth_edges[-1]\n\n        plot_psth._plot_spike_raster(\n            aligned_spikes,\n            trial_ids=trial_ids,\n            ax=axs[0],\n            title=f\"{dict(**key, unit=unit)}\",\n            xlim=xlim,\n        )\n        plot_psth._plot_psth(psth, psth_edges, bin_size, ax=axs[1], title=\"\", xlim=xlim)\n\n        return fig\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.AlignedTrialSpikes", "title": "<code>AlignedTrialSpikes</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Spike activity aligned to the event time within the designated window</p> <p>Attributes:</p> Name Type Description <code>SpikesAlignmentCondition.Trial</code> <code>foreign key</code> <p>Trial foreign key</p> <code>ephys.CuratedClustering.Unit</code> <code>foreign key</code> <p>Unit foreign key</p> <code>SpikesAlignmentCondition.Trial</code> <code>foreign key</code> <p>Trial foreign key</p> <code>aligned_spike_times</code> <code>longblob</code> <p>(s) spike times relative to alignment event</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>class AlignedTrialSpikes(dj.Part):\n\"\"\"Spike activity aligned to the event time within the designated window\n\n    Attributes:\n        SpikesAlignmentCondition.Trial (foreign key): Trial foreign key\n        ephys.CuratedClustering.Unit (foreign key): Unit foreign key\n        SpikesAlignmentCondition.Trial (foreign key): Trial foreign key\n        aligned_spike_times (longblob): (s) spike times relative to alignment event\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; ephys.CuratedClustering.Unit\n    -&gt; SpikesAlignmentCondition.Trial\n    ---\n    aligned_spike_times: longblob # (s) spike times relative to alignment event time\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.UnitPSTH", "title": "<code>UnitPSTH</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Event-aligned spike peristimulus time histogram (PSTH) by unit</p> <p>Attributes:</p> Name Type Description <code>SpikesAlignment</code> <code>foreign key</code> <p>SpikesAlignment foreign key</p> <code>ephys.CuratedClustering.Unit</code> <code>foreign key</code> <p>Unit foreign key</p> <code>psth</code> <code>longblob</code> <p>event-aligned spike peristimulus time histogram (PSTH)</p> <code>psth_edges</code> <code>longblob</code> <p>set of PSTH edges</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>class UnitPSTH(dj.Part):\n\"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit\n\n    Attributes:\n        SpikesAlignment (foreign key): SpikesAlignment foreign key\n        ephys.CuratedClustering.Unit (foreign key): Unit foreign key\n        psth (longblob): event-aligned spike peristimulus time histogram (PSTH)\n        psth_edges (longblob): set of PSTH edges\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; ephys.CuratedClustering.Unit\n    ---\n    psth: longblob  # event-aligned spike peristimulus time histogram (PSTH)\n    psth_edges: longblob\n    \"\"\"\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.make", "title": "<code>make(key)</code>", "text": "<p>Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dict uniquely identifying one SpikesAlignmentCondition</p> required Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH\n\n    Args:\n        key (dict): Dict uniquely identifying one SpikesAlignmentCondition\n    \"\"\"\n    unit_keys, unit_spike_times = (\n        _linking_module.ephys.CuratedClustering.Unit &amp; key\n    ).fetch(\"KEY\", \"spike_times\", order_by=\"unit\")\n    bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n\n    trialized_event_times = (\n        _linking_module.trial.get_trialized_alignment_event_times(\n            key,\n            _linking_module.trial.Trial &amp; (SpikesAlignmentCondition.Trial &amp; key),\n        )\n    )\n\n    min_limit = (trialized_event_times.event - trialized_event_times.start).max()\n    max_limit = (trialized_event_times.end - trialized_event_times.event).max()\n\n    # Spike raster\n    aligned_trial_spikes = []\n    units_spike_raster = {\n        u[\"unit\"]: {**key, **u, \"aligned_spikes\": []} for u in unit_keys\n    }\n    for _, r in trialized_event_times.iterrows():\n        if np.isnan(r.event):\n            continue\n        alignment_start_time = r.event - min_limit\n        alignment_end_time = r.event + max_limit\n        for unit_key, spikes in zip(unit_keys, unit_spike_times):\n            aligned_spikes = (\n                spikes[\n                    (alignment_start_time &lt;= spikes) &amp; (spikes &lt; alignment_end_time)\n                ]\n                - r.event\n            )\n            aligned_trial_spikes.append(\n                {\n                    **key,\n                    **unit_key,\n                    **r.trial_key,\n                    \"aligned_spike_times\": aligned_spikes,\n                }\n            )\n            units_spike_raster[unit_key[\"unit\"]][\"aligned_spikes\"].append(\n                aligned_spikes\n            )\n\n    # PSTH\n    for unit_spike_raster in units_spike_raster.values():\n        spikes = np.concatenate(unit_spike_raster[\"aligned_spikes\"])\n\n        psth, edges = np.histogram(\n            spikes, bins=np.arange(-min_limit, max_limit, bin_size)\n        )\n        unit_spike_raster[\"psth\"] = (\n            psth / len(unit_spike_raster.pop(\"aligned_spikes\")) / bin_size\n        )\n        unit_spike_raster[\"psth_edges\"] = edges[1:]\n\n    self.insert1(key)\n    self.AlignedTrialSpikes.insert(aligned_trial_spikes)\n    self.UnitPSTH.insert(list(units_spike_raster.values()))\n</code></pre>"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.plot", "title": "<code>plot(key, unit, axs=None)</code>", "text": "<p>Plot event-aligned and trial-averaged spiking</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key of SpikesAlignmentCondition master table</p> required <code>unit</code> <code>int</code> <p>ID of ephys.CuratedClustering.Unit table</p> required <code>axs</code> <code>tuple</code> <p>Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8))</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Plot event-aligned and trial-averaged spikes</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>def plot(self, key: dict, unit: int, axs: tuple = None) -&gt; Figure:\n\"\"\"Plot event-aligned and trial-averaged spiking\n\n    Args:\n        key (dict): key of SpikesAlignmentCondition master table\n        unit (int): ID of ephys.CuratedClustering.Unit table\n        axs (tuple, optional): Definition of axes for plot.\n            Default is plt.subplots(2, 1, figsize=(12, 8))\n\n    Returns:\n        fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes\n    \"\"\"\n    from .plotting import plot_psth\n\n    fig = None\n    if axs is None:\n        fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n\n    bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n    trial_ids, aligned_spikes = (\n        self.AlignedTrialSpikes &amp; key &amp; {\"unit\": unit}\n    ).fetch(\"trial_id\", \"aligned_spike_times\")\n    psth, psth_edges = (self.UnitPSTH &amp; key &amp; {\"unit\": unit}).fetch1(\n        \"psth\", \"psth_edges\"\n    )\n\n    xlim = psth_edges[0], psth_edges[-1]\n\n    plot_psth._plot_spike_raster(\n        aligned_spikes,\n        trial_ids=trial_ids,\n        ax=axs[0],\n        title=f\"{dict(**key, unit=unit)}\",\n        xlim=xlim,\n    )\n    plot_psth._plot_psth(psth, psth_edges, bin_size, ax=axs[1], title=\"\", xlim=xlim)\n\n    return fig\n</code></pre>"}, {"location": "api/workflow_array_ephys/export/", "title": "export.py", "text": "<p>For didactic purposes, import upstream NWB export functions</p> <p>Real use-cases should import these functions directly.</p>"}, {"location": "api/workflow_array_ephys/export/#workflow_array_ephys.export.ecephys_session_to_nwb", "title": "<code>ecephys_session_to_nwb(session_key, raw=True, spikes=True, lfp='source', end_frame=None, lab_key=None, project_key=None, protocol_key=None, nwbfile_kwargs=None)</code>", "text": "<p>Main function for converting ephys data to NWB</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>raw</code> <code>bool</code> <p>Optional. Default True. Include the raw data from source. SpikeGLX &amp; OpenEphys are supported</p> <code>True</code> <code>spikes</code> <code>bool</code> <p>Optional. Default True. Whether to include CuratedClustering</p> <code>True</code> <code>lfp</code> <code>str</code> <p>One of the following. \"dj\", read LFP data from ephys.LFP. \"source\", read LFP data from source (SpikeGLX supported). False, do not convert LFP.</p> <code>'source'</code> <code>end_frame</code> <code>int</code> <p>Optional limit on frames for small test conversions.</p> <code>None</code> <code>lab_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>project_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>protocol_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>nwbfile_kwargs</code> <code>dict</code> <p>Optional. If Element Session is not used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required  minimal data for instantiating an NWBFile object. If element-session is  being used, this argument can optionally be used to overwrite NWBFile  fields.</p> <code>None</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def ecephys_session_to_nwb(\n    session_key,\n    raw=True,\n    spikes=True,\n    lfp=\"source\",\n    end_frame=None,\n    lab_key=None,\n    project_key=None,\n    protocol_key=None,\n    nwbfile_kwargs=None,\n):\n\"\"\"Main function for converting ephys data to NWB\n\n    Arguments:\n        session_key (dict): key from Session table\n        raw (bool): Optional. Default True. Include the raw data from source.\n            SpikeGLX &amp; OpenEphys are supported\n        spikes (bool): Optional. Default True. Whether to include CuratedClustering\n        lfp (str): One of the following.\n            \"dj\", read LFP data from ephys.LFP.\n            \"source\", read LFP data from source (SpikeGLX supported).\n            False, do not convert LFP.\n        end_frame (int): Optional limit on frames for small test conversions.\n        lab_key (dict): Optional key to add metadata from other Element Lab.\n        project_key (dict): Optional key to add metadata from other Element Lab.\n        protocol_key (dict): Optional key to add metadata from other Element Lab.\n        nwbfile_kwargs (dict): Optional. If Element Session is not used, this argument\n            is required and must be a dictionary containing 'session_description' (str),\n            'identifier' (str), and 'session_start_time' (datetime), the required\n             minimal data for instantiating an NWBFile object. If element-session is\n             being used, this argument can optionally be used to overwrite NWBFile\n             fields.\n    \"\"\"\n\n    session_to_nwb = getattr(ephys._linking_module, \"session_to_nwb\", False)\n\n    if session_to_nwb:\n        nwbfile = session_to_nwb(\n            session_key,\n            lab_key=lab_key,\n            project_key=project_key,\n            protocol_key=protocol_key,\n            additional_nwbfile_kwargs=nwbfile_kwargs,\n        )\n    else:\n        nwbfile = pynwb.NWBFile(**nwbfile_kwargs)\n\n    ephys_root_data_dir = ephys.get_ephys_root_data_dir()\n\n    if raw:\n        add_ephys_recording_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    if spikes:\n        add_ephys_units_to_nwb(session_key, nwbfile)\n\n    if lfp == \"dj\":\n        add_ephys_lfp_from_dj_to_nwb(session_key, nwbfile)\n\n    if lfp == \"source\":\n        add_ephys_lfp_from_source_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    return nwbfile\n</code></pre>"}, {"location": "api/workflow_array_ephys/export/#workflow_array_ephys.export.write_nwb", "title": "<code>write_nwb(nwbfile, fname, check_read=True)</code>", "text": "<p>Export NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>NWBFile</code> <p>nwb file</p> required <code>fname</code> <code>str</code> <p>Absolute path including <code>*.nwb</code> extension.</p> required <code>check_read</code> <code>bool</code> <p>If True, PyNWB will try to read the produced NWB file and ensure that it can be read.</p> <code>True</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def write_nwb(nwbfile, fname, check_read=True):\n\"\"\"Export NWBFile\n\n    Arguments:\n        nwbfile (NWBFile): nwb file\n        fname (str): Absolute path including `*.nwb` extension.\n        check_read (bool): If True, PyNWB will try to read the produced NWB file and\n            ensure that it can be read.\n    \"\"\"\n    with pynwb.NWBHDF5IO(fname, \"w\") as io:\n        io.write(nwbfile)\n\n    if check_read:\n        with pynwb.NWBHDF5IO(fname, \"r\") as io:\n            io.read()\n</code></pre>"}, {"location": "api/workflow_array_ephys/ingest/", "title": "ingest.py", "text": ""}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n\n    data_dir = dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_lab", "title": "<code>ingest_lab(lab_csv_path='./user_data/lab/labs.csv', project_csv_path='./user_data/lab/projects.csv', publication_csv_path='./user_data/lab/publications.csv', keyword_csv_path='./user_data/lab/keywords.csv', protocol_csv_path='./user_data/lab/protocols.csv', users_csv_path='./user_data/lab/users.csv', project_user_csv_path='./user_data/lab/project_users.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Inserts data from a CSVs into their corresponding lab schema tables.</p> <p>By default, uses data from workflow/user_data/lab/</p> <p>Parameters:</p> Name Type Description Default <code>lab_csv_path</code> <code>str</code> <p>relative path of lab csv</p> <code>'./user_data/lab/labs.csv'</code> <code>project_csv_path</code> <code>str</code> <p>relative path of project csv</p> <code>'./user_data/lab/projects.csv'</code> <code>publication_csv_path</code> <code>str</code> <p>relative path of publication csv</p> <code>'./user_data/lab/publications.csv'</code> <code>keyword_csv_path</code> <code>str</code> <p>relative path of keyword csv</p> <code>'./user_data/lab/keywords.csv'</code> <code>protocol_csv_path</code> <code>str</code> <p>relative path of protocol csv</p> <code>'./user_data/lab/protocols.csv'</code> <code>users_csv_path</code> <code>str</code> <p>relative path of users csv</p> <code>'./user_data/lab/users.csv'</code> <code>project_user_csv_path</code> <code>str</code> <p>relative path of project users csv</p> <code>'./user_data/lab/project_users.csv'</code> <code>skip_duplicates=True</code> <code>str</code> <p>datajoint insert function param</p> required <code>verbose</code> <code>str</code> <p>print number inserted (i.e., table length change)</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_lab(\n    lab_csv_path=\"./user_data/lab/labs.csv\",\n    project_csv_path=\"./user_data/lab/projects.csv\",\n    publication_csv_path=\"./user_data/lab/publications.csv\",\n    keyword_csv_path=\"./user_data/lab/keywords.csv\",\n    protocol_csv_path=\"./user_data/lab/protocols.csv\",\n    users_csv_path=\"./user_data/lab/users.csv\",\n    project_user_csv_path=\"./user_data/lab/project_users.csv\",\n    skip_duplicates=True,\n    verbose=True,\n):\n\"\"\"Inserts data from a CSVs into their corresponding lab schema tables.\n\n    By default, uses data from workflow/user_data/lab/\n\n    Args:\n        lab_csv_path (str): relative path of lab csv\n        project_csv_path (str): relative path of project csv\n        publication_csv_path (str): relative path of publication csv\n        keyword_csv_path (str): relative path of keyword csv\n        protocol_csv_path (str): relative path of protocol csv\n        users_csv_path (str): relative path of users csv\n        project_user_csv_path (str): relative path of project users csv\n        skip_duplicates=True (str): datajoint insert function param\n        verbose (str): print number inserted (i.e., table length change)\n    \"\"\"\n\n    # List with repeats for when multiple dj.tables fed by same CSV\n    csvs = [\n        lab_csv_path,\n        lab_csv_path,\n        lab_csv_path,\n        lab_csv_path,\n        project_csv_path,\n        project_csv_path,\n        publication_csv_path,\n        keyword_csv_path,\n        protocol_csv_path,\n        protocol_csv_path,\n        users_csv_path,\n        users_csv_path,\n        users_csv_path,\n        project_user_csv_path,\n    ]\n    tables = [\n        lab.Organization(),\n        lab.Lab(),\n        lab.Lab.Organization(),\n        lab.Location(),\n        lab.Project(),\n        lab.ProjectSourceCode(),\n        lab.ProjectPublication(),\n        lab.ProjectKeywords(),\n        lab.ProtocolType(),\n        lab.Protocol(),\n        lab.UserRole(),\n        lab.User(),\n        lab.LabMembership(),\n        lab.ProjectUser(),\n    ]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n\n    # For project schema\n    project_csvs = [\n        project_csv_path,\n        project_user_csv_path,\n        keyword_csv_path,\n        publication_csv_path,\n        project_csv_path,\n    ]\n\n    project_tables = [\n        project.Project(),\n        project.ProjectPersonnel(),\n        project.ProjectKeywords(),\n        project.ProjectPublication(),\n        project.ProjectSourceCode(),\n    ]\n\n    ingest_csv_to_table(\n        project_csvs, project_tables, skip_duplicates=skip_duplicates, verbose=verbose\n    )\n</code></pre>"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_subjects", "title": "<code>ingest_subjects(subject_csv_path='./user_data/subjects.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest subjects listed in the subject column of ./user_data/subjects.csv</p> <p>Parameters:</p> Name Type Description Default <code>subject_csv_path</code> <code>str</code> <p>Relative path to subject csv. Defaults to \"./user_data/subjects.csv\".</p> <code>'./user_data/subjects.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>See DataJoint <code>insert</code> function. Default True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_subjects(\n    subject_csv_path: str = \"./user_data/subjects.csv\",\n    skip_duplicates: bool = True,\n    verbose: bool = True,\n):\n\"\"\"Ingest subjects listed in the subject column of ./user_data/subjects.csv\n\n    Args:\n        subject_csv_path (str, optional): Relative path to subject csv.\n            Defaults to \"./user_data/subjects.csv\".\n        skip_duplicates (bool, optional): See DataJoint `insert` function. Default True.\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n    \"\"\"\n    csvs = [subject_csv_path]\n    tables = [subject.Subject()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_sessions", "title": "<code>ingest_sessions(session_csv_path='./user_data/sessions.csv', verbose=True, **_)</code>", "text": "<p>Ingest SpikeGLX and OpenEphys files from directories listed in csv</p> <p>Parameters:</p> Name Type Description Default <code>session_csv_path</code> <code>str</code> <p>List of sessions. Defaults to \"./user_data/sessions.csv\".</p> <code>'./user_data/sessions.csv'</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Neither SpikeGLX nor OpenEphys recording files found in dir</p> <code>NotImplementedError</code> <p>Acquisition software provided does not match those implemented - SpikeGLX and OpenEphys</p> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_sessions(\n    session_csv_path: str = \"./user_data/sessions.csv\", verbose: bool = True, **_\n):\n\"\"\"Ingest SpikeGLX and OpenEphys files from directories listed in csv\n\n    Args:\n        session_csv_path (str, optional): List of sessions.\n            Defaults to \"./user_data/sessions.csv\".\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n\n    Raises:\n        FileNotFoundError: Neither SpikeGLX nor OpenEphys recording files found in dir\n        NotImplementedError: Acquisition software provided does not match those\n            implemented - SpikeGLX and OpenEphys\n    \"\"\"\n\n    # ---------- Insert new \"Session\" and \"ProbeInsertion\" ---------\n    with open(session_csv_path, newline=\"\") as f:\n        input_sessions = list(csv.DictReader(f, delimiter=\",\"))\n\n    # Folder structure: root / subject / session / probe / .ap.meta\n    (session_list, session_dir_list) = ([], [])\n    session_note_list, session_experimenter_list, lab_user_list = [], [], []\n    probe_list, probe_insertion_list = [], []\n\n    for this_session in input_sessions:\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), this_session[\"session_dir\"]\n        )\n        session_datetimes, insertions = [], []\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in zip(\n            [\"*.ap.meta\", \"*.oebin\"], [\"SpikeGLX\", \"OpenEphys\"]\n        ):\n            ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n            if len(ephys_meta_filepaths):\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                \"Ephys recording data not found! Neither SpikeGLX \"\n                + \"nor OpenEphys recording files found in: \"\n                + f\"{session_dir}\"\n            )\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                probe_key = {\n                    \"probe_type\": spikeglx_meta.probe_model,\n                    \"probe\": spikeglx_meta.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n\n                probe_dir = meta_filepath.parent\n                probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                probe_number = int(probe_number.replace(\"imec\", \"\"))\n\n                insertions.append(\n                    {\n                        \"probe\": spikeglx_meta.probe_SN,\n                        \"insertion_number\": int(probe_number),\n                    }\n                )\n                session_datetimes.append(spikeglx_meta.recording_time)\n        elif acq_software == \"OpenEphys\":\n            loaded_oe = openephys.OpenEphys(session_dir)\n            session_datetimes.append(loaded_oe.experiment.datetime)\n            for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n                probe_key = {\n                    \"probe_type\": oe_probe.probe_model,\n                    \"probe\": oe_probe.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n                insertions.append(\n                    {\"probe\": oe_probe.probe_SN, \"insertion_number\": probe_idx}\n                )\n        else:\n            raise NotImplementedError(\n                \"Unknown acquisition software: \" + f\"{acq_software}\"\n            )\n\n        # new session/probe-insertion\n        session_key = {\n            \"subject\": this_session[\"subject\"],\n            \"session_datetime\": min(session_datetimes),\n        }\n        if session_key not in session.Session():\n            session_list.append(session_key)\n            root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n            session_dir_list.append(\n                {\n                    **session_key,\n                    \"session_dir\": session_dir.relative_to(root_dir).as_posix(),\n                }\n            )\n            session_note_list.append(\n                {**session_key, \"session_note\": this_session[\"session_note\"]}\n            )\n            session_experimenter_list.append(\n                {**session_key, \"user\": this_session[\"user\"]}\n            )\n            lab_user_list.append(\n                (this_session[\"user\"], \"\", \"\", \"\")\n            )  # empty email/phone/name\n            probe_insertion_list.extend(\n                [{**session_key, **insertion} for insertion in insertions]\n            )\n\n    session.Session.insert(session_list)\n    lab.User.insert(lab_user_list, skip_duplicates=True)\n    session.SessionDirectory.insert(session_dir_list)\n    session.SessionNote.insert(session_note_list)\n    session.SessionExperimenter.insert(session_experimenter_list)\n\n    log_string = \"---- Inserting %d entry(s) into %s ----\"\n\n    if verbose:\n        logger.info(log_string % (len(session_list), \"session.Session\"))\n\n    probe.Probe.insert(probe_list)\n    if verbose:\n        logger.info(log_string % (len(probe_list), \"probe.Probe\"))\n\n    ephys.ProbeInsertion.insert(probe_insertion_list)\n    if verbose:\n        logger.info(log_string % (len(probe_insertion_list), \"ephys.ProbeInsertion\"))\n        logger.info(\"---- Successfully completed ingest_subjects ----\")\n</code></pre>"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_events", "title": "<code>ingest_events(recording_csv_path='./user_data/behavior_recordings.csv', block_csv_path='./user_data/blocks.csv', trial_csv_path='./user_data/trials.csv', event_csv_path='./user_data/events.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest each level of experiment hierarchy for element-trial</p> Ingestion hierarchy includes <p>recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial).</p> <p>Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging</p> <p>Parameters:</p> Name Type Description Default <code>recording_csv_path</code> <code>str</code> <p>Relative path to recording csv. Defaults to \"./user_data/behavior_recordings.csv\".</p> <code>'./user_data/behavior_recordings.csv'</code> <code>block_csv_path</code> <code>str</code> <p>Relative path to block csv. Defaults to \"./user_data/blocks.csv\".</p> <code>'./user_data/blocks.csv'</code> <code>trial_csv_path</code> <code>str</code> <p>Relative path to trial csv. Defaults to \"./user_data/trials.csv\".</p> <code>'./user_data/trials.csv'</code> <code>event_csv_path</code> <code>str</code> <p>Relative path to event csv. Defaults to \"./user_data/events.csv\".</p> <code>'./user_data/events.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>See DataJoint <code>insert</code> function. Default True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_events(\n    recording_csv_path: str = \"./user_data/behavior_recordings.csv\",\n    block_csv_path: str = \"./user_data/blocks.csv\",\n    trial_csv_path: str = \"./user_data/trials.csv\",\n    event_csv_path: str = \"./user_data/events.csv\",\n    skip_duplicates: bool = True,\n    verbose: bool = True,\n):\n\"\"\"Ingest each level of experiment hierarchy for element-trial\n\n    Ingestion hierarchy includes:\n        recording, block (i.e., phases of trials), trials (repeated units),\n        events (optionally 0-duration occurrences within trial).\n\n    Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging\n\n    Args:\n        recording_csv_path (str, optional): Relative path to recording csv.\n            Defaults to \"./user_data/behavior_recordings.csv\".\n        block_csv_path (str, optional): Relative path to block csv.\n            Defaults to \"./user_data/blocks.csv\".\n        trial_csv_path (str, optional): Relative path to trial csv.\n            Defaults to \"./user_data/trials.csv\".\n        event_csv_path (str, optional): Relative path to event csv.\n            Defaults to \"./user_data/events.csv\".\n        skip_duplicates (bool, optional): See DataJoint `insert` function. Default True.\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n    \"\"\"\n    csvs = [\n        recording_csv_path,\n        recording_csv_path,\n        block_csv_path,\n        block_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        event_csv_path,\n        event_csv_path,\n        event_csv_path,\n    ]\n    tables = [\n        event.BehaviorRecording(),\n        event.BehaviorRecording.File(),\n        trial.Block(),\n        trial.Block.Attribute(),\n        trial.TrialType(),\n        trial.Trial(),\n        trial.Trial.Attribute(),\n        trial.BlockTrial(),\n        event.EventType(),\n        event.Event(),\n        trial.TrialEvent(),\n    ]\n\n    # Allow direct insert required because element-event has Imported that should be Manual\n    ingest_csv_to_table(\n        csvs,\n        tables,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n        allow_direct_insert=True,\n    )\n</code></pre>"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_alignment", "title": "<code>ingest_alignment(alignment_csv_path='./user_data/alignments.csv', skip_duplicates=True, verbose=True)</code>", "text": "<p>Ingest event alignment data from local CSVs</p> <p>Note: This is duplicated across wf-array-ephys and wf-calcium-imaging</p> <p>Parameters:</p> Name Type Description Default <code>alignment_csv_path</code> <code>str</code> <p>Relative path to event alignment csv. Defaults to \"./user_data/alignments.csv\".</p> <code>'./user_data/alignments.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>See DataJoint <code>insert</code> function. Default True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_alignment(\n    alignment_csv_path: str = \"./user_data/alignments.csv\",\n    skip_duplicates: bool = True,\n    verbose: bool = True,\n):\n\"\"\"Ingest event alignment data from local CSVs\n\n    Note: This is duplicated across wf-array-ephys and wf-calcium-imaging\n\n    Args:\n        alignment_csv_path (str, optional): Relative path to event alignment csv.\n            Defaults to \"./user_data/alignments.csv\".\n        skip_duplicates (bool, optional): See DataJoint `insert` function. Default True.\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n    \"\"\"\n\n    csvs = [alignment_csv_path]\n    tables = [event.AlignmentEvent()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"}, {"location": "api/workflow_array_ephys/localization/", "title": "localization.py", "text": "<p>Load CCF files.</p> <p>When imported, checks to see that CCF (nrrd and query.csv) files in the ephys_root_data_dir have been loaded into element_electrode_localization.coordinate_framework. Default voxel resolution is 100. To load other resolutions, please modify this script.</p>"}, {"location": "api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n\n    data_dir = dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Return relative path from SessionDirectory table given key</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>Key uniquely identifying a session</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Relative path of session directory</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Return relative path from SessionDirectory table given key\n\n    Args:\n        session_key (dict): Key uniquely identifying a session\n\n    Returns:\n        path (str): Relative path of session directory\n    \"\"\"\n    from .pipeline import session\n\n    return (session.SessionDirectory &amp; session_key).fetch1(\"session_dir\")\n</code></pre>"}, {"location": "api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_electrode_localization_dir", "title": "<code>get_electrode_localization_dir(probe_insertion_key)</code>", "text": "<p>Return root directory of localization data for a given probe</p> <p>Parameters:</p> Name Type Description Default <code>probe_insertion_key</code> <code>dict</code> <p>key uniquely identifying one ephys.EphysRecording</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Full path to localization data for either SpikeGLX or OpenEphys</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_electrode_localization_dir(probe_insertion_key: dict) -&gt; str:\n\"\"\"Return root directory of localization data for a given probe\n\n    Args:\n        probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording\n\n    Returns:\n        path (str): Full path to localization data for either SpikeGLX or OpenEphys\n    \"\"\"\n    from .pipeline import ephys\n\n    acq_software = (ephys.EphysRecording &amp; probe_insertion_key).fetch1(\"acq_software\")\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = pathlib.Path(\n            (\n                ephys.EphysRecording.EphysFile\n                &amp; probe_insertion_key\n                &amp; 'file_path LIKE \"%.ap.meta\"'\n            ).fetch1(\"file_path\")\n        )\n        probe_dir = element_interface.utils.find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath.parent\n        )\n    elif acq_software == \"Open Ephys\":\n        probe_path = (ephys.EphysRecording.EphysFile &amp; probe_insertion_key).fetch1(\n            \"file_path\"\n        )\n        probe_dir = element_interface.utils.find_full_path(\n            get_ephys_root_data_dir(), probe_path\n        )\n\n    return probe_dir\n</code></pre>"}, {"location": "api/workflow_array_ephys/paths/", "title": "paths.py", "text": ""}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n\n    data_dir = dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Return root directory for all processed data from 'ephys_processed_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>posixpath</code> <p>Absolute path if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_processed_root_data_dir():\n\"\"\"Return root directory for all processed data from 'ephys_processed_data_dir' in dj.config\n\n    Returns:\n        path (posixpath): Absolute path if available or None\n    \"\"\"\n    data_dir = dj.config.get(\"custom\", {}).get(\"ephys_processed_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Return relative path from SessionDirectory table given key</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>Key uniquely identifying a session</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Relative path of session directory</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Return relative path from SessionDirectory table given key\n\n    Args:\n        session_key (dict): Key uniquely identifying a session\n\n    Returns:\n        path (str): Relative path of session directory\n    \"\"\"\n    from .pipeline import session\n\n    return (session.SessionDirectory &amp; session_key).fetch1(\"session_dir\")\n</code></pre>"}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_electrode_localization_dir", "title": "<code>get_electrode_localization_dir(probe_insertion_key)</code>", "text": "<p>Return root directory of localization data for a given probe</p> <p>Parameters:</p> Name Type Description Default <code>probe_insertion_key</code> <code>dict</code> <p>key uniquely identifying one ephys.EphysRecording</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Full path to localization data for either SpikeGLX or OpenEphys</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_electrode_localization_dir(probe_insertion_key: dict) -&gt; str:\n\"\"\"Return root directory of localization data for a given probe\n\n    Args:\n        probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording\n\n    Returns:\n        path (str): Full path to localization data for either SpikeGLX or OpenEphys\n    \"\"\"\n    from .pipeline import ephys\n\n    acq_software = (ephys.EphysRecording &amp; probe_insertion_key).fetch1(\"acq_software\")\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = pathlib.Path(\n            (\n                ephys.EphysRecording.EphysFile\n                &amp; probe_insertion_key\n                &amp; 'file_path LIKE \"%.ap.meta\"'\n            ).fetch1(\"file_path\")\n        )\n        probe_dir = element_interface.utils.find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath.parent\n        )\n    elif acq_software == \"Open Ephys\":\n        probe_path = (ephys.EphysRecording.EphysFile &amp; probe_insertion_key).fetch1(\n            \"file_path\"\n        )\n        probe_dir = element_interface.utils.find_full_path(\n            get_ephys_root_data_dir(), probe_path\n        )\n\n    return probe_dir\n</code></pre>"}, {"location": "api/workflow_array_ephys/pipeline/", "title": "pipeline.py", "text": ""}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_ephys_root_data_dir", "title": "<code>get_ephys_root_data_dir()</code>", "text": "<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n\n    data_dir = dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_processed_root_data_dir", "title": "<code>get_processed_root_data_dir()</code>", "text": "<p>Return root directory for all processed data from 'ephys_processed_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>posixpath</code> <p>Absolute path if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_processed_root_data_dir():\n\"\"\"Return root directory for all processed data from 'ephys_processed_data_dir' in dj.config\n\n    Returns:\n        path (posixpath): Absolute path if available or None\n    \"\"\"\n    data_dir = dj.config.get(\"custom\", {}).get(\"ephys_processed_data_dir\", None)\n    return pathlib.Path(data_dir) if data_dir else None\n</code></pre>"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_session_directory", "title": "<code>get_session_directory(session_key)</code>", "text": "<p>Return relative path from SessionDirectory table given key</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>Key uniquely identifying a session</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Relative path of session directory</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Return relative path from SessionDirectory table given key\n\n    Args:\n        session_key (dict): Key uniquely identifying a session\n\n    Returns:\n        path (str): Relative path of session directory\n    \"\"\"\n    from .pipeline import session\n\n    return (session.SessionDirectory &amp; session_key).fetch1(\"session_dir\")\n</code></pre>"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_electrode_localization_dir", "title": "<code>get_electrode_localization_dir(probe_insertion_key)</code>", "text": "<p>Return root directory of localization data for a given probe</p> <p>Parameters:</p> Name Type Description Default <code>probe_insertion_key</code> <code>dict</code> <p>key uniquely identifying one ephys.EphysRecording</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Full path to localization data for either SpikeGLX or OpenEphys</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_electrode_localization_dir(probe_insertion_key: dict) -&gt; str:\n\"\"\"Return root directory of localization data for a given probe\n\n    Args:\n        probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording\n\n    Returns:\n        path (str): Full path to localization data for either SpikeGLX or OpenEphys\n    \"\"\"\n    from .pipeline import ephys\n\n    acq_software = (ephys.EphysRecording &amp; probe_insertion_key).fetch1(\"acq_software\")\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = pathlib.Path(\n            (\n                ephys.EphysRecording.EphysFile\n                &amp; probe_insertion_key\n                &amp; 'file_path LIKE \"%.ap.meta\"'\n            ).fetch1(\"file_path\")\n        )\n        probe_dir = element_interface.utils.find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath.parent\n        )\n    elif acq_software == \"Open Ephys\":\n        probe_path = (ephys.EphysRecording.EphysFile &amp; probe_insertion_key).fetch1(\n            \"file_path\"\n        )\n        probe_dir = element_interface.utils.find_full_path(\n            get_ephys_root_data_dir(), probe_path\n        )\n\n    return probe_dir\n</code></pre>"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.ecephys_session_to_nwb", "title": "<code>ecephys_session_to_nwb(session_key, raw=True, spikes=True, lfp='source', end_frame=None, lab_key=None, project_key=None, protocol_key=None, nwbfile_kwargs=None)</code>", "text": "<p>Main function for converting ephys data to NWB</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>key from Session table</p> required <code>raw</code> <code>bool</code> <p>Optional. Default True. Include the raw data from source. SpikeGLX &amp; OpenEphys are supported</p> <code>True</code> <code>spikes</code> <code>bool</code> <p>Optional. Default True. Whether to include CuratedClustering</p> <code>True</code> <code>lfp</code> <code>str</code> <p>One of the following. \"dj\", read LFP data from ephys.LFP. \"source\", read LFP data from source (SpikeGLX supported). False, do not convert LFP.</p> <code>'source'</code> <code>end_frame</code> <code>int</code> <p>Optional limit on frames for small test conversions.</p> <code>None</code> <code>lab_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>project_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>protocol_key</code> <code>dict</code> <p>Optional key to add metadata from other Element Lab.</p> <code>None</code> <code>nwbfile_kwargs</code> <code>dict</code> <p>Optional. If Element Session is not used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required  minimal data for instantiating an NWBFile object. If element-session is  being used, this argument can optionally be used to overwrite NWBFile  fields.</p> <code>None</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def ecephys_session_to_nwb(\n    session_key,\n    raw=True,\n    spikes=True,\n    lfp=\"source\",\n    end_frame=None,\n    lab_key=None,\n    project_key=None,\n    protocol_key=None,\n    nwbfile_kwargs=None,\n):\n\"\"\"Main function for converting ephys data to NWB\n\n    Arguments:\n        session_key (dict): key from Session table\n        raw (bool): Optional. Default True. Include the raw data from source.\n            SpikeGLX &amp; OpenEphys are supported\n        spikes (bool): Optional. Default True. Whether to include CuratedClustering\n        lfp (str): One of the following.\n            \"dj\", read LFP data from ephys.LFP.\n            \"source\", read LFP data from source (SpikeGLX supported).\n            False, do not convert LFP.\n        end_frame (int): Optional limit on frames for small test conversions.\n        lab_key (dict): Optional key to add metadata from other Element Lab.\n        project_key (dict): Optional key to add metadata from other Element Lab.\n        protocol_key (dict): Optional key to add metadata from other Element Lab.\n        nwbfile_kwargs (dict): Optional. If Element Session is not used, this argument\n            is required and must be a dictionary containing 'session_description' (str),\n            'identifier' (str), and 'session_start_time' (datetime), the required\n             minimal data for instantiating an NWBFile object. If element-session is\n             being used, this argument can optionally be used to overwrite NWBFile\n             fields.\n    \"\"\"\n\n    session_to_nwb = getattr(ephys._linking_module, \"session_to_nwb\", False)\n\n    if session_to_nwb:\n        nwbfile = session_to_nwb(\n            session_key,\n            lab_key=lab_key,\n            project_key=project_key,\n            protocol_key=protocol_key,\n            additional_nwbfile_kwargs=nwbfile_kwargs,\n        )\n    else:\n        nwbfile = pynwb.NWBFile(**nwbfile_kwargs)\n\n    ephys_root_data_dir = ephys.get_ephys_root_data_dir()\n\n    if raw:\n        add_ephys_recording_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    if spikes:\n        add_ephys_units_to_nwb(session_key, nwbfile)\n\n    if lfp == \"dj\":\n        add_ephys_lfp_from_dj_to_nwb(session_key, nwbfile)\n\n    if lfp == \"source\":\n        add_ephys_lfp_from_source_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    return nwbfile\n</code></pre>"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.write_nwb", "title": "<code>write_nwb(nwbfile, fname, check_read=True)</code>", "text": "<p>Export NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>nwbfile</code> <code>NWBFile</code> <p>nwb file</p> required <code>fname</code> <code>str</code> <p>Absolute path including <code>*.nwb</code> extension.</p> required <code>check_read</code> <code>bool</code> <p>If True, PyNWB will try to read the produced NWB file and ensure that it can be read.</p> <code>True</code> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def write_nwb(nwbfile, fname, check_read=True):\n\"\"\"Export NWBFile\n\n    Arguments:\n        nwbfile (NWBFile): nwb file\n        fname (str): Absolute path including `*.nwb` extension.\n        check_read (bool): If True, PyNWB will try to read the produced NWB file and\n            ensure that it can be read.\n    \"\"\"\n    with pynwb.NWBHDF5IO(fname, \"w\") as io:\n        io.write(nwbfile)\n\n    if check_read:\n        with pynwb.NWBHDF5IO(fname, \"r\") as io:\n            io.read()\n</code></pre>"}, {"location": "api/workflow_array_ephys/process/", "title": "process.py", "text": ""}, {"location": "api/workflow_array_ephys/process/#workflow_array_ephys.process.run", "title": "<code>run(display_progress=True, reserve_jobs=False, suppress_errors=False)</code>", "text": "<p>Execute all populate commands in Element Array Ephys</p> <p>Parameters:</p> Name Type Description Default <code>display_progress</code> <code>bool</code> <p>See DataJoint <code>populate</code>. Defaults to True.</p> <code>True</code> <code>reserve_jobs</code> <code>bool</code> <p>See DataJoint <code>populate</code>. Defaults to False.</p> <code>False</code> <code>suppress_errors</code> <code>bool</code> <p>See DataJoint <code>populate</code>. Defaults to False.</p> <code>False</code> Source code in <code>workflow_array_ephys/process.py</code> <pre><code>def run(\n    display_progress: bool = True,\n    reserve_jobs: bool = False,\n    suppress_errors: bool = False,\n):\n\"\"\"Execute all populate commands in Element Array Ephys\n\n    Args:\n        display_progress (bool, optional): See DataJoint `populate`. Defaults to True.\n        reserve_jobs (bool, optional): See DataJoint `populate`. Defaults to False.\n        suppress_errors (bool, optional): See DataJoint `populate`. Defaults to False.\n    \"\"\"\n\n    populate_settings = {\n        \"display_progress\": display_progress,\n        \"reserve_jobs\": reserve_jobs,\n        \"suppress_errors\": suppress_errors,\n    }\n\n    print(\"\\n---- Populate ephys.EphysRecording ----\")\n    ephys.EphysRecording.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.LFP ----\")\n    ephys.LFP.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.Clustering ----\")\n    ephys.Clustering.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.CuratedClustering ----\")\n    ephys.CuratedClustering.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.WaveformSet ----\")\n    ephys.WaveformSet.populate(**populate_settings)\n</code></pre>"}, {"location": "api/workflow_array_ephys/version/", "title": "version.py", "text": "<p>Package metadata Update the Docker image tag in <code>docker-compose.yaml</code> to match</p>"}, {"location": "api/workflow_array_ephys/plotting/plot_psth/", "title": "plot_psth.py", "text": ""}, {"location": "tutorials/", "title": "Tutorials", "text": ""}, {"location": "tutorials/#installation", "title": "Installation", "text": "<p>Installation of the Element requires an integrated development environment and database. Instructions to setup each of the components can be found on the User Instructions page.  These instructions use the example workflow for Element Array Ephys, which can be modified for a user's specific experimental requirements.  This example workflow uses several Elements (Lab, Animal, Session, Event, and Electrophysiology) to construct a complete pipeline, and is able to ingest experimental metadata and run model training and inference.</p>"}, {"location": "tutorials/#videos", "title": "Videos", "text": "<p>The Element Array Ephys tutorial gives an overview of the workflow files and notebooks as well as core concepts related to Electrophysiology.</p> <p></p>"}, {"location": "tutorials/#notebooks", "title": "Notebooks", "text": "<p>Each of the notebooks in the workflow (download here steps through ways to interact with the Element itself. For convenience, these notebooks are also rendered as part of this site. To try out the Elements notebooks in an online Jupyter environment with access to example data, visit CodeBook. (Electrophysiology notebooks coming soon!)</p> <ul> <li>Data Download highlights how to use DataJoint   tools to download a sample model for trying out the Element.</li> <li>Configure helps configure your local DataJoint installation to   point to the correct database.</li> <li>Workflow Structure demonstrates the table    architecture of the Element and key DataJoint basics for interacting with these    tables.</li> <li>Process steps through adding data to these tables and launching    key Electrophysiology features, like model training.</li> <li>Automate highlights the same steps as above, but   utilizing all built-in automation tools.</li> <li>Explore demonstrates how to fetch data from the Element.</li> <li>Drop schemas provides the steps for dropping all the   tables to start fresh.</li> <li>Downstream Analysis highlights how to link    this Element to Element Event for event-based analyses.</li> <li>Visualizations highlights how to use a built-in module    for visualizing units, probes and quality metrics.</li> <li>Electrode Localization demonstrates how to link   this Element to   Element Electrode Localization.</li> <li>NWB Export highlights the export functionality available for the   <code>no-curation</code> schema.</li> </ul>"}, {"location": "tutorials/10-data_visualization/", "title": "Visualizations", "text": "<p>This notebook is intended to demonstrate the use of visualization functions for ephys data and datajoint tables for storing the results.</p> In\u00a0[1]: Copied! <pre>import os\n\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\n</pre> import os  if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\")  <p>Visualization results (e.g., figures) will be stored in tables created under the <code>ephys_report</code> schema. By default, the schema will be activated with the <code>ephys</code> schema.</p> In\u00a0[2]: Copied! <pre>import datajoint as dj\nimport datetime\nfrom workflow_array_ephys.pipeline import ephys, probe, ephys_report\n</pre> import datajoint as dj import datetime from workflow_array_ephys.pipeline import ephys, probe, ephys_report  <pre>[2023-01-11 09:54:32,642][WARNING]: lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\n[2023-01-11 09:54:32,695][WARNING]: Element Lab's \"project\" schema does not yet feature NWB export.Please use lab.Project and lab.Protocol tables\n[2023-01-11 09:54:33,267][INFO]: Connecting cbroz@dss-db.datajoint.io:3306\n[2023-01-11 09:54:33,693][INFO]: Connected cbroz@dss-db.datajoint.io:3306\n</pre> <p>First, we fetch one example unit data from the <code>ephys.CuratedClustering.Unit</code> table that will be used for this demonstration.</p> In\u00a0[3]: Copied! <pre># Get the unit key\nunit_number = 13\n\nunit_key = {\n    \"subject\": \"subject6\",\n    \"session_datetime\": datetime.datetime(2021, 1, 15, 11, 16, 38),\n    \"insertion_number\": 0,\n    \"paramset_idx\": 0,\n    \"unit\": unit_number,\n}\n\nephys.CuratedClustering.Unit &amp; unit_key\n</pre> # Get the unit key unit_number = 13  unit_key = {     \"subject\": \"subject6\",     \"session_datetime\": datetime.datetime(2021, 1, 15, 11, 16, 38),     \"insertion_number\": 0,     \"paramset_idx\": 0,     \"unit\": unit_number, }  ephys.CuratedClustering.Unit &amp; unit_key  Out[3]: Properties of a given unit from a round of clustering (and curation) <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>paramset_idx</p> <p>unit</p> <p>electrode_config_hash</p> <p>probe_type</p> e.g. neuropixels_1.0 <p>electrode</p> electrode index, starts at 0 <p>cluster_quality_label</p> cluster quality type - e.g. 'good', 'MUA', 'noise', etc. <p>spike_count</p> how many spikes in this recording for this unit <p>spike_times</p> (s) spike times of this unit, relative to the start of the EphysRecording <p>spike_sites</p> array of electrode associated with each spike <p>spike_depths</p> (um) array of depths associated with each spike, relative to the (0, 0) of the probe subject6 2021-01-15 11:16:38 0 0 13 6f6d8149-1603-9d2c-f884-0fdf995ec8b3 neuropixels 1.0 - 3B 13 mua 13607 =BLOB= =BLOB= =BLOB= <p>Total: 1</p> <p>Figures can be generated either at the probe level or a single unit level. Here we start start with the unit-level visualization and import functions from <code>element_array_ephys.plotting.unit_level</code></p> <ul> <li><p>There are functions to plot the 1) unit waveform, 2) autocorrelogram, and 3) depth waveforms which plot the peak waveform as well as waveforms detected from neighboring sites from a given probe.</p> </li> <li><p>Each figure was plotted with the plotly library to allow interactive exploration of the data.</p> </li> </ul> In\u00a0[4]: Copied! <pre>from element_array_ephys.plotting.unit_level import (\n    plot_waveform,\n    plot_auto_correlogram,\n    plot_depth_waveforms,\n)\n\n# Fetch unit data\nsampling_rate = (ephys.EphysRecording &amp; unit_key).fetch1(\n    \"sampling_rate\"\n) / 1e3  # in kHz\n\nunit_data = (\n    (ephys.CuratedClustering.Unit &amp; unit_key) * ephys.WaveformSet.PeakWaveform\n).fetch1()\n\nwaveform = unit_data[\"peak_electrode_waveform\"]\nspike_times = unit_data[\"spike_times\"]\n\n# Fetch unit data\nplot_waveform(waveform, sampling_rate)\n</pre> from element_array_ephys.plotting.unit_level import (     plot_waveform,     plot_auto_correlogram,     plot_depth_waveforms, )  # Fetch unit data sampling_rate = (ephys.EphysRecording &amp; unit_key).fetch1(     \"sampling_rate\" ) / 1e3  # in kHz  unit_data = (     (ephys.CuratedClustering.Unit &amp; unit_key) * ephys.WaveformSet.PeakWaveform ).fetch1()  waveform = unit_data[\"peak_electrode_waveform\"] spike_times = unit_data[\"spike_times\"]  # Fetch unit data plot_waveform(waveform, sampling_rate)  In\u00a0[13]: Copied! <pre># Plot Correlogram\nplot_auto_correlogram(spike_times=spike_times, bin_size=0.001, window_size=1)\n</pre> # Plot Correlogram plot_auto_correlogram(spike_times=spike_times, bin_size=0.001, window_size=1)  <p>The electrode site where the peak waveform was found will be plotted in red. The <code>y_range</code> parameter can be modified to alter the vertical range in which the neighboring waveforms are found.</p> In\u00a0[36]: Copied! <pre># Plot depth Waveforms\nplot_depth_waveforms(ephys, unit_key=unit_key, y_range=60)\n</pre> # Plot depth Waveforms plot_depth_waveforms(ephys, unit_key=unit_key, y_range=60)  <p>At the probe level, we plot a driftmap to visualize the activity of all neurons recorded on that probe per shank. Here we import the function from <code>element_array_ephys.plotting.probe_level</code></p> In\u00a0[37]: Copied! <pre>from element_array_ephys.plotting.probe_level import plot_driftmap\n</pre> from element_array_ephys.plotting.probe_level import plot_driftmap  <ul> <li><p>Specify the probe key and the shank from that probe. Fetch <code>spikes_times</code> and <code>spike_depths</code> from all units from the shank, which will be used as an input argument for the function <code>plot_driftmap</code>.</p> </li> <li><p>Units are aligned relative to the distance from the probe tip. An increase in activity (firing rates) is indicated by dark red.</p> </li> </ul> In\u00a0[54]: Copied! <pre>import matplotlib.pyplot as plt\n\nprobe_key = {\n    \"subject\": \"subject6\",\n    \"session_datetime\": datetime.datetime(2021, 1, 15, 11, 16, 38),\n    \"insertion_number\": 0,\n    \"paramset_idx\": 0,\n}\n\n# Fetch all units recorded from the probe and specify the shank\nunits = ephys.CuratedClustering.Unit &amp; probe_key &amp; \"cluster_quality_label='good'\"\nshank_no = 0\n\ntable = units * ephys.ProbeInsertion * probe.ProbeType.Electrode &amp; {\"shank\": shank_no}\n\nspike_times, spike_depths = table.fetch(\"spike_times\", \"spike_depths\", order_by=\"unit\")\n\nplot_driftmap(spike_times, spike_depths, colormap=\"gist_heat_r\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  probe_key = {     \"subject\": \"subject6\",     \"session_datetime\": datetime.datetime(2021, 1, 15, 11, 16, 38),     \"insertion_number\": 0,     \"paramset_idx\": 0, }  # Fetch all units recorded from the probe and specify the shank units = ephys.CuratedClustering.Unit &amp; probe_key &amp; \"cluster_quality_label='good'\" shank_no = 0  table = units * ephys.ProbeInsertion * probe.ProbeType.Electrode &amp; {\"shank\": shank_no}  spike_times, spike_depths = table.fetch(\"spike_times\", \"spike_depths\", order_by=\"unit\")  plot_driftmap(spike_times, spike_depths, colormap=\"gist_heat_r\") plt.show()  <p>Now, let's populate tables with these figures.</p> In\u00a0[\u00a0]: Copied! <pre>ephys_report.ProbeLevelReport.populate()\nephys_report.UnitLevelReport.populate()\n</pre> ephys_report.ProbeLevelReport.populate() ephys_report.UnitLevelReport.populate() <p>All of the above plots for probes and units ingested into datajoint tables can be visualized with a single widget, which can be imported as follows:</p> In\u00a0[\u00a0]: Copied! <pre>from element_array_ephys.plotting.widget import main\n\nmain(ephys)\n</pre> from element_array_ephys.plotting.widget import main  main(ephys) <pre>VBox(children=(interactive(children=(Dropdown(description='Select Probe Insertion : ', layout=Layout(width='80\u2026</pre> <p>You can select a probe &amp; shank and all the individual units associated with them via dropdown and it will automatically fetch &amp; render the plot stored in tables from the <code>ephys_report</code> schema.</p> <p>The Element also offers Quality Metric visualizations. These are generated using an output from kilosort, <code>metrics.csv</code>. First, ensure your <code>QualityMetrics</code> table is populated with this data:</p> In\u00a0[6]: Copied! <pre>ephys.QualityMetrics.populate()\n</pre> ephys.QualityMetrics.populate()  <p>We'll grab an example key for demonstration.</p> In\u00a0[4]: Copied! <pre>my_key = ephys.QualityMetrics.fetch(\"KEY\")[0]\nmy_key\n</pre> my_key = ephys.QualityMetrics.fetch(\"KEY\")[0] my_key  Out[4]: <pre>{'subject': 'subject4',\n 'session_datetime': datetime.datetime(2020, 2, 20, 14, 34, 56),\n 'insertion_number': 0,\n 'paramset_idx': 0}</pre> <p>The <code>QualityMetricFigs</code> class will be used to create individual plots of the quality metrics calculated by <code>element-array-ephys</code> as well as a dashboard of all metrics. Next, initialize the <code>QualityMetricFigs</code> class with the <code>ephys</code> module.</p> In\u00a0[5]: Copied! <pre>from element_array_ephys.plotting.qc import QualityMetricFigs\n\nqm = QualityMetricFigs(ephys, key=my_key, dark_mode=True)\nqm.plot_list  # Available plots\n</pre> from element_array_ephys.plotting.qc import QualityMetricFigs  qm = QualityMetricFigs(ephys, key=my_key, dark_mode=True) qm.plot_list  # Available plots  Out[5]: <pre>['firing_rate',\n 'presence_ratio',\n 'amp_cutoff',\n 'isi_violation',\n 'snr',\n 'iso_dist',\n 'd_prime',\n 'nn_hit']</pre> <p>To see just one plot, we can call for it by name.</p> In\u00a0[6]: Copied! <pre>fig = qm.get_single_fig(\"snr\", scale=0.5)\nfig.show(\n    \"png\"\n)  # .show('png') is optional. Here, it is used to render the image within a notebook that is embedded in a browser.\n</pre> fig = qm.get_single_fig(\"snr\", scale=0.5) fig.show(     \"png\" )  # .show('png') is optional. Here, it is used to render the image within a notebook that is embedded in a browser.  <p>Or, we can see all available plots as a grid.</p> In\u00a0[7]: Copied! <pre>qm.get_grid().show(\"png\")\n</pre> qm.get_grid().show(\"png\")  <p>We can update the key and even add or remove plots.</p> In\u00a0[8]: Copied! <pre>qm.key = {\n    \"subject\": \"subject4\"\n}  # Update the key. Must uniquely identify a row in the `QualityMetrics` table\n</pre> qm.key = {     \"subject\": \"subject4\" }  # Update the key. Must uniquely identify a row in the `QualityMetrics` table  In\u00a0[9]: Copied! <pre>import numpy as np\n\nqm.plots = {  # Add a plot to the list\n    \"log_d_prime\": {\n        \"xaxis\": \"log d-prime\",  # x-axis label\n        \"data\": np.log10(qm.units[\"d_prime\"]),  # Histogram data\n        \"bins\": np.linspace(0, 3, 50),  # Histogram bins\n        \"vline\": 1,  # Vertical line\n    }\n}\nqm.remove_plot(\"isi_violation\")  # Drop a plot from those rendered\nqm.get_grid().show(\"png\")\n</pre> import numpy as np  qm.plots = {  # Add a plot to the list     \"log_d_prime\": {         \"xaxis\": \"log d-prime\",  # x-axis label         \"data\": np.log10(qm.units[\"d_prime\"]),  # Histogram data         \"bins\": np.linspace(0, 3, 50),  # Histogram bins         \"vline\": 1,  # Vertical line     } } qm.remove_plot(\"isi_violation\")  # Drop a plot from those rendered qm.get_grid().show(\"png\")  <p>Scientists may want to apply cutoffs to the units visualized. By default, none are applied, but they can easily be set with a dictionary that corresponds to an entry in the <code>ephys_report.QualityMetricCutoffs</code> table.</p> In\u00a0[10]: Copied! <pre>my_cutoffs = (ephys_report.QualityMetricCutoffs &amp; \"cutoffs_id=1\").fetch1()\nmy_cutoffs\n</pre> my_cutoffs = (ephys_report.QualityMetricCutoffs &amp; \"cutoffs_id=1\").fetch1() my_cutoffs  Out[10]: <pre>{'cutoffs_id': 1,\n 'amplitude_cutoff_maximum': 0.1,\n 'presence_ratio_minimum': 0.9,\n 'isi_violations_maximum': 0.5,\n 'cutoffs_hash': UUID('f74ccd77-0b3a-2bf8-0bfd-ec9713b5dca8')}</pre> In\u00a0[11]: Copied! <pre>qm.cutoffs = my_cutoffs\n</pre> qm.cutoffs = my_cutoffs  <p>Relevant data items are available as a pandas dataframe in the <code>units</code> property, which correspond to the data stored in the <code>ephys.QualityMetric.Cluster</code> and <code>ephys.QualityMetric.Waveform</code> tables.</p> In\u00a0[12]: Copied! <pre>from IPython.display import HTML  # For pretty printing\n\nHTML(qm.units.iloc[0:3].to_html(index=False))  # First few rows only\n</pre> from IPython.display import HTML  # For pretty printing  HTML(qm.units.iloc[0:3].to_html(index=False))  # First few rows only  Out[12]: firing_rate snr presence_ratio isi_violation number_violation amplitude_cutoff isolation_distance l_ratio d_prime nn_hit_rate nn_miss_rate silhouette_score max_drift cumulative_drift contamination_rate amplitude duration halfwidth pt_ratio repolarization_slope recovery_slope spread velocity_above velocity_below 1.316800 3.230420 0.99 NaN None 0.5 19.9267 0.002013 5.24332 0.981333 0.000846 0.137668 134.22 3523.91 NaN 79.7310 1.24992 0.123618 NaN 0.263708 -0.001869 0.0 NaN NaN 0.026997 1.112870 0.84 NaN None 0.5 12.1206 0.001683 2.33009 0.185185 0.000300 -0.034638 NaN 0.00 NaN 43.4414 1.71692 0.700503 NaN 0.146716 -0.004821 3320.0 0.033097 NaN 0.016992 0.931551 0.66 NaN None 0.5 11.9093 0.001203 2.84158 0.200000 0.000233 -0.034638 NaN 0.00 NaN 42.5355 1.59330 1.208710 NaN 0.006649 -0.016113 2060.0 0.060787 NaN"}, {"location": "tutorials/10-data_visualization/#data-visualization", "title": "Data Visualization\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#unit-level-visualization", "title": "Unit level visualization\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#plot-waveform", "title": "Plot waveform\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#plot-autocorrelogram", "title": "Plot autocorrelogram\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#plot-depth-waveforms", "title": "Plot depth waveforms\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#probe-level-visualization", "title": "Probe level visualization\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#plot-driftmap", "title": "Plot driftmap\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#using-ipywidget", "title": "Using ipywidget\u00b6", "text": ""}, {"location": "tutorials/10-data_visualization/#quality-metrics", "title": "Quality Metrics\u00b6", "text": ""}, {"location": "tutorials/demo_prepare/", "title": "Demo prepare", "text": "In\u00a0[\u00a0]: Copied! <pre># Runs in about 45s\nimport datajoint as dj\nimport datetime\nfrom workflow_array_ephys.pipeline import subject, session, probe, ephys\nfrom element_array_ephys import ephys_report\n</pre> # Runs in about 45s import datajoint as dj import datetime from workflow_array_ephys.pipeline import subject, session, probe, ephys from element_array_ephys import ephys_report  In\u00a0[\u00a0]: Copied! <pre>subject.Subject.insert1(\n    dict(\n        subject='subject5',\n        subject_birth_date='2023-01-01',\n        sex='U'\n    )\n)\n</pre> subject.Subject.insert1(     dict(         subject='subject5',         subject_birth_date='2023-01-01',         sex='U'     ) ) In\u00a0[\u00a0]: Copied! <pre>session_key = dict(subject='subject5', \n                   session_datetime=datetime.datetime.now())\n\nsession.Session.insert1(session_key)\n\nsession.SessionDirectory.insert1(\n    dict(\n        session_key, \n        session_dir='subject5/session1'\n    )\n)\n</pre> session_key = dict(subject='subject5',                     session_datetime=datetime.datetime.now())  session.Session.insert1(session_key)  session.SessionDirectory.insert1(     dict(         session_key,          session_dir='subject5/session1'     ) ) In\u00a0[\u00a0]: Copied! <pre>probe.Probe.insert1(\n    dict(probe=\"714000838\", \n         probe_type=\"neuropixels 1.0 - 3B\")\n)\n\nephys.ProbeInsertion.insert1(\n    dict(\n        session_key,\n        insertion_number=1,\n        probe=\"714000838\",\n    )\n)\n</pre> probe.Probe.insert1(     dict(probe=\"714000838\",           probe_type=\"neuropixels 1.0 - 3B\") )  ephys.ProbeInsertion.insert1(     dict(         session_key,         insertion_number=1,         probe=\"714000838\",     ) ) In\u00a0[\u00a0]: Copied! <pre>populate_settings = {\"display_progress\": True}\n\nephys.EphysRecording.populate(**populate_settings)\n</pre> populate_settings = {\"display_progress\": True}  ephys.EphysRecording.populate(**populate_settings) In\u00a0[\u00a0]: Copied! <pre>kilosort_params = {\n    \"fs\": 30000,\n    \"fshigh\": 150,\n    \"minfr_goodchannels\": 0.1,\n    \"Th\": [10, 4],\n    \"lam\": 10,\n    \"AUCsplit\": 0.9,\n    \"minFR\": 0.02,\n    \"momentum\": [20, 400],\n    \"sigmaMask\": 30,\n    \"ThPr\": 8,\n    \"spkTh\": -6,\n    \"reorder\": 1,\n    \"nskip\": 25,\n    \"GPU\": 1,\n    \"Nfilt\": 1024,\n    \"nfilt_factor\": 4,\n    \"ntbuff\": 64,\n    \"whiteningRange\": 32,\n    \"nSkipCov\": 25,\n    \"scaleproc\": 200,\n    \"nPCs\": 3,\n    \"useRAM\": 0,\n}\n\nephys.ClusteringParamSet.insert_new_params(\n    clustering_method=\"kilosort2\",\n    paramset_idx=1,\n    params=kilosort_params,\n    paramset_desc=\"Spike sorting using Kilosort2\",\n)\n</pre> kilosort_params = {     \"fs\": 30000,     \"fshigh\": 150,     \"minfr_goodchannels\": 0.1,     \"Th\": [10, 4],     \"lam\": 10,     \"AUCsplit\": 0.9,     \"minFR\": 0.02,     \"momentum\": [20, 400],     \"sigmaMask\": 30,     \"ThPr\": 8,     \"spkTh\": -6,     \"reorder\": 1,     \"nskip\": 25,     \"GPU\": 1,     \"Nfilt\": 1024,     \"nfilt_factor\": 4,     \"ntbuff\": 64,     \"whiteningRange\": 32,     \"nSkipCov\": 25,     \"scaleproc\": 200,     \"nPCs\": 3,     \"useRAM\": 0, }  ephys.ClusteringParamSet.insert_new_params(     clustering_method=\"kilosort2\",     paramset_idx=1,     params=kilosort_params,     paramset_desc=\"Spike sorting using Kilosort2\", ) In\u00a0[\u00a0]: Copied! <pre>ephys.ClusteringTask.insert1(\n    dict(\n        session_key,\n        insertion_number=1,\n        paramset_idx=1,\n        task_mode='load', # load or trigger\n        clustering_output_dir=\"subject5/session1/probe_1/kilosort2-5_1\"\n    )\n)\n\nephys.Clustering.populate(**populate_settings)\n</pre> ephys.ClusteringTask.insert1(     dict(         session_key,         insertion_number=1,         paramset_idx=1,         task_mode='load', # load or trigger         clustering_output_dir=\"subject5/session1/probe_1/kilosort2-5_1\"     ) )  ephys.Clustering.populate(**populate_settings) In\u00a0[\u00a0]: Copied! <pre>clustering_key = (ephys.ClusteringTask &amp; session_key).fetch1('KEY')\nephys.Curation().create1_from_clustering_task(clustering_key)\n</pre> clustering_key = (ephys.ClusteringTask &amp; session_key).fetch1('KEY') ephys.Curation().create1_from_clustering_task(clustering_key) In\u00a0[\u00a0]: Copied! <pre># Runs in about 12m\nephys.CuratedClustering.populate(**populate_settings)\nephys.WaveformSet.populate(**populate_settings)\nephys_report.ProbeLevelReport.populate(**populate_settings)\nephys_report.UnitLevelReport.populate(**populate_settings)\n</pre> # Runs in about 12m ephys.CuratedClustering.populate(**populate_settings) ephys.WaveformSet.populate(**populate_settings) ephys_report.ProbeLevelReport.populate(**populate_settings) ephys_report.UnitLevelReport.populate(**populate_settings) In\u00a0[\u00a0]: Copied! <pre>def drop_databases(databases):\n    import pymysql.err\n    conn = dj.conn()\n\n    with dj.config(safemode=False):\n        for database in databases:\n            schema = dj.Schema(f'{dj.config[\"custom\"][\"database.prefix\"]}{database}')\n            while schema.list_tables():\n                for table in schema.list_tables():\n                    try:\n                        conn.query(f\"DROP TABLE `{schema.database}`.`{table}`\")\n                    except pymysql.err.OperationalError:\n                        print(f\"Can't drop `{schema.database}`.`{table}`. Retrying...\")\n            schema.drop()\n\n# drop_databases(databases=['analysis', 'trial', 'event', 'ephys_report', 'ephys', 'probe', 'session', 'subject', 'project', 'lab'])\n</pre> def drop_databases(databases):     import pymysql.err     conn = dj.conn()      with dj.config(safemode=False):         for database in databases:             schema = dj.Schema(f'{dj.config[\"custom\"][\"database.prefix\"]}{database}')             while schema.list_tables():                 for table in schema.list_tables():                     try:                         conn.query(f\"DROP TABLE `{schema.database}`.`{table}`\")                     except pymysql.err.OperationalError:                         print(f\"Can't drop `{schema.database}`.`{table}`. Retrying...\")             schema.drop()  # drop_databases(databases=['analysis', 'trial', 'event', 'ephys_report', 'ephys', 'probe', 'session', 'subject', 'project', 'lab'])"}, {"location": "tutorials/demo_prepare/#demo-preparation-notebook", "title": "Demo Preparation Notebook\u00b6", "text": "<p>Please Note: This notebook and demo are NOT intended to be used as learning materials. To gain a thorough understanding of the DataJoint workflow for extracellular electrophysiology, please see the <code>tutorial</code> notebook.</p>"}, {"location": "tutorials/demo_prepare/#drop-schemas", "title": "Drop schemas\u00b6", "text": "<ul> <li>Schemas are not typically dropped in a production workflow with real data in it.</li> <li>At the developmental phase, it might be required for the table redesign.</li> <li>When dropping all schemas is needed, the following is the dependency order.</li> </ul>"}, {"location": "tutorials/demo_run/", "title": "Demo run", "text": "In\u00a0[\u00a0]: Copied! <pre>import datajoint as dj\nfrom workflow_array_ephys.pipeline import subject, session, probe, ephys\nfrom element_array_ephys.plotting.widget import main\n</pre> import datajoint as dj from workflow_array_ephys.pipeline import subject, session, probe, ephys from element_array_ephys.plotting.widget import main In\u00a0[\u00a0]: Copied! <pre>dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(probe) + dj.Diagram(ephys)\n</pre> dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(probe) + dj.Diagram(ephys) In\u00a0[\u00a0]: Copied! <pre>main(ephys)\n</pre> main(ephys) <p>For an in-depth tutorial please see the tutorial notebook.</p>"}, {"location": "tutorials/demo_run/#datajoint-workflow-for-neuropixels-analysis", "title": "DataJoint Workflow for Neuropixels Analysis\u00b6", "text": "<ul> <li>This notebook demonstrates using the open-source DataJoint Element to build a workflow for extracellular electrophysiology.</li> <li>For a detailed tutorial, please see the tutorial notebook.</li> </ul>"}, {"location": "tutorials/demo_run/#import-dependencies", "title": "Import dependencies\u00b6", "text": ""}, {"location": "tutorials/demo_run/#view-workflow", "title": "View workflow\u00b6", "text": ""}, {"location": "tutorials/demo_run/#insert-an-entry-in-a-manual-table-by-calling-the-insert-method", "title": "Insert an entry in a manual table by calling the <code>insert()</code> method\u00b6", "text": "<pre>subject.Subject.insert1(\n    dict(subject='subject1',\n         subject_birth_date='2023-01-01',\n         sex='U',\n    )\n)\n</pre>"}, {"location": "tutorials/demo_run/#automatically-process-data-with-the-populate-method", "title": "Automatically process data with the <code>populate()</code> method\u00b6", "text": "<ul> <li><p>Once data is inserted into manual tables, the <code>populate()</code> function automatically runs the ingestion and processing routines.</p> </li> <li><p>For example, to run Kilosort processing in the <code>Clustering</code> table:</p> <pre>ephys.Clustering.populate()\n</pre> </li> </ul>"}, {"location": "tutorials/demo_run/#visualize-processed-data", "title": "Visualize processed data\u00b6", "text": ""}, {"location": "tutorials/quality_metrics/", "title": "Quality Metrics", "text": "In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom workflow_array_ephys.pipeline import ephys\n</pre> import matplotlib.pyplot as plt import numpy as np from workflow_array_ephys.pipeline import ephys <pre>[2023-04-21 00:44:04,859][WARNING]: lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\n[2023-04-21 00:44:04,862][INFO]: Connecting root@fakeservices.datajoint.io:3306\n[2023-04-21 00:44:04,868][INFO]: Connected root@fakeservices.datajoint.io:3306\n</pre> In\u00a0[2]: Copied! <pre>ephys.QualityMetrics.populate()\n</pre> ephys.QualityMetrics.populate() In\u00a0[3]: Copied! <pre>key = {\"subject\": \"subject5\", \"insertion_number\": 1}\n\nquery = ephys.QualityMetrics.Cluster &amp; key\nquery\n</pre> key = {\"subject\": \"subject5\", \"insertion_number\": 1}  query = ephys.QualityMetrics.Cluster &amp; key query Out[3]: Cluster metrics for a particular unit <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>paramset_idx</p> <p>curation_id</p> <p>unit</p> <p>firing_rate</p> (Hz) firing rate for a unit <p>snr</p> signal-to-noise ratio for a unit <p>presence_ratio</p> fraction of time in which spikes are present <p>isi_violation</p> rate of ISI violation as a fraction of overall rate <p>number_violation</p> total number of ISI violations <p>amplitude_cutoff</p> estimate of miss rate based on amplitude histogram <p>isolation_distance</p> distance to nearest cluster in Mahalanobis space <p>l_ratio</p> <p>d_prime</p> Classification accuracy based on LDA <p>nn_hit_rate</p> Fraction of neighbors for target cluster that are also in target cluster <p>nn_miss_rate</p> Fraction of neighbors outside target cluster that are in target cluster <p>silhouette_score</p> Standard metric for cluster overlap <p>max_drift</p> Maximum change in spike depth throughout recording <p>cumulative_drift</p> Cumulative change in spike depth throughout recording <p>contamination_rate</p> subject5 2023-04-21 00:40:32 1 1 1 0 6.71766 0.392532 0.98 1.0 105 0.126571 nan 0.0 nan nan nan 0.0256577 0.0 0.0 1.13881subject5 2023-04-21 00:40:32 1 1 1 1 4.64183 0.391947 0.99 1.0 24 0.5 nan 0.0 nan nan nan 0.126882 7.77 20.13 1.08565subject5 2023-04-21 00:40:32 1 1 1 2 0.171264 1.91799 0.39 0.0 0 0.5 41.5366 0.01055 2.00497 0.109195 0.00941423 0.0382387 17.01 0.0 0.0subject5 2023-04-21 00:40:32 1 1 1 3 8.46869 0.454273 0.98 1.0 53 0.5 48.976 0.369185 2.74545 0.987333 1.0 0.0382387 3.9 9.13 0.670886subject5 2023-04-21 00:40:32 1 1 1 4 0.413395 2.56485 0.73 0.0 0 0.5 360.148 0.00934174 4.40108 0.966667 0.135897 0.0571513 6.35 13.91 0.0subject5 2023-04-21 00:40:32 1 1 1 5 0.218509 2.4666 0.48 0.0 0 0.5 162.35 3.64917e-05 6.75158 0.981982 0.0102881 0.10466 6.24 9.81 0.0subject5 2023-04-21 00:40:32 1 1 1 6 0.100396 3.87529 0.31 0.0 0 0.5 7.98228e+16 nan 4.6297 0.843137 0.026749 0.0571513 0.0 0.0 0.0subject5 2023-04-21 00:40:32 1 1 1 7 0.064962 1.85867 0.21 0.0 0 0.279006 4.61058e+16 nan 4.39722 0.727273 0.0147849 nan nan 0.0 1.0subject5 2023-04-21 00:40:32 1 1 1 8 15.1657 0.445549 0.99 1.0 412 0.5 nan 0.0 nan nan nan 0.131529 0.0 0.0 1.11235subject5 2023-04-21 00:40:32 1 1 1 9 1.27267 0.255138 0.88 1.0 2 0.158831 185.314 0.0121516 5.55476 0.996906 0.0454545 0.152277 7.8 17.6 0.201681subject5 2023-04-21 00:40:32 1 1 1 10 0.0856318 1.71077 0.25 0.0 0 0.5 1.32036e+16 nan 5.22485 0.977012 0.000674764 0.0705441 nan 0.0 1.0subject5 2023-04-21 00:40:32 1 1 1 11 0.0767733 1.25363 0.22 0.0 0 0.5 8456290000000000.0 nan 6.3283 0.858974 0.000118455 nan nan 0.0 1.0 <p>...</p> <p>Total: 227</p> <p>Plot histograms of the cluster metrics.</p> In\u00a0[4]: Copied! <pre>def plot_metric(ax, data, bins, x_axis_label=None, title=None, color='k', smoothing=True, density=False):\n\"\"\"A function modified from https://allensdk.readthedocs.io/en/latest/_static/examples/nb/ecephys_quality_metrics.html\n    \"\"\"\n    from scipy.ndimage import gaussian_filter1d\n    if any(data) and np.nansum(data):\n        h, b = np.histogram(data, bins=bins, density=density)\n        x = b[:-1]\n\n        y = gaussian_filter1d(h, 1) if smoothing else h\n        ax.plot(x, y, color=color)\n        ax.set_xlabel(x_axis_label)\n        ax.set_ylim([0, None])\n    ax.set_title(title)\n    ax.spines[['right', 'top']].set_visible(False)\n</pre> def plot_metric(ax, data, bins, x_axis_label=None, title=None, color='k', smoothing=True, density=False):     \"\"\"A function modified from https://allensdk.readthedocs.io/en/latest/_static/examples/nb/ecephys_quality_metrics.html     \"\"\"     from scipy.ndimage import gaussian_filter1d     if any(data) and np.nansum(data):         h, b = np.histogram(data, bins=bins, density=density)         x = b[:-1]          y = gaussian_filter1d(h, 1) if smoothing else h         ax.plot(x, y, color=color)         ax.set_xlabel(x_axis_label)         ax.set_ylim([0, None])     ax.set_title(title)     ax.spines[['right', 'top']].set_visible(False) In\u00a0[5]: Copied! <pre>fig, axes = plt.subplots(4, 4, figsize=(12, 9))\naxes = axes.flatten()\nplt.suptitle(f\"Cluster Quality Metrics for {key}\", y=.99, fontsize=12)\n\n# Firing Rates\ndata = np.log10(query.fetch(\"firing_rate\"))\nbins = np.linspace(-3,2,100)\nplot_metric(axes[0], data, bins, title=\"Firing Rate (Hz) (log$_{10}$)\")\naxes[0].set_ylabel(\"Count\")\n\n# Signal-to-Noise Ratio\ndata = query.fetch(\"snr\")\nbins = np.linspace(0, 10, 100)\nplot_metric(axes[1], data, bins, title=\"Signal-to-Noise Ratio\")\n\n# Presence Ratio\ndata = query.fetch(\"presence_ratio\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[2], data, bins, title=\"Presence Ratio\")\n\n# ISI Violation\ndata = query.fetch(\"isi_violation\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[3], data, bins, title=\"ISI Violation\")\n\n# Number Violation\ndata = query.fetch(\"number_violation\")\nbins = np.linspace(0, 1000, 100)\nplot_metric(axes[4], data, bins, title=\"Number Violation\")\naxes[4].set_ylabel(\"Count\")\n\n# Amplitude Cutoff\ndata = query.fetch(\"amplitude_cutoff\")\nbins = np.linspace(0, 0.5, 100)\nplot_metric(axes[5], data, bins, title=\"Amplitude Cutoff\")\n\n# Isolation Distance\ndata = query.fetch(\"isolation_distance\")\nbins = np.linspace(0, 170, 50)\nplot_metric(axes[6], data, bins, title=\"Isolation Distance\")\n\n# L-Ratio\ndata = query.fetch(\"l_ratio\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[7], data, bins, title=\"L-Ratio\")\n\n# d-Prime\ndata = query.fetch(\"d_prime\")\nbins = np.linspace(0, 15, 50)\nplot_metric(axes[8], data, bins, title=\"d-Prime\")\naxes[8].set_ylabel(\"Count\")\n\n# Nearest-Neighbors Hit Rate\ndata = query.fetch(\"nn_hit_rate\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[9], data, bins, title=\"Nearest-Neighbors Hit Rate\")\n\n# Nearest-Neighbors Miss Rate\ndata = query.fetch(\"nn_miss_rate\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[10], data, bins, title=\"Nearest-Neighbors Miss Rate\")\n\n# Silhouette Score\ndata = query.fetch(\"silhouette_score\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[11], data, bins, title=\"Silhouette Score\")\n\n# Max Drift\ndata = query.fetch(\"max_drift\")\nbins = np.linspace(0, 100, 100)\nplot_metric(axes[12], data, bins, title=\"Max Drift\")\naxes[12].set_ylabel(\"Count\")\n\n# Cumulative Drift\ndata = query.fetch(\"cumulative_drift\")\nbins = np.linspace(0, 100, 100)\nplot_metric(axes[13], data, bins, title=\"Cumulative Drift\")\n\n[ax.remove() for ax in axes[14:]]\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(4, 4, figsize=(12, 9)) axes = axes.flatten() plt.suptitle(f\"Cluster Quality Metrics for {key}\", y=.99, fontsize=12)  # Firing Rates data = np.log10(query.fetch(\"firing_rate\")) bins = np.linspace(-3,2,100) plot_metric(axes[0], data, bins, title=\"Firing Rate (Hz) (log$_{10}$)\") axes[0].set_ylabel(\"Count\")  # Signal-to-Noise Ratio data = query.fetch(\"snr\") bins = np.linspace(0, 10, 100) plot_metric(axes[1], data, bins, title=\"Signal-to-Noise Ratio\")  # Presence Ratio data = query.fetch(\"presence_ratio\") bins = np.linspace(0, 1, 100) plot_metric(axes[2], data, bins, title=\"Presence Ratio\")  # ISI Violation data = query.fetch(\"isi_violation\") bins = np.linspace(0, 1, 100) plot_metric(axes[3], data, bins, title=\"ISI Violation\")  # Number Violation data = query.fetch(\"number_violation\") bins = np.linspace(0, 1000, 100) plot_metric(axes[4], data, bins, title=\"Number Violation\") axes[4].set_ylabel(\"Count\")  # Amplitude Cutoff data = query.fetch(\"amplitude_cutoff\") bins = np.linspace(0, 0.5, 100) plot_metric(axes[5], data, bins, title=\"Amplitude Cutoff\")  # Isolation Distance data = query.fetch(\"isolation_distance\") bins = np.linspace(0, 170, 50) plot_metric(axes[6], data, bins, title=\"Isolation Distance\")  # L-Ratio data = query.fetch(\"l_ratio\") bins = np.linspace(0, 1, 100) plot_metric(axes[7], data, bins, title=\"L-Ratio\")  # d-Prime data = query.fetch(\"d_prime\") bins = np.linspace(0, 15, 50) plot_metric(axes[8], data, bins, title=\"d-Prime\") axes[8].set_ylabel(\"Count\")  # Nearest-Neighbors Hit Rate data = query.fetch(\"nn_hit_rate\") bins = np.linspace(0, 1, 100) plot_metric(axes[9], data, bins, title=\"Nearest-Neighbors Hit Rate\")  # Nearest-Neighbors Miss Rate data = query.fetch(\"nn_miss_rate\") bins = np.linspace(0, 1, 100) plot_metric(axes[10], data, bins, title=\"Nearest-Neighbors Miss Rate\")  # Silhouette Score data = query.fetch(\"silhouette_score\") bins = np.linspace(0, 1, 100) plot_metric(axes[11], data, bins, title=\"Silhouette Score\")  # Max Drift data = query.fetch(\"max_drift\") bins = np.linspace(0, 100, 100) plot_metric(axes[12], data, bins, title=\"Max Drift\") axes[12].set_ylabel(\"Count\")  # Cumulative Drift data = query.fetch(\"cumulative_drift\") bins = np.linspace(0, 100, 100) plot_metric(axes[13], data, bins, title=\"Cumulative Drift\")  [ax.remove() for ax in axes[14:]] plt.tight_layout() In\u00a0[7]: Copied! <pre>query = ephys.QualityMetrics.Waveform &amp; key\nquery\n</pre> query = ephys.QualityMetrics.Waveform &amp; key query Out[7]: Waveform metrics for a particular unit <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>paramset_idx</p> <p>curation_id</p> <p>unit</p> <p>amplitude</p> (uV) absolute difference between waveform peak and trough <p>duration</p> (ms) time between waveform peak and trough <p>halfwidth</p> (ms) spike width at half max amplitude <p>pt_ratio</p> absolute amplitude of peak divided by absolute amplitude of trough relative to 0 <p>repolarization_slope</p> the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak <p>recovery_slope</p> the recovery slope was defined by fitting a regression line to the first 30us from peak to tail <p>spread</p> (um) the range with amplitude above 12-percent of the maximum amplitude along the probe <p>velocity_above</p> (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe <p>velocity_below</p> (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe subject5 2023-04-21 00:40:32 1 1 1 0 120.402 1.08509 nan 0.74622 0.0388668 -0.3638 280.0 0.0735822 -0.0114461subject5 2023-04-21 00:40:32 1 1 1 1 151.819 1.05762 nan 0.657574 0.0700459 -0.0806154 300.0 -0.0576308 -0.137353subject5 2023-04-21 00:40:32 1 1 1 2 142.161 0.59062 nan 0.711684 0.175466 -0.119525 300.0 0.081758 -0.0572306subject5 2023-04-21 00:40:32 1 1 1 3 144.028 1.03015 nan 0.741737 0.0801046 -0.0811708 300.0 -0.138989 0.0711516subject5 2023-04-21 00:40:32 1 1 1 4 87.3382 0.563149 nan 0.869518 0.189142 -0.0258353 300.0 0.595198 -0.582626subject5 2023-04-21 00:40:32 1 1 1 5 80.6693 0.508208 nan 0.864687 0.24975 -0.0206796 300.0 -1.13317 1.08081subject5 2023-04-21 00:40:32 1 1 1 6 86.7877 0.576884 nan 0.834892 0.256677 -0.0614186 300.0 2.34645 0.0654064subject5 2023-04-21 00:40:32 1 1 1 7 80.8594 0.453266 nan 0.840421 0.212582 -0.0467612 300.0 2.08319 0.228922subject5 2023-04-21 00:40:32 1 1 1 8 100.313 1.05762 nan 0.800589 0.0222054 -0.518803 300.0 0.173843 -0.0654064subject5 2023-04-21 00:40:32 1 1 1 9 63.2377 0.576884 nan 0.909614 0.152643 -0.0177585 280.0 1.0138 0.147164subject5 2023-04-21 00:40:32 1 1 1 10 124.865 0.480737 nan 0.733803 0.26627 -0.0227592 300.0 0.869905 0.220747subject5 2023-04-21 00:40:32 1 1 1 11 56.4303 0.329648 nan 0.885421 0.202103 -0.0599684 300.0 3.85898 -2.00307 <p>...</p> <p>Total: 227</p> <p>Plot histograms of the waveform metrics.</p> In\u00a0[8]: Copied! <pre>fig, axes = plt.subplots(2, 4, figsize=(12, 5))\naxes = axes.flatten()\nplt.suptitle(f\"Waveform Quality Metrics for {key}\", y=.99, fontsize=12)\n\n# Amplitude\ndata = np.log10(query.fetch(\"amplitude\"))\nbins = np.linspace(0, 3, 100)\nplot_metric(axes[0], data, bins, title=\"Amplitude (\u03bcV) (log$_{10}$)\")\naxes[0].set_ylabel(\"Count\")\n\n# Duration\ndata = query.fetch(\"duration\")\nbins = np.linspace(0, 3, 100)\nplot_metric(axes[1], data, bins, title=\"Duration (ms)\")\n\n# Peak-to-Trough Ratio\ndata = query.fetch(\"pt_ratio\")\nbins = np.linspace(0, 1, 100)\nplot_metric(axes[2], data, bins, title=\"Peak-to-Trough Ratio\")\n\n# Repolarization Slope\ndata = query.fetch(\"repolarization_slope\")\nbins = np.linspace(-0.1, 2, 100)\nplot_metric(axes[3], data, bins, title=\"Repolarization Slope\")\n\n# Recovery Slope\ndata = query.fetch(\"recovery_slope\")\nbins = np.linspace(-0.5, 0.5, 100)\nplot_metric(axes[4], data, bins, title=\"Recovery Slope\")\naxes[4].set_ylabel(\"Count\")\n\n# Spread\ndata = np.log10(query.fetch(\"spread\"))\nbins = np.linspace(0, 3, 100)\nplot_metric(axes[5], data, bins, title=\"Spread (\u03bcm) (log$_{10}$)\")\n\n# Velocity Above\ndata = query.fetch(\"velocity_above\")\nbins = np.linspace(-5, 15, 100)\nplot_metric(axes[6], data, bins, title=\"Velocity Above (s/m)\")\n\n# Velocity Below\ndata = query.fetch(\"velocity_below\")\nbins = np.linspace(-10, 10, 100)\nplot_metric(axes[7], data, bins, title=\"Velocity Below (s/m)\")\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(2, 4, figsize=(12, 5)) axes = axes.flatten() plt.suptitle(f\"Waveform Quality Metrics for {key}\", y=.99, fontsize=12)  # Amplitude data = np.log10(query.fetch(\"amplitude\")) bins = np.linspace(0, 3, 100) plot_metric(axes[0], data, bins, title=\"Amplitude (\u03bcV) (log$_{10}$)\") axes[0].set_ylabel(\"Count\")  # Duration data = query.fetch(\"duration\") bins = np.linspace(0, 3, 100) plot_metric(axes[1], data, bins, title=\"Duration (ms)\")  # Peak-to-Trough Ratio data = query.fetch(\"pt_ratio\") bins = np.linspace(0, 1, 100) plot_metric(axes[2], data, bins, title=\"Peak-to-Trough Ratio\")  # Repolarization Slope data = query.fetch(\"repolarization_slope\") bins = np.linspace(-0.1, 2, 100) plot_metric(axes[3], data, bins, title=\"Repolarization Slope\")  # Recovery Slope data = query.fetch(\"recovery_slope\") bins = np.linspace(-0.5, 0.5, 100) plot_metric(axes[4], data, bins, title=\"Recovery Slope\") axes[4].set_ylabel(\"Count\")  # Spread data = np.log10(query.fetch(\"spread\")) bins = np.linspace(0, 3, 100) plot_metric(axes[5], data, bins, title=\"Spread (\u03bcm) (log$_{10}$)\")  # Velocity Above data = query.fetch(\"velocity_above\") bins = np.linspace(-5, 15, 100) plot_metric(axes[6], data, bins, title=\"Velocity Above (s/m)\")  # Velocity Below data = query.fetch(\"velocity_below\") bins = np.linspace(-10, 10, 100) plot_metric(axes[7], data, bins, title=\"Velocity Below (s/m)\") plt.tight_layout()"}, {"location": "tutorials/quality_metrics/#quality-metrics", "title": "Quality Metrics\u00b6", "text": "<p>Visualize the spike sorting quality metrics that are generated from the Kilosort results with the ecephys_spike_sorting package (i.e. <code>metrics.csv</code>) and stored in the DataJoint pipeline (i.e. <code>element-array-ephys</code>).</p> <p>If you are new to using this DataJoint pipeline for analyzing electrophysiology recordings from Neuropixels probes, please see the tutorial notebook for an in-depth explanation to set up and run the workflow.</p> <p>This notebook can run in a GitHub Codespace, and requires the example data to be populated into the database using the demo_prepare notebook.</p>"}, {"location": "tutorials/quality_metrics/#populate-the-qualitymetrics-table", "title": "Populate the <code>QualityMetrics</code> table\u00b6", "text": ""}, {"location": "tutorials/quality_metrics/#unit-quality-metrics", "title": "Unit quality metrics\u00b6", "text": "Metric Description Firing rate (Hz) Total number of spikes per second. Signal-to-noise ratio Ratio of the maximum amplitude of the mean spike waveform to the standard deviation of the background noise on a given channel. Presence ratio Proportion of time during a session that a unit is spiking, ranging from 0 to 0.99. Interspike interval (ISI) violation Rate of inter-spike-interval (ISI) refractory period violations. Number violation Total number of ISI violations. Amplitude cut-off False negative rate of a unit measured by the degree to which its distribution of spike amplitudes is truncated, indicating the fraction of missing spikes. An amplitude cutoff of 0.1 indicates approximately 10% missing spikes. Isolation distance A metric that uses the principal components (PCs) of a unit's waveforms, which are projected into a lower-dimensional PC space after spike sorting. This quantifies how well-isolated the unit is from other potential clusters. L-ratio A metric to quantify the distribution of spike distances from a cluster. A low L-ratio indicates that there is a relatively low number of non-member spikes around the target cluster. D-prime A metric calculated from waveform principal components using linear discriminant analysis. This measures the separability of one unit's PC cluster from all the others, with a higher d-prime value indicating better isolation of the unit. Nearest-neighbors hit rate The proportion of its nearest neighbors that belong to the same given cluster based on its first principal components. Nearest-neighbors miss rate The proportion of its nearest neighbors that do not belong to the same given cluster based on its first principal components. Silhouette score The ratio between cohesiveness of a cluster (distance between member spikes) and its separation from other clusters (distance to non-member spikes). Max drift The maximum shift in spike location, calculated as the center of mass of the energy of the first principal component score. Cumulative drift The cumulative change in spike position throughout a recording. <p>For further details of the quality metrics, please see:</p> <ul> <li><p>Allen Institute Documentation</p> </li> <li><p>Buccino et al., eLife 2020</p> </li> </ul> <p>We'll grab an example key for demonstration.</p>"}, {"location": "tutorials/quality_metrics/#waveform-quality-metrics", "title": "Waveform quality metrics\u00b6", "text": "Metric Description Amplitude (\u03bcV) Absolute difference between the waveform peak and trough. Duration (ms) Time interval between the waveform peak and trough. Peak-to-Trough (PT)  Ratio Absolute amplitude of the peak divided by the absolute amplitude of the trough relative to 0. Repolarization Slope Slope of the fitted regression line to the first 30\u03bcs from trough to peak. Recovery Slope Slope of the fitted regression line to the first 30\u03bcs from peak to tail. Spread (\u03bcm) Spatial extent of channels where the waveform amplitude exceeds 12% of the peak amplitude. Velocity Above (s/m) Inverse velocity of waveform propagation from the soma toward the top of the probe. Velocity Below (s/m) Inverse velocity of waveform propagation from the soma toward the bottom of the probe."}, {"location": "tutorials/tutorial/", "title": "Tutorial", "text": "In\u00a0[1]: Copied! <pre>import os\n\nif os.path.basename(os.getcwd()) == \"notebooks\":\n    os.chdir(\"..\")\n\nimport datajoint as dj\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\n</pre> import os  if os.path.basename(os.getcwd()) == \"notebooks\":     os.chdir(\"..\")  import datajoint as dj import datetime import matplotlib.pyplot as plt import numpy as np In\u00a0[2]: Copied! <pre>from workflow_array_ephys.pipeline import subject, session, probe, ephys\n</pre> from workflow_array_ephys.pipeline import subject, session, probe, ephys <pre>[2023-06-30 03:34:11,767][WARNING]: lab.Project and related tables will be removed in a future version of Element Lab. Please use the project schema.\n[2023-06-30 03:34:11,871][INFO]: Connecting root@fakeservices.datajoint.io:3306\n[2023-06-30 03:34:11,899][INFO]: Connected root@fakeservices.datajoint.io:3306\n</pre> In\u00a0[3]: Copied! <pre>(\n    dj.Diagram(subject.Subject)\n    + dj.Diagram(session.Session)\n    + dj.Diagram(probe)\n    + dj.Diagram(ephys)\n)\n</pre> (     dj.Diagram(subject.Subject)     + dj.Diagram(session.Session)     + dj.Diagram(probe)     + dj.Diagram(ephys) ) Out[3]: %3 ephys.EphysRecording ephys.EphysRecording ephys.LFP ephys.LFP ephys.EphysRecording-&gt;ephys.LFP ephys.ClusteringTask ephys.ClusteringTask ephys.EphysRecording-&gt;ephys.ClusteringTask ephys.EphysRecording.EphysFile ephys.EphysRecording.EphysFile ephys.EphysRecording-&gt;ephys.EphysRecording.EphysFile ephys.QualityMetrics ephys.QualityMetrics ephys.QualityMetrics.Cluster ephys.QualityMetrics.Cluster ephys.QualityMetrics-&gt;ephys.QualityMetrics.Cluster ephys.QualityMetrics.Waveform ephys.QualityMetrics.Waveform ephys.QualityMetrics-&gt;ephys.QualityMetrics.Waveform ephys.ClusteringMethod ephys.ClusteringMethod ephys.ClusteringParamSet ephys.ClusteringParamSet ephys.ClusteringMethod-&gt;ephys.ClusteringParamSet session.Session session.Session ephys.ProbeInsertion ephys.ProbeInsertion session.Session-&gt;ephys.ProbeInsertion ephys.InsertionLocation ephys.InsertionLocation probe.ElectrodeConfig.Electrode probe.ElectrodeConfig.Electrode ephys.CuratedClustering.Unit ephys.CuratedClustering.Unit probe.ElectrodeConfig.Electrode-&gt;ephys.CuratedClustering.Unit ephys.WaveformSet.Waveform ephys.WaveformSet.Waveform probe.ElectrodeConfig.Electrode-&gt;ephys.WaveformSet.Waveform ephys.LFP.Electrode ephys.LFP.Electrode probe.ElectrodeConfig.Electrode-&gt;ephys.LFP.Electrode ephys.AcquisitionSoftware ephys.AcquisitionSoftware ephys.AcquisitionSoftware-&gt;ephys.EphysRecording ephys.CuratedClustering.Unit-&gt;ephys.WaveformSet.Waveform ephys.CuratedClustering.Unit-&gt;ephys.QualityMetrics.Cluster ephys.CuratedClustering.Unit-&gt;ephys.QualityMetrics.Waveform ephys.WaveformSet.PeakWaveform ephys.WaveformSet.PeakWaveform ephys.CuratedClustering.Unit-&gt;ephys.WaveformSet.PeakWaveform ephys.ClusterQualityLabel ephys.ClusterQualityLabel ephys.ClusterQualityLabel-&gt;ephys.CuratedClustering.Unit probe.ElectrodeConfig probe.ElectrodeConfig probe.ElectrodeConfig-&gt;ephys.EphysRecording probe.ElectrodeConfig-&gt;probe.ElectrodeConfig.Electrode ephys.ProbeInsertion-&gt;ephys.EphysRecording ephys.ProbeInsertion-&gt;ephys.InsertionLocation probe.ProbeType probe.ProbeType probe.ProbeType-&gt;probe.ElectrodeConfig probe.Probe probe.Probe probe.ProbeType-&gt;probe.Probe probe.ProbeType.Electrode probe.ProbeType.Electrode probe.ProbeType-&gt;probe.ProbeType.Electrode probe.Probe-&gt;ephys.ProbeInsertion ephys.LFP-&gt;ephys.LFP.Electrode ephys.ClusteringParamSet-&gt;ephys.ClusteringTask ephys.Curation ephys.Curation ephys.CuratedClustering ephys.CuratedClustering ephys.Curation-&gt;ephys.CuratedClustering ephys.Clustering ephys.Clustering ephys.Clustering-&gt;ephys.Curation probe.ProbeType.Electrode-&gt;probe.ElectrodeConfig.Electrode ephys.WaveformSet ephys.WaveformSet ephys.WaveformSet-&gt;ephys.WaveformSet.Waveform ephys.WaveformSet-&gt;ephys.WaveformSet.PeakWaveform ephys.ClusteringTask-&gt;ephys.Clustering ephys.CuratedClustering-&gt;ephys.QualityMetrics ephys.CuratedClustering-&gt;ephys.CuratedClustering.Unit ephys.CuratedClustering-&gt;ephys.WaveformSet subject.Subject subject.Subject subject.Subject-&gt;session.Session In\u00a0[4]: Copied! <pre>print(subject.Subject.describe())\n</pre> print(subject.Subject.describe()) <pre>subject              : varchar(8)                   \n---\nsubject_nickname     : varchar(64)                  \nsex                  : enum('M','F','U')            \nsubject_birth_date   : date                         \nsubject_description  : varchar(1024)                \n\n</pre> In\u00a0[5]: Copied! <pre>subject.Subject.heading\n</pre> subject.Subject.heading Out[5]: <pre># \nsubject              : varchar(8)                   # \n---\nsubject_nickname     : varchar(64)                  # \nsex                  : enum('M','F','U')            # \nsubject_birth_date   : date                         # \nsubject_description  : varchar(1024)                # </pre> <p>The cells above show all attributes of the subject table. These are particularly useful functions if you are new to DataJoint Elements and are unsure of the attributes required for each table. We will insert data into the <code>subject.Subject</code> table.</p> In\u00a0[6]: Copied! <pre>subject.Subject.insert1(\n    dict(\n        subject='subject5',\n        subject_nickname='subject5',\n        subject_birth_date='2023-01-01',\n        sex='U',\n        subject_description='Example subject'\n    )\n)\nsubject.Subject()\n</pre> subject.Subject.insert1(     dict(         subject='subject5',         subject_nickname='subject5',         subject_birth_date='2023-01-01',         sex='U',         subject_description='Example subject'     ) ) subject.Subject() Out[6]: <p>subject</p> <p>subject_nickname</p> <p>sex</p> <p>subject_birth_date</p> <p>subject_description</p> subject5 subject5 U 2023-01-01 Example subject <p>Total: 1</p> <p>Let's repeat the steps above for the <code>Session</code> table and see how the output varies between <code>.describe</code> and <code>.heading</code>.</p> In\u00a0[7]: Copied! <pre>print(session.Session.describe())\n</pre> print(session.Session.describe()) <pre>-&gt; subject.Subject\nsession_datetime     : datetime                     \n\n</pre> In\u00a0[8]: Copied! <pre>session.Session.heading\n</pre> session.Session.heading Out[8]: <pre># \nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # </pre> <p>The cells above show the dependencies and attributes for the <code>session.Session</code> table. Notice that <code>describe</code> shows the dependencies of the table on upstream tables. The <code>Session</code> table depends on the upstream <code>Subject</code> table.</p> <p>Whereas <code>heading</code> lists all the attributes of the <code>Session</code> table, regardless of whether they are declared in an upstream table.</p> <p>Here we will demonstrate a very useful way of inserting data by assigning the dictionary to a variable <code>session_key</code>. This variable can be used to insert entries into tables that contain the <code>Session</code> table as one of its attributes.</p> In\u00a0[9]: Copied! <pre>session_key = dict(subject='subject5', \n                   session_datetime=datetime.datetime.now())\n\nsession.Session.insert1(session_key)\nsession.Session()\n</pre> session_key = dict(subject='subject5',                     session_datetime=datetime.datetime.now())  session.Session.insert1(session_key) session.Session() Out[9]: <p>subject</p> <p>session_datetime</p> subject5 2023-06-30 03:34:20 <p>Total: 1</p> <p>The <code>SessionDirectory</code> table locates the relevant data files in a directory path relative to the root directory defined in your <code>dj.config[\"custom\"]</code>. More information about <code>dj.config</code> is provided at the end of this tutorial and is particularly useful for local deployments of this workflow.</p> In\u00a0[10]: Copied! <pre>print(session.SessionDirectory.describe())\n</pre> print(session.SessionDirectory.describe()) <pre>-&gt; session.Session\n---\nsession_dir          : varchar(256)                 # Path to the data directory for a session\n\n</pre> In\u00a0[11]: Copied! <pre>session.SessionDirectory.heading\n</pre> session.SessionDirectory.heading Out[11]: <pre># \nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \n---\nsession_dir          : varchar(256)                 # Path to the data directory for a session</pre> In\u00a0[12]: Copied! <pre>session.SessionDirectory.insert1(\n    dict(\n        session_key, \n        session_dir='subject5/session1'\n    )\n)\nsession.SessionDirectory()\n</pre> session.SessionDirectory.insert1(     dict(         session_key,          session_dir='subject5/session1'     ) ) session.SessionDirectory() Out[12]: <p>subject</p> <p>session_datetime</p> <p>session_dir</p> Path to the data directory for a session subject5 2023-06-30 03:34:20 subject5/session1 <p>Total: 1</p> <p>As the workflow diagram indicates, the tables in the <code>probe</code> schemas need to contain data before the tables in the <code>ephys</code> schema accept any data. Let's start by inserting into <code>probe.Probe</code>, a table containing metadata about a multielectrode probe.</p> In\u00a0[13]: Copied! <pre>print(probe.Probe.describe())\n</pre> print(probe.Probe.describe()) <pre># Represent a physical probe with unique identification\nprobe                : varchar(32)                  # unique identifier for this model of probe (e.g. serial number)\n---\n-&gt; probe.ProbeType\nprobe_comment        : varchar(1000)                \n\n</pre> In\u00a0[14]: Copied! <pre>probe.Probe.heading\n</pre> probe.Probe.heading Out[14]: <pre># Represent a physical probe with unique identification\nprobe                : varchar(32)                  # unique identifier for this model of probe (e.g. serial number)\n---\nprobe_type           : varchar(32)                  # e.g. neuropixels_1.0\nprobe_comment        : varchar(1000)                # </pre> In\u00a0[15]: Copied! <pre>probe.Probe.insert1(\n    dict(probe=\"714000838\", \n         probe_type=\"neuropixels 1.0 - 3B\",\n         probe_comment=\"Example probe\")\n)  # this info could be achieve from neuropixels meta file.\nprobe.Probe()\n</pre> probe.Probe.insert1(     dict(probe=\"714000838\",           probe_type=\"neuropixels 1.0 - 3B\",          probe_comment=\"Example probe\") )  # this info could be achieve from neuropixels meta file. probe.Probe() Out[15]: Represent a physical probe with unique identification <p>probe</p> unique identifier for this model of probe (e.g. serial number) <p>probe_type</p> e.g. neuropixels_1.0 <p>probe_comment</p> 714000838 neuropixels 1.0 - 3B Example probe <p>Total: 1</p> <p>The probe metadata is used by the downstream <code>ProbeInsertion</code> table which we insert data into in the cells below:</p> In\u00a0[16]: Copied! <pre>print(ephys.ProbeInsertion.describe())\n</pre> print(ephys.ProbeInsertion.describe()) <pre># Probe insertion implanted into an animal for a given session.\n-&gt; session.Session\ninsertion_number     : tinyint unsigned             \n---\n-&gt; probe.Probe\n\n</pre> In\u00a0[17]: Copied! <pre>ephys.ProbeInsertion.heading\n</pre> ephys.ProbeInsertion.heading Out[17]: <pre># Probe insertion implanted into an animal for a given session.\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \ninsertion_number     : tinyint unsigned             # \n---\nprobe                : varchar(32)                  # unique identifier for this model of probe (e.g. serial number)</pre> In\u00a0[18]: Copied! <pre>ephys.ProbeInsertion.insert1(\n    dict(\n        session_key,\n        insertion_number=1,\n        probe=\"714000838\",\n    )\n) # probe, subject, session_datetime needs to follow the restrictions of foreign keys.\nephys.ProbeInsertion()\n</pre> ephys.ProbeInsertion.insert1(     dict(         session_key,         insertion_number=1,         probe=\"714000838\",     ) ) # probe, subject, session_datetime needs to follow the restrictions of foreign keys. ephys.ProbeInsertion() Out[18]: Probe insertion implanted into an animal for a given session. <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>probe</p> unique identifier for this model of probe (e.g. serial number) subject5 2023-06-30 03:34:20 1 714000838 <p>Total: 1</p> In\u00a0[19]: Copied! <pre>ephys.EphysRecording.heading\n</pre> ephys.EphysRecording.heading Out[19]: <pre># Ephys recording from a probe insertion for a given session.\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \ninsertion_number     : tinyint unsigned             # \n---\nelectrode_config_hash : uuid                         # \nacq_software         : varchar(24)                  # \nsampling_rate        : float                        # (Hz)\nrecording_datetime   : datetime                     # datetime of the recording from this probe\nrecording_duration   : float                        # (seconds) duration of the recording from this probe</pre> In\u00a0[20]: Copied! <pre>ephys.EphysRecording.EphysFile.heading\n</pre> ephys.EphysRecording.EphysFile.heading Out[20]: <pre># Paths of files of a given EphysRecording round.\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \ninsertion_number     : tinyint unsigned             # \nfile_path            : varchar(255)                 # filepath relative to root data directory</pre> In\u00a0[21]: Copied! <pre>ephys.EphysRecording()\n</pre> ephys.EphysRecording() Out[21]: Ephys recording from a probe insertion for a given session. <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>electrode_config_hash</p> <p>acq_software</p> <p>sampling_rate</p> (Hz) <p>recording_datetime</p> datetime of the recording from this probe <p>recording_duration</p> (seconds) duration of the recording from this probe <p>Total: 0</p> In\u00a0[22]: Copied! <pre>ephys.EphysRecording.EphysFile()\n</pre> ephys.EphysRecording.EphysFile() Out[22]: Paths of files of a given EphysRecording round. <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>file_path</p> filepath relative to root data directory <p>Total: 0</p> In\u00a0[23]: Copied! <pre>ephys.EphysRecording.populate(session_key, display_progress=True)\n</pre> ephys.EphysRecording.populate(session_key, display_progress=True) <pre>EphysRecording:   0%|          | 0/1 [00:00&lt;?, ?it/s]EphysRecording: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.07s/it]\n</pre> <p>Let's view the information was entered into each of these tables:</p> In\u00a0[24]: Copied! <pre>ephys.EphysRecording()\n</pre> ephys.EphysRecording() Out[24]: Ephys recording from a probe insertion for a given session. <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>electrode_config_hash</p> <p>acq_software</p> <p>sampling_rate</p> (Hz) <p>recording_datetime</p> datetime of the recording from this probe <p>recording_duration</p> (seconds) duration of the recording from this probe subject5 2023-06-30 03:34:20 1 8d4cc6d8-a02d-42c8-bf27-7459c39ea0ee SpikeGLX 30000.0 2018-07-03 20:32:28 338.666 <p>Total: 1</p> In\u00a0[25]: Copied! <pre>ephys.EphysRecording.EphysFile()\n</pre> ephys.EphysRecording.EphysFile() Out[25]: Paths of files of a given EphysRecording round. <p>subject</p> <p>session_datetime</p> <p>insertion_number</p> <p>file_path</p> filepath relative to root data directory subject5 2023-06-30 03:34:20 1 subject5/session1/probe_1/npx_g0_t0.imec.ap.meta <p>Total: 1</p> <p>We're almost ready to spike sort the data with <code>kilosort</code>. An important step before processing is managing the parameters which will be used in that step. To do so, we will define the kilosort parameters in a dictionary and insert them into a DataJoint table <code>ClusteringParamSet</code>. This table keeps track of all combinations of your spike sorting parameters. You can choose which parameters are used during processing in a later step.</p> <p>Let's view the attributes and insert data into <code>ephys.ClusteringParamSet</code>.</p> In\u00a0[26]: Copied! <pre>ephys.ClusteringParamSet.heading\n</pre> ephys.ClusteringParamSet.heading Out[26]: <pre># Parameter set to be used in a clustering procedure\nparamset_idx         : smallint                     # \n---\nclustering_method    : varchar(16)                  # \nparamset_desc        : varchar(128)                 # \nparam_set_hash       : uuid                         # \nparams               : longblob                     # dictionary of all applicable parameters</pre> In\u00a0[27]: Copied! <pre># insert clustering task manually\nparams_ks = {\n    \"fs\": 30000,\n    \"fshigh\": 150,\n    \"minfr_goodchannels\": 0.1,\n    \"Th\": [10, 4],\n    \"lam\": 10,\n    \"AUCsplit\": 0.9,\n    \"minFR\": 0.02,\n    \"momentum\": [20, 400],\n    \"sigmaMask\": 30,\n    \"ThPr\": 8,\n    \"spkTh\": -6,\n    \"reorder\": 1,\n    \"nskip\": 25,\n    \"GPU\": 1,\n    \"Nfilt\": 1024,\n    \"nfilt_factor\": 4,\n    \"ntbuff\": 64,\n    \"whiteningRange\": 32,\n    \"nSkipCov\": 25,\n    \"scaleproc\": 200,\n    \"nPCs\": 3,\n    \"useRAM\": 0,\n}\nephys.ClusteringParamSet.insert_new_params(\n    clustering_method=\"kilosort2\",\n    paramset_idx=0,\n    params=params_ks,\n    paramset_desc=\"Spike sorting using Kilosort2\",\n)\nephys.ClusteringParamSet()\n</pre> # insert clustering task manually params_ks = {     \"fs\": 30000,     \"fshigh\": 150,     \"minfr_goodchannels\": 0.1,     \"Th\": [10, 4],     \"lam\": 10,     \"AUCsplit\": 0.9,     \"minFR\": 0.02,     \"momentum\": [20, 400],     \"sigmaMask\": 30,     \"ThPr\": 8,     \"spkTh\": -6,     \"reorder\": 1,     \"nskip\": 25,     \"GPU\": 1,     \"Nfilt\": 1024,     \"nfilt_factor\": 4,     \"ntbuff\": 64,     \"whiteningRange\": 32,     \"nSkipCov\": 25,     \"scaleproc\": 200,     \"nPCs\": 3,     \"useRAM\": 0, } ephys.ClusteringParamSet.insert_new_params(     clustering_method=\"kilosort2\",     paramset_idx=0,     params=params_ks,     paramset_desc=\"Spike sorting using Kilosort2\", ) ephys.ClusteringParamSet() Out[27]: Parameter set to be used in a clustering procedure <p>paramset_idx</p> <p>clustering_method</p> <p>paramset_desc</p> <p>param_set_hash</p> <p>params</p> dictionary of all applicable parameters 0 kilosort2 Spike sorting using Kilosort2 de78cee1-526f-319e-b6d5-8a2ba04963d8 =BLOB= <p>Total: 1</p> <p>Now that we've inserted kilosort parameters into the <code>ClusteringParamSet</code> table, we're almost ready to sort our data. DataJoint uses a <code>ClusteringTask</code> table to manage which <code>EphysRecording</code> and <code>ClusteringParamSet</code> should be used during processing.</p> <p>This table is important for defining several important aspects of downstream processing. Let's view the attributes to get a better understanding.</p> In\u00a0[28]: Copied! <pre>print(ephys.ClusteringTask.describe())\n</pre> print(ephys.ClusteringTask.describe()) <pre># Manual table for defining a clustering task ready to be run\n-&gt; ephys.EphysRecording\n-&gt; ephys.ClusteringParamSet\n---\nclustering_output_dir : varchar(255)                 # clustering output directory relative to the clustering root data directory\ntask_mode            : enum('load','trigger')       # 'load': load computed analysis results, 'trigger': trigger computation\n\n</pre> In\u00a0[29]: Copied! <pre>ephys.ClusteringTask.heading\n</pre> ephys.ClusteringTask.heading Out[29]: <pre># Manual table for defining a clustering task ready to be run\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \ninsertion_number     : tinyint unsigned             # \nparamset_idx         : smallint                     # \n---\nclustering_output_dir : varchar(255)                 # clustering output directory relative to the clustering root data directory\ntask_mode            : enum('load','trigger')       # 'load': load computed analysis results, 'trigger': trigger computation</pre> <p>The <code>ClusteringTask</code> table contains two important attributes:</p> <ul> <li><code>paramset_idx</code></li> <li><code>task_mode</code></li> </ul> <p>The <code>paramset_idx</code> attribute is tracks your kilosort parameter sets. You can choose the parameter set using which you want spike sort ephys data. For example, <code>paramset_idx=0</code> may contain default parameters for kilosort processing whereas <code>paramset_idx=1</code> contains your custom parameters for sorting. This attribute tells the <code>Processing</code> table which set of parameters you are processing in a given <code>populate()</code>.</p> <p>The <code>task_mode</code> attribute can be set to either <code>load</code> or <code>trigger</code>. When set to <code>load</code>, running the processing step initiates a search for exisiting kilosort output files. When set to <code>trigger</code>, the processing step will run kilosort on the raw data.</p> In\u00a0[30]: Copied! <pre>ephys.ClusteringTask.insert1(\n    dict(\n        session_key,\n        insertion_number=1,\n        paramset_idx=0,\n        task_mode='load', # load or trigger\n        clustering_output_dir=\"subject5/session1/probe_1/kilosort2-5_1\"\n    )\n)\n</pre> ephys.ClusteringTask.insert1(     dict(         session_key,         insertion_number=1,         paramset_idx=0,         task_mode='load', # load or trigger         clustering_output_dir=\"subject5/session1/probe_1/kilosort2-5_1\"     ) ) <p>Notice we set the <code>task_mode</code> to <code>load</code>. Let's call populate on the <code>Clustering</code> table in the pipeline.</p> In\u00a0[31]: Copied! <pre>ephys.Clustering.populate(session_key, display_progress=True)\n</pre> ephys.Clustering.populate(session_key, display_progress=True) <pre>Clustering:   0%|          | 0/1 [00:00&lt;?, ?it/s]Clustering: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.85it/s]\n</pre> <p>While spike sorting is completed in the above step, you can optionally curate the output of image processing using the <code>Curation</code> table. For this demo, we will simply use the results of the spike sorting output from the <code>Clustering</code> task.</p> In\u00a0[32]: Copied! <pre>ephys.Curation.heading\n</pre> ephys.Curation.heading Out[32]: <pre># Manual curation procedure\nsubject              : varchar(8)                   # \nsession_datetime     : datetime                     # \ninsertion_number     : tinyint unsigned             # \nparamset_idx         : smallint                     # \ncuration_id          : int                          # \n---\ncuration_time        : datetime                     # time of generation of this set of curated clustering results\ncuration_output_dir  : varchar(255)                 # output directory of the curated results, relative to root data directory\nquality_control      : tinyint                      # has this clustering result undergone quality control?\nmanual_curation      : tinyint                      # has manual curation been performed on this clustering result?\ncuration_note        : varchar(2000)                # </pre> In\u00a0[33]: Copied! <pre>clustering_key = (ephys.ClusteringTask &amp; session_key).fetch1('KEY')\nephys.Curation().create1_from_clustering_task(clustering_key)\n</pre> clustering_key = (ephys.ClusteringTask &amp; session_key).fetch1('KEY') ephys.Curation().create1_from_clustering_task(clustering_key) <p>Once the <code>Curation</code> table receives an entry, we can populate the remaining tables in the workflow including <code>CuratedClustering</code>, <code>WaveformSet</code>, and <code>LFP</code>.</p> In\u00a0[34]: Copied! <pre>ephys.CuratedClustering.populate(session_key, display_progress=True)\nephys.LFP.populate(session_key, display_progress=True)\nephys.WaveformSet.populate(session_key, display_progress=True)\n</pre> ephys.CuratedClustering.populate(session_key, display_progress=True) ephys.LFP.populate(session_key, display_progress=True) ephys.WaveformSet.populate(session_key, display_progress=True) <pre>CuratedClustering:   0%|          | 0/1 [00:00&lt;?, ?it/s]/home/vscode/.local/lib/python3.9/site-packages/element_array_ephys/readers/kilosort.py:172: RuntimeWarning: invalid value encountered in divide\n  self._data[\"spike_depths\"] = np.sum(\nCuratedClustering: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:26&lt;00:00, 26.13s/it]\nLFP: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [01:27&lt;00:00, 87.95s/it]\nWaveformSet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:53&lt;00:00, 53.70s/it]\n</pre> <p>Now that we've populated the tables in this workflow, there are one of several next steps. If you have an existing workflow for aligning waveforms to behavior data or other stimuli, you can easily invoke <code>element-event</code> or define your custom DataJoint tables to extend the pipeline.</p> <p>In this tutorial, we will do some exploratory analysis by fetching the data from the database and creating a few plots.</p> In\u00a0[35]: Copied! <pre>lfp_average = (ephys.LFP &amp; \"insertion_number = '1'\").fetch1(\"lfp_mean\")\n</pre> lfp_average = (ephys.LFP &amp; \"insertion_number = '1'\").fetch1(\"lfp_mean\") <p>In the query above, we fetch a single <code>lfp_mean</code> attribute from the <code>LFP</code> table. We also restrict the query to insertion number 1.</p> <p>Let's go ahead and plot the LFP mean.</p> In\u00a0[36]: Copied! <pre>plt.plot(lfp_average)\nplt.title(\"Average LFP Waveform for Insertion 1\")\nplt.xlabel(\"Samples\")\nplt.ylabel(\"microvolts (uV)\");\n</pre> plt.plot(lfp_average) plt.title(\"Average LFP Waveform for Insertion 1\") plt.xlabel(\"Samples\") plt.ylabel(\"microvolts (uV)\"); <p>DataJoint queries are a highly flexible tool to manipulate and visualize your data. After all, visualizing traces or generating rasters is likely just the start of your analysis workflow. This can also make the queries seem more complex at first. However, we'll walk through them slowly to simplify their content in this notebook.</p> <p>The examples below perform several operations using DataJoint queries:</p> <ul> <li>Fetch the primary key attributes of all units that are in <code>insertion_number=1</code>.</li> <li>Use multiple restrictions to fetch timestamps and create a raster plot.</li> <li>Use a join operation and multiple restrictions to fetch a waveform trace, along with unit data to create a single waveform plot</li> </ul> In\u00a0[37]: Copied! <pre>insert_key = (ephys.ProbeInsertion &amp; \"insertion_number = '1'\").fetch1(\"KEY\")\nunits, unit_spiketimes = (ephys.CuratedClustering.Unit &amp; insert_key &amp; 'unit IN (\"6\",\"7\",\"9\",\"14\",\"15\",\"17\",\"19\")').fetch(\"unit\", \"spike_times\")\n</pre> insert_key = (ephys.ProbeInsertion &amp; \"insertion_number = '1'\").fetch1(\"KEY\") units, unit_spiketimes = (ephys.CuratedClustering.Unit &amp; insert_key &amp; 'unit IN (\"6\",\"7\",\"9\",\"14\",\"15\",\"17\",\"19\")').fetch(\"unit\", \"spike_times\")  In\u00a0[38]: Copied! <pre>x = np.hstack(unit_spiketimes)\ny = np.hstack([np.full_like(s, u) for u, s in zip(units, unit_spiketimes)])\nplt.plot(x, y, \"|\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Unit\");\n</pre> x = np.hstack(unit_spiketimes) y = np.hstack([np.full_like(s, u) for u, s in zip(units, unit_spiketimes)]) plt.plot(x, y, \"|\") plt.xlabel(\"Time (s)\") plt.ylabel(\"Unit\"); In\u00a0[39]: Copied! <pre>unit_key = (ephys.CuratedClustering.Unit &amp; insert_key &amp; \"unit = '15'\").fetch1(\"KEY\")\nunit_data = (ephys.CuratedClustering.Unit * ephys.WaveformSet.PeakWaveform &amp; unit_key).fetch1()\n</pre> unit_key = (ephys.CuratedClustering.Unit &amp; insert_key &amp; \"unit = '15'\").fetch1(\"KEY\") unit_data = (ephys.CuratedClustering.Unit * ephys.WaveformSet.PeakWaveform &amp; unit_key).fetch1() In\u00a0[40]: Copied! <pre>unit_data\n</pre> unit_data Out[40]: <pre>{'subject': 'subject5',\n 'session_datetime': datetime.datetime(2023, 6, 30, 3, 34, 20),\n 'insertion_number': 1,\n 'paramset_idx': 0,\n 'curation_id': 1,\n 'unit': 15,\n 'electrode_config_hash': UUID('8d4cc6d8-a02d-42c8-bf27-7459c39ea0ee'),\n 'probe_type': 'neuropixels 1.0 - 3A',\n 'electrode': 92,\n 'cluster_quality_label': 'noise',\n 'spike_count': 292,\n 'spike_times': array([  1.02606667,   1.19973333,   1.5044    ,   1.52283333,\n          1.86786667,   1.8688    ,   1.8806    ,   1.88553333,\n          2.0581    ,   2.76826667,   2.89186667,   2.9311    ,\n          4.8753    ,   5.2964    ,   7.02266667,   9.65273333,\n          9.81373333,  10.22443333,  11.96146667,  15.07173333,\n         15.08983333,  15.78326667,  21.30936667,  22.7549    ,\n         23.5582    ,  23.6582    ,  23.87043333,  24.16726667,\n         24.25456667,  24.2671    ,  25.0249    ,  27.89116667,\n         35.3036    ,  36.0171    ,  40.00396667,  40.0873    ,\n         41.015     ,  42.70086667,  45.8682    ,  47.9291    ,\n         48.90843333,  49.37996667,  49.39596667,  49.4058    ,\n         49.65926667,  49.68203333,  49.74273333,  51.52213333,\n         52.41486667,  55.27623333,  55.54576667,  55.81213333,\n         56.0544    ,  56.29426667,  56.36896667,  56.3743    ,\n         56.47403333,  56.7147    ,  56.71683333,  60.764     ,\n         61.5317    ,  61.54046667,  61.7721    ,  62.10233333,\n         62.10726667,  62.2118    ,  62.81173333,  63.15896667,\n         65.11126667,  65.495     ,  67.18373333,  77.59256667,\n         79.2709    ,  80.46186667,  82.1178    ,  85.6568    ,\n         86.52613333,  89.12126667,  89.46963333,  89.64663333,\n         90.19103333,  92.0923    ,  92.99573333,  93.36923333,\n         93.68086667,  95.2097    ,  97.96296667,  98.1067    ,\n         98.69713333,  99.26963333,  99.28013333, 101.04216667,\n        101.2002    , 101.3843    , 101.3975    , 101.40656667,\n        102.00996667, 102.07066667, 102.17033333, 103.5861    ,\n        104.68523333, 104.71643333, 105.2595    , 105.5166    ,\n        105.54723333, 107.0848    , 109.82746667, 110.14853333,\n        110.92203333, 111.2481    , 112.26      , 113.09466667,\n        113.09846667, 113.1005    , 113.33513333, 118.842     ,\n        118.96803333, 119.32606667, 119.64796667, 119.96313333,\n        119.97343333, 120.19573333, 120.19693333, 120.21163333,\n        121.1879    , 121.233     , 121.9611    , 122.40203333,\n        123.1745    , 124.6798    , 124.68196667, 124.97396667,\n        125.2205    , 125.87786667, 125.957     , 125.95803333,\n        127.7149    , 136.8558    , 136.86413333, 138.49966667,\n        139.49553333, 139.93163333, 139.9819    , 142.57013333,\n        142.70053333, 143.343     , 143.40796667, 148.46066667,\n        148.47023333, 148.89893333, 152.79766667, 153.2198    ,\n        153.39096667, 153.62456667, 153.82253333, 153.82693333,\n        153.8364    , 153.98863333, 155.4047    , 157.52706667,\n        157.53233333, 163.1177    , 163.12516667, 163.46916667,\n        163.69406667, 163.7119    , 166.6946    , 166.7006    ,\n        170.78053333, 170.7891    , 170.79653333, 180.36596667,\n        180.43416667, 182.72556667, 182.73706667, 182.76576667,\n        184.24713333, 185.5131    , 185.7329    , 186.4198    ,\n        186.48443333, 186.72036667, 186.95926667, 187.1402    ,\n        187.16683333, 189.4036    , 193.27583333, 195.40946667,\n        200.2427    , 203.03376667, 203.04076667, 205.84043333,\n        206.1151    , 207.3402    , 210.95773333, 217.73583333,\n        220.35      , 221.1235    , 227.7719    , 227.946     ,\n        228.02456667, 231.2108    , 231.80003333, 233.48253333,\n        236.8342    , 241.78993333, 243.6285    , 245.61546667,\n        245.95573333, 246.04586667, 246.06206667, 247.12933333,\n        248.17143333, 249.16873333, 251.13613333, 252.4734    ,\n        253.2007    , 254.50943333, 255.22563333, 255.2502    ,\n        255.5314    , 255.53423333, 255.7684    , 256.81196667,\n        256.99066667, 256.9928    , 257.0588    , 257.06206667,\n        257.0801    , 257.15523333, 257.27496667, 257.291     ,\n        257.99483333, 259.6955    , 259.7061    , 259.71746667,\n        263.59203333, 266.7052    , 266.70576667, 267.0376    ,\n        267.35913333, 267.87316667, 268.36156667, 268.94556667,\n        269.01516667, 269.63623333, 269.88553333, 270.16176667,\n        270.39123333, 273.41633333, 273.43763333, 274.2084    ,\n        276.36113333, 278.68053333, 279.50626667, 281.0338    ,\n        281.9869    , 283.69233333, 285.0663    , 287.0179    ,\n        289.5541    , 291.78113333, 292.16493333, 292.85823333,\n        292.8702    , 295.93823333, 295.9514    , 295.9679    ,\n        296.00623333, 296.0391    , 296.04393333, 298.9717    ,\n        299.01556667, 299.0859    , 299.7463    , 306.65153333,\n        307.38243333, 307.7771    , 311.80163333, 311.83026667,\n        313.37816667, 314.948     , 317.10726667, 317.98363333,\n        319.25856667, 321.01533333, 321.16426667, 330.15466667,\n        330.73633333, 330.91536667, 332.83243333, 332.86266667]),\n 'spike_sites': array([92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n        92, 92, 92]),\n 'spike_depths': array([ 917.68931051, 1002.6141662 ,  893.1431346 ,  912.33104931,\n         902.87693568,  872.31105976,  875.26191363,  922.47333477,\n         949.16252597,  801.99946108,  858.01692677,  921.7837487 ,\n         897.25087548,  874.42894267,  914.89717798,  861.55454858,\n         904.22082978,  900.71556806,  848.89880207,  871.55857472,\n         849.40502424,  887.21898509,  957.57407264,  857.52950094,\n         941.13752662,  931.03217518,  898.99527832,  962.92735677,\n         915.56550832,  922.14542481,  894.27941318,  874.31413165,\n         847.78323382,  960.73651726,  902.09721294,  899.97936727,\n         936.76767365,  908.68471877,  931.97562897,  892.8563381 ,\n         940.49920044,  925.87119487,  915.55667036,  786.01763842,\n         867.61268444, 1007.66794229,  867.43220284,  941.51843395,\n         955.66765617,  970.36344663,  894.82624021,  915.00922015,\n         889.99169864,  878.28545616,  850.33714087,  928.43421969,\n         939.52352775,  962.60336897,  903.36858487,  896.37548128,\n         963.73199174,  789.82843798,  920.43722936,  828.40065648,\n         943.98810503,  835.85766882,  841.76946105,  937.36801859,\n         896.49642885,  967.15672796,  918.39713395,  906.36795986,\n         919.40916519,  875.89694182,  908.3418033 ,  972.05674531,\n         927.07275902,  940.77150366,  918.04680759,  981.59648505,\n         940.32229921,  938.36235287,  849.04038544,  893.3058527 ,\n         913.09883832,  912.35625832,  989.78190796,  879.34780591,\n         928.81644221,  892.97322993,  898.70158737,  906.91746418,\n         946.42526162,  914.08702993,  859.34034454,  907.18338275,\n         989.30138781,  926.06769221,  838.08269605,  929.50654863,\n         827.39586238,  927.36188021,  910.17661856,  935.41822322,\n         885.63661049,  914.48018584,  895.93753147,  911.13864214,\n         881.60996972,  938.20931097, 1015.75963624,  938.56188955,\n         909.09481672,  867.81417174,  912.15092578,  907.0519042 ,\n         913.43873474,  913.23570042,  983.89208176,  899.44708324,\n         910.55603061,  950.67199374,  916.27718038,  886.16272254,\n         975.57372749,  882.46972449,  908.99151954,  893.41760103,\n         938.6441373 ,  885.19738826,  947.44382361,  918.75218854,\n         942.78614663,  968.52291541,  915.78656008,  989.00683215,\n         855.88223229,  976.62553529,  919.25644927,  958.63970635,\n         934.03540249,  926.45247121,  961.36373273,  926.55709697,\n         896.6375551 ,  897.47943897,  907.74803191,  954.04187795,\n         882.10295293, 1004.09432843,  892.73726557,  846.13198111,\n         929.42733278,  894.24531402,  921.97988827,  860.601478  ,\n         912.55635483,  997.22339505,  985.88635074,  938.45775184,\n         944.63766895,  942.18376197,  984.23087354,  922.40370934,\n         965.34813049,  921.32552393,  969.81367405,  917.74503135,\n         833.76599428,  894.97963584,  878.49090123,  864.91049261,\n         985.04808527,  927.18133162,  844.64492657,  913.88047009,\n         928.03561194,  930.79686847,  930.13055052,  839.5761256 ,\n         943.37875897,  956.44476838,  931.39781252,  989.20932655,\n        1006.29928459,  942.03146682,  901.72518137,  932.53880871,\n         935.96943144,  925.53161728,  874.67439465,  951.48997974,\n         960.        ,  874.14717137,  921.49562818,  927.26749515,\n         926.41158172,  896.61260138,  938.6103718 ,  959.64828575,\n         935.22630845,  955.63103852,  950.8495808 ,  888.45366602,\n         912.06814044,  862.81354284,  875.05713562,  875.4100677 ,\n         923.5426824 ,  839.78562837,  889.74471045,  867.8332007 ,\n         933.88736839,  868.07155277,  883.34738777,  913.12741673,\n         915.77804797,  962.99775026,  932.9485248 ,  930.37415413,\n         944.21567324,  776.98228821,  936.37311401,  941.58975024,\n         941.55965056,  864.51860126,  930.38329188,  912.78502395,\n         947.32712079, 1011.13494089,  937.99624501,  916.59535343,\n         873.82437868,  901.49630725,  846.11674006,  998.64016883,\n         978.32805846,  876.27655411,  905.78089027,  827.25234516,\n        1002.18813125,  898.65291068,  920.350764  ,  874.17361952,\n         938.70381451,  945.81921622,  874.98921047,  915.97985629,\n         920.14460695,  965.25282342,  977.01542391,  863.18856888,\n         907.62393082,  906.19584978,  851.37149337,  956.85804045,\n         969.94521463,  912.02588788,  953.60047228,  819.53086148,\n         911.99611929,  845.66361521,  962.24964999,  921.43338903,\n         982.04267741,  931.1841306 ,  896.56744607,  926.6958426 ,\n         841.56778533,  961.77261899,  943.15601639,  786.18136435,\n         864.98026608,  950.29718422,  930.39820918,  983.57154574,\n         909.55498598,  925.76591071,  906.07808244,  964.41923255,\n         911.17276099,  919.34859876,  906.11721418,  905.96844161,\n         911.53683431,  958.97512493,  953.15452691,  889.09240389,\n         957.98473324,  993.29170991,  928.99175736,  908.10563408]),\n 'peak_electrode_waveform': array([-503.33636558, -502.734375  , -501.32973031, -500.34246575,\n        -499.73244863, -498.66491866, -494.41887842, -491.6015625 ,\n        -486.59300086, -481.55233305, -474.36055223, -466.79152397,\n        -458.98169949, -451.10766267, -442.41491866, -431.03328339,\n        -420.81549658, -410.91074486, -402.25010702, -395.59610445,\n        -389.25513699, -384.02985873, -381.67005565, -379.81592466,\n        -378.79655394, -380.26541096, -384.54355736, -392.41759418,\n        -401.11033818, -410.67797517, -420.13324058, -430.6640625 ,\n        -442.34267979, -452.71297089, -461.42979452, -469.67305223,\n        -478.64672517, -486.40036387, -493.83294092, -498.87360873,\n        -504.82127568, -509.58101455, -511.55554366, -514.19627568,\n        -515.77750428, -516.41160103, -517.3828125 , -518.16941353,\n        -517.26241438, -518.46639555, -517.76808647, -517.3828125 ,\n        -516.61226455, -514.32470034, -512.5187286 , -511.66791524,\n        -510.65657106, -507.74293664, -504.84535531, -503.02333048,\n        -501.93172089, -499.3552012 , -498.77729024, -495.78339041,\n        -495.42219606, -495.78339041, -494.30650685, -492.30789812,\n        -491.83433219, -492.14736729, -490.16481164, -489.69124572,\n        -488.84043236, -488.5354238 , -489.24175942, -487.68461045,\n        -486.48062928, -485.50941781, -484.23319777, -484.24925086,\n        -484.26530394, -483.3984375 ])}</pre> In\u00a0[41]: Copied! <pre>sampling_rate = (ephys.EphysRecording &amp; insert_key).fetch1(\n    \"sampling_rate\"\n) / 1000  # in kHz\nplt.plot(\n    np.r_[: unit_data[\"peak_electrode_waveform\"].size] * 1 / sampling_rate,\n    unit_data[\"peak_electrode_waveform\"],\n)\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(r\"Voltage ($\\mu$V)\");\n</pre> sampling_rate = (ephys.EphysRecording &amp; insert_key).fetch1(     \"sampling_rate\" ) / 1000  # in kHz plt.plot(     np.r_[: unit_data[\"peak_electrode_waveform\"].size] * 1 / sampling_rate,     unit_data[\"peak_electrode_waveform\"], ) plt.xlabel(\"Time (ms)\") plt.ylabel(r\"Voltage ($\\mu$V)\"); <p>To run this tutorial notebook on your own data, please use the following steps:</p> <ul> <li>Download the mysql-docker image for DataJoint and run the container according to the instructions provide in the repository.</li> <li>Create a fork of this repository to your GitHub account.</li> <li>Clone the repository and open the files using your IDE.</li> <li>Add a code cell immediately after the first code cell in the notebook - we will setup the local connection using this cell. In this cell, type in the following code.</li> </ul> <pre>import datajoint as dj\ndj.config[\"database.host\"] = \"localhost\"\ndj.config[\"database.user\"] = \"&lt;your-username&gt;\"\ndj.config[\"database.password\"] = \"&lt;your-password&gt;\"\ndj.config[\"custom\"] = {\"imaging_root_data_dir\": \"path/to/your/data/dir\",\n\"database_prefix\": \"&lt;your-username_&gt;\"}\ndj.config.save_local()\ndj.conn()\n</pre> <ul> <li>Run this code cell and proceed with the rest of the notebook.</li> </ul>"}, {"location": "tutorials/tutorial/#process-neuropixels-electrophysiology-data-with-datajoint-elements", "title": "Process NeuroPixels Electrophysiology data with DataJoint Elements\u00b6", "text": "<p>This notebook will walk through processing NeuroPixels array electrophysiology data acquired with spikeGLX and processed with kilosort. While anyone can work through this notebook to process electrophysiology data through DataJoint's <code>element-array-ephys</code> pipeline, for a detailed tutorial about the fundamentals of DataJoint including table types, make functions, and querying, please see the DataJoint Tutorial.</p> <p>The DataJoint Python API and Element Array Electrophysiology offer a lot of features to support collaboration, automation, reproducibility, and visualizations.</p> <p>For more information on these topics, please visit our documentation:</p> <ul> <li><p>DataJoint Core: General principles</p> </li> <li><p>DataJoint Python and MATLAB APIs: in-depth reviews of specifics</p> </li> <li><p>DataJoint Element Array Ephys: A modular pipeline for electrophysiology analysis</p> </li> </ul> <p>Let's start by importing the packages necessary to run this tutorial.</p>"}, {"location": "tutorials/tutorial/#the-basics", "title": "The Basics:\u00b6", "text": "<p>Any DataJoint workflow can be broken down into basic 3 parts:</p> <ul> <li><code>Insert</code></li> <li><code>Populate</code> (or process)</li> <li><code>Query</code></li> </ul> <p>In this demo we will:</p> <ul> <li><code>Insert</code> metadata about an animal subject, recording session, and parameters related to processing electrophysiology data.</li> <li><code>Populate</code> tables with outputs of ephys recording data including LFPs, and spike sorted waveforms and units.</li> <li><code>Query</code> the processed data from the database and plot waveform traces.</li> </ul> <p>Each of these topics will be explained thoroughly in this notebook.</p>"}, {"location": "tutorials/tutorial/#workflow-diagram", "title": "Workflow diagram\u00b6", "text": "<p>This workflow is assembled from 4 DataJoint elements:</p> <ul> <li>element-lab</li> <li>element-animal</li> <li>element-session</li> <li>element-array-ephys</li> </ul> <p>Each element declares its own schema in the database. These schemas can be imported like any other Python package. This workflow is composed of schemas from each of the Elements above and correspond to a module within <code>workflow_array_ephys.pipeline</code>.</p> <p>The schema diagram is a good reference for understanding the order of the tables within the workflow, as well as the corresponding table type. Let's activate the elements and view the schema diagram.</p>"}, {"location": "tutorials/tutorial/#diagram-breakdown", "title": "Diagram Breakdown\u00b6", "text": "<p>While the diagram above seems complex at first, it becomes more clear when it's approached as a hierarchy of tables that define the order in which the workflow expects to receive data in each of its tables.</p> <ul> <li>Tables with a green, or rectangular shape expect to receive data manually using the <code>insert()</code> function. The tables higher up in the diagram such as <code>subject.Subject()</code> should be the first to receive data. This ensures data integrity by preventing orphaned data within DataJoint schemas.</li> <li>Tables with a purple oval or red circle can be automatically filled with relevant data by calling <code>populate()</code>. For example <code>ephys.EphysRecording</code> and its part-table <code>ephys.EphysRecording.EphysFile</code> are both populated with <code>ephys.EphysRecording.populate()</code>.</li> <li>Tables connected by a solid line depend on attributes (entries) in the table above it.</li> </ul>"}, {"location": "tutorials/tutorial/#table-types", "title": "Table Types\u00b6", "text": "<p>There are 5 table types in DataJoint. Each of these appear in the diagram above.</p> <ul> <li>Manual table: green box, manually inserted table, expect new entries daily, e.g. <code>Subject</code>, <code>ProbeInsertion</code>.</li> <li>Lookup table: gray box, pre inserted table, commonly used for general facts or parameters. e.g. <code>Strain</code>, <code>ClusteringMethod</code>, <code>ClusteringParamSet</code>.</li> <li>Imported table: blue oval, auto-processing table, the processing depends on the importing of external files. e.g. process of <code>Clustering</code> requires output files from kilosort2.</li> <li>Computed table: red circle, auto-processing table, the processing does not depend on files external to the database.</li> <li>Part table: plain text, as an appendix to the master table, all the part entries of a given master entry represent a intact set of the master entry. e.g. <code>Unit</code> of a <code>CuratedClustering</code>.</li> </ul>"}, {"location": "tutorials/tutorial/#starting-the-workflow-insert", "title": "Starting the workflow: Insert\u00b6", "text": ""}, {"location": "tutorials/tutorial/#insert-entries-into-manual-tables", "title": "Insert entries into manual tables\u00b6", "text": "<p>To view details about a table's dependencies and attributes, use functions <code>.describe()</code> and <code>.heading</code>, respectively.</p> <p>Let's start with the first table in the schema diagram (the <code>subject</code> table) and view the table attributes we need to insert. There are two ways you can do this: run each of the two cells below</p>"}, {"location": "tutorials/tutorial/#populate", "title": "Populate\u00b6", "text": ""}, {"location": "tutorials/tutorial/#automatically-populate-tables", "title": "Automatically populate tables\u00b6", "text": "<p><code>ephys.EphysRecording</code> is the first table in the pipeline that can be populated automatically. If a table contains a part table, this part table is also populated during the <code>populate()</code> call. <code>populate()</code> takes several arguments including the a session key. This key restricts <code>populate()</code> to performing the operation on the session of interest rather than all possible sessions which could be a time-intensive process for databases with lots of entries.</p> <p>Let's view the <code>ephys.EphysRecording</code> and its part table <code>ephys.EphysRecording.EphysFile</code> and populate both through a single <code>populate()</code> call.</p>"}, {"location": "tutorials/tutorial/#query", "title": "Query\u00b6", "text": "<p>This section focuses on working with data that is already in the database.</p> <p>DataJoint queries allow you to view and import data from the database into a python variable using the <code>fetch()</code> method.</p> <p>There are several important features supported by <code>fetch()</code>:</p> <ul> <li>By default, an empty <code>fetch()</code> imports a list of dictionaries containing all attributes of all entries in the table that is queried.</li> <li><code>fetch1()</code>, on the other hand, imports a dictionary containing all attributes of one of the entries in the table. By default, if a table has multiple entries, <code>fetch1()</code> imports the first entry in the table.</li> <li>Both <code>fetch()</code> and <code>fetch1()</code> accept table attributes as an argument to query that particular attribute. For example <code>fetch1('fps')</code> will fetch the first value of the <code>fps</code> attribute if it exists in the table.</li> <li>Recommended best practice is to restrict queries by primary key attributes of the table to ensure the accuracy of imported data.<ul> <li>The most common restriction for entries in DataJoint tables is performed using the <code>&amp;</code> operator. For example to fetch all session start times belonging to <code>subject1</code>, a possible query could be <code>subject1_sessions = (session.Session &amp; \"subject = 'subject1'\").fetch(\"session_datetime\")</code>.</li> </ul> </li> <li><code>fetch()</code> can also be used to obtain the primary keys of a table. To fetch the primary keys of a table use <code>&lt;table_name&gt;.fetch(\"KEY\")</code> syntax.</li> </ul> <p>Let's walk through these concepts of querying by moving from simple to more complex queries.</p>"}]}