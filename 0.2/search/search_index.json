{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "Element Array Electrophysiology \u00b6 This Element features DataJoint schemas for analyzing extracellular array electrophysiology data acquired with Neuropixels probes and spike sorted using Kilosort spike sorter. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. The Element is comprised of probe and ephys schemas. Several ephys schemas are developed to handle various use cases of this pipeline and workflow: ephys_acute : A probe is inserted into a new location during each session. ephys_chronic : A probe is inserted once and used to record across multiple sessions. ephys_precluster : A probe is inserted into a new location during each session. Pre-clustering steps are performed on the data from each probe prior to Kilosort analysis. ephys_no_curation : A probe is inserted into a new location during each session and Kilosort-triggered clustering is performed without the option to manually curate the results. Visit the Concepts page for more information about the use cases of ephys schemas and an explanation of the tables. To get started with building your own data pipeline, visit the Tutorials page .", "title": "Element Array Ephys"}, {"location": "#element-array-electrophysiology", "text": "This Element features DataJoint schemas for analyzing extracellular array electrophysiology data acquired with Neuropixels probes and spike sorted using Kilosort spike sorter. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. The Element is comprised of probe and ephys schemas. Several ephys schemas are developed to handle various use cases of this pipeline and workflow: ephys_acute : A probe is inserted into a new location during each session. ephys_chronic : A probe is inserted once and used to record across multiple sessions. ephys_precluster : A probe is inserted into a new location during each session. Pre-clustering steps are performed on the data from each probe prior to Kilosort analysis. ephys_no_curation : A probe is inserted into a new location during each session and Kilosort-triggered clustering is performed without the option to manually curate the results. Visit the Concepts page for more information about the use cases of ephys schemas and an explanation of the tables. To get started with building your own data pipeline, visit the Tutorials page .", "title": "Element Array Electrophysiology"}, {"location": "changelog/", "text": "../../CHANGELOG.md", "title": "Changelog"}, {"location": "citation/", "text": "Citation \u00b6 If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID): Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element Array Electrophysiology (version 0.2.0)", "title": "Citation"}, {"location": "citation/#citation", "text": "If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID): Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element Array Electrophysiology (version 0.2.0)", "title": "Citation"}, {"location": "concepts/", "text": "Concepts \u00b6 Acquisition Tools for Electrophysiology \u00b6 Neuropixels Probes \u00b6 Neuropixels probes were developed by a collaboration between HHMI Janelia, industry partners, and others 1 . Since their initial release in October 2018, 300 labs have ordered 1200 probes. Since the rollout of Neuropixels 2.0 in October 2020, IMEC has been shipping 100+ probes monthly (correspondence with Tim Harris). Neuropixels probes offer 960 electrode sites along a 10mm long shank, with 384 recordable channels per probe that can record hundreds of units spanning multiple brain regions (Neuropixels 2.0 version is a 4-shank probe with 1280 electrode sites per shank). Such large recording capacity has offered tremendous opportunities for the field of neurophysiology research, yet this is accompanied by an equally great challenge in terms of data and computation management. Data Acquisition Tools \u00b6 Some commonly used acquisiton tools and systems by the neuroscience research community include: Neuropixels probes Tetrodes SpikeGLX OpenEphys Neuralynx Axona ... Data Preprocessing Tools \u00b6 The preprocessing pipeline includes bandpass filtering for LFP extraction, bandpass filtering for spike sorting, spike sorting, manual curation of the spike sorting results, and calculation of quality control metrics. In trial-based experiments, the spike trains are aligned and separated into trials. Standard processing may include PSTH computation aligned to trial onset or other events, and often grouped by different trial types. Neuroscience groups have traditionally developed custom home-made toolchains. In recent years, several leaders have been emerging as de facto standards with significant community uptake: Kilosort pyKilosort JRClust KlustaKwik Mountainsort spikeinterface (wrapper) spyking-circus spikeforest ... Kilosort provides most automation and has gained significant popularity, being adopted as one of the key spike sorting methods in the majority of the teams/collaborations we have worked with. As part of our Year-1 NIH U24 effort, we provide support for data ingestion of spike sorting results from Kilosort. Further effort will be devoted for the ingestion support of other spike sorting methods. On this end, a framework for unifying existing spike sorting methods, named SpikeInterface , has been developed by Alessio Buccino, et al. SpikeInterface provides a convenient Python-based wrapper to invoke, extract, compare spike sorting results from different sorting algorithms. Key Partnerships \u00b6 Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for Neuropixels probes. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experimental workflow, pipeline design, associated tools, and interfaces. These teams include: International Brain Lab - https://github.com/int-brain-lab/IBL-pipeline Mesoscale Activity Project (HHMI Janelia) - https://github.com/mesoscale-activity-map/map-ephys Moser Group (Norwegian University of Science and Technology) - see pipeline design Andreas Tolias Lab (Baylor College of Medicine) BrainCoGs (Princeton Neuroscience Institute) Brody Lab (Princeton University) Element Architecture \u00b6 Each of the DataJoint Elements creates a set of tables for common neuroscience data modalities to organize, preprocess, and analyze data. Each node in the following diagram is a table within the Element or a table connected to the Element. ephys_acute module \u00b6 ephys_chronic module \u00b6 ephys_precluster module \u00b6 subject schema ( API docs ) \u00b6 Although not required, most choose to connect the Session table to a Subject table. Table Description Subject A table containing basic information of the research subject. session schema ( API docs ) \u00b6 Table Description Session A table for unique experimental session identifiers. probe schema ( API docs ) \u00b6 Tables related to the Neuropixels probe and electrode configuration. Table Description ProbeType A lookup table specifying the type of Neuropixels probe (e.g. \"neuropixels 1.0\", \"neuropixels 2.0 single-shank\"). ProbeType.Electrode A table containing electrodes and their properties for a particular probe type. Probe A record of an actual physical probe. ElectrodeConfig A record of a particular electrode configuration to be used for ephys recording. ElectrodeConfig.Electrode A record of electrodes out of those in ProbeType.Electrode that are used for recording. ephys schema ( API docs ) \u00b6 Tables related to information about physiological recordings and automatic ingestion of spike sorting results. Table Description ProbeInsertion A record of surgical insertions of a probe in the brain. EphysRecording A table with metadata about each electrophysiogical recording. Clustering A table with clustering data for spike sorting extracellular electrophysiology data. Curation A table to declare optional manual curation of spike sorting results. CuratedClustering A table with metadata for sorted data generated after each curation. CuratedClusting.Unit A part table containing single unit information after spike sorting and optional curation. WaveformSet A table containing spike waveforms for single units. Element Development \u00b6 Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Array Electrophysiology Element . Major features of the Array Electrophysiology Element include: Pipeline architecture detailed by: + Probe, electrode configuration compatible with Neuropixels probes and generalizable to other types of probes (e.g. tetrodes) - supporting both chronic and acute probe insertion modes. + Probe-insertion, ephys-recordings, LFP extraction, clusterings, curations, sorted units and the associated data (e.g. spikes, waveforms, etc.). + Store/track/manage different curations of the spike sorting results - supporting both curated clustering and kilosort triggered clustering (i.e., no_curation ). Ingestion support for data acquired with SpikeGLX and OpenEphys acquisition systems. Ingestion support for spike sorting outputs from Kilosort. Triggering support for workflow integrated Kilosort processing. Sample data and complete test suite for quality assurance. Data Export and Publishing \u00b6 Element Array Electrophysiology supports exporting of all data into standard Neurodata Without Borders (NWB) files. This makes it easy to share files with collaborators and publish results on DANDI Archive . NWB , as an organization, is dedicated to standardizing data formats and maximizing interoperability across tools for neurophysiology. For more information on uploading NWB files to DANDI within the DataJoint Elements ecosystem, visit our documentation for the DANDI upload feature of Element Array Electrophysiology . Roadmap \u00b6 Incorporation of SpikeInterface into the Array Electrophysiology Element will be on DataJoint Elements development roadmap. Dr. Loren Frank has led a development effort of a DataJoint pipeline with SpikeInterface framework and NeurodataWithoutBorders format integrated https://github.com/LorenFrankLab/nwb_datajoint . Future additions to this element will add functionality to support large (> 48 hours) neuropixel recordings via an overlapping segmented processing approach. Further development of this Element is community driven. Upon user requests we will continue adding features to this Element. References \u00b6 [1]: Jun, J., Steinmetz, N., Siegle, J. et al. Fully integrated silicon probes for high-density recording of neural activity. Nature 551, 232\u2013236 (2017). https://doi.org/10.1038/nature24636 .", "title": "Concepts"}, {"location": "concepts/#concepts", "text": "", "title": "Concepts"}, {"location": "concepts/#acquisition-tools-for-electrophysiology", "text": "", "title": "Acquisition Tools for Electrophysiology"}, {"location": "concepts/#neuropixels-probes", "text": "Neuropixels probes were developed by a collaboration between HHMI Janelia, industry partners, and others 1 . Since their initial release in October 2018, 300 labs have ordered 1200 probes. Since the rollout of Neuropixels 2.0 in October 2020, IMEC has been shipping 100+ probes monthly (correspondence with Tim Harris). Neuropixels probes offer 960 electrode sites along a 10mm long shank, with 384 recordable channels per probe that can record hundreds of units spanning multiple brain regions (Neuropixels 2.0 version is a 4-shank probe with 1280 electrode sites per shank). Such large recording capacity has offered tremendous opportunities for the field of neurophysiology research, yet this is accompanied by an equally great challenge in terms of data and computation management.", "title": "Neuropixels Probes"}, {"location": "concepts/#data-acquisition-tools", "text": "Some commonly used acquisiton tools and systems by the neuroscience research community include: Neuropixels probes Tetrodes SpikeGLX OpenEphys Neuralynx Axona ...", "title": "Data Acquisition Tools"}, {"location": "concepts/#data-preprocessing-tools", "text": "The preprocessing pipeline includes bandpass filtering for LFP extraction, bandpass filtering for spike sorting, spike sorting, manual curation of the spike sorting results, and calculation of quality control metrics. In trial-based experiments, the spike trains are aligned and separated into trials. Standard processing may include PSTH computation aligned to trial onset or other events, and often grouped by different trial types. Neuroscience groups have traditionally developed custom home-made toolchains. In recent years, several leaders have been emerging as de facto standards with significant community uptake: Kilosort pyKilosort JRClust KlustaKwik Mountainsort spikeinterface (wrapper) spyking-circus spikeforest ... Kilosort provides most automation and has gained significant popularity, being adopted as one of the key spike sorting methods in the majority of the teams/collaborations we have worked with. As part of our Year-1 NIH U24 effort, we provide support for data ingestion of spike sorting results from Kilosort. Further effort will be devoted for the ingestion support of other spike sorting methods. On this end, a framework for unifying existing spike sorting methods, named SpikeInterface , has been developed by Alessio Buccino, et al. SpikeInterface provides a convenient Python-based wrapper to invoke, extract, compare spike sorting results from different sorting algorithms.", "title": "Data Preprocessing Tools"}, {"location": "concepts/#key-partnerships", "text": "Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for Neuropixels probes. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experimental workflow, pipeline design, associated tools, and interfaces. These teams include: International Brain Lab - https://github.com/int-brain-lab/IBL-pipeline Mesoscale Activity Project (HHMI Janelia) - https://github.com/mesoscale-activity-map/map-ephys Moser Group (Norwegian University of Science and Technology) - see pipeline design Andreas Tolias Lab (Baylor College of Medicine) BrainCoGs (Princeton Neuroscience Institute) Brody Lab (Princeton University)", "title": "Key Partnerships"}, {"location": "concepts/#element-architecture", "text": "Each of the DataJoint Elements creates a set of tables for common neuroscience data modalities to organize, preprocess, and analyze data. Each node in the following diagram is a table within the Element or a table connected to the Element.", "title": "Element Architecture"}, {"location": "concepts/#ephys_acute-module", "text": "", "title": "ephys_acute module"}, {"location": "concepts/#ephys_chronic-module", "text": "", "title": "ephys_chronic module"}, {"location": "concepts/#ephys_precluster-module", "text": "", "title": "ephys_precluster module"}, {"location": "concepts/#subject-schema-api-docs", "text": "Although not required, most choose to connect the Session table to a Subject table. Table Description Subject A table containing basic information of the research subject.", "title": "subject schema (API docs)"}, {"location": "concepts/#session-schema-api-docs", "text": "Table Description Session A table for unique experimental session identifiers.", "title": "session schema (API docs)"}, {"location": "concepts/#probe-schema-api-docs", "text": "Tables related to the Neuropixels probe and electrode configuration. Table Description ProbeType A lookup table specifying the type of Neuropixels probe (e.g. \"neuropixels 1.0\", \"neuropixels 2.0 single-shank\"). ProbeType.Electrode A table containing electrodes and their properties for a particular probe type. Probe A record of an actual physical probe. ElectrodeConfig A record of a particular electrode configuration to be used for ephys recording. ElectrodeConfig.Electrode A record of electrodes out of those in ProbeType.Electrode that are used for recording.", "title": "probe schema (API docs)"}, {"location": "concepts/#ephys-schema-api-docs", "text": "Tables related to information about physiological recordings and automatic ingestion of spike sorting results. Table Description ProbeInsertion A record of surgical insertions of a probe in the brain. EphysRecording A table with metadata about each electrophysiogical recording. Clustering A table with clustering data for spike sorting extracellular electrophysiology data. Curation A table to declare optional manual curation of spike sorting results. CuratedClustering A table with metadata for sorted data generated after each curation. CuratedClusting.Unit A part table containing single unit information after spike sorting and optional curation. WaveformSet A table containing spike waveforms for single units.", "title": "ephys schema (API docs)"}, {"location": "concepts/#element-development", "text": "Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Array Electrophysiology Element . Major features of the Array Electrophysiology Element include: Pipeline architecture detailed by: + Probe, electrode configuration compatible with Neuropixels probes and generalizable to other types of probes (e.g. tetrodes) - supporting both chronic and acute probe insertion modes. + Probe-insertion, ephys-recordings, LFP extraction, clusterings, curations, sorted units and the associated data (e.g. spikes, waveforms, etc.). + Store/track/manage different curations of the spike sorting results - supporting both curated clustering and kilosort triggered clustering (i.e., no_curation ). Ingestion support for data acquired with SpikeGLX and OpenEphys acquisition systems. Ingestion support for spike sorting outputs from Kilosort. Triggering support for workflow integrated Kilosort processing. Sample data and complete test suite for quality assurance.", "title": "Element Development"}, {"location": "concepts/#data-export-and-publishing", "text": "Element Array Electrophysiology supports exporting of all data into standard Neurodata Without Borders (NWB) files. This makes it easy to share files with collaborators and publish results on DANDI Archive . NWB , as an organization, is dedicated to standardizing data formats and maximizing interoperability across tools for neurophysiology. For more information on uploading NWB files to DANDI within the DataJoint Elements ecosystem, visit our documentation for the DANDI upload feature of Element Array Electrophysiology .", "title": "Data Export and Publishing"}, {"location": "concepts/#roadmap", "text": "Incorporation of SpikeInterface into the Array Electrophysiology Element will be on DataJoint Elements development roadmap. Dr. Loren Frank has led a development effort of a DataJoint pipeline with SpikeInterface framework and NeurodataWithoutBorders format integrated https://github.com/LorenFrankLab/nwb_datajoint . Future additions to this element will add functionality to support large (> 48 hours) neuropixel recordings via an overlapping segmented processing approach. Further development of this Element is community driven. Upon user requests we will continue adding features to this Element.", "title": "Roadmap"}, {"location": "concepts/#references", "text": "[1]: Jun, J., Steinmetz, N., Siegle, J. et al. Fully integrated silicon probes for high-density recording of neural activity. Nature 551, 232\u2013236 (2017). https://doi.org/10.1038/nature24636 .", "title": "References"}, {"location": "tutorials/", "text": "Tutorials \u00b6 Coming soon!", "title": "Tutorials"}, {"location": "tutorials/#tutorials", "text": "Coming soon!", "title": "Tutorials"}, {"location": "api/element_array_ephys/ephys_acute/", "text": "AcquisitionSoftware \u00b6 Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_acute.py 121 122 123 124 125 126 127 128 129 130 131 132 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Software used for recording of neuropixels probes acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ]) ClusterQualityLabel \u00b6 Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_acute.py 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) # cluster quality type - e.g. 'good', 'MUA', 'noise', etc. --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ] Clustering \u00b6 Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_acute.py 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) make ( key ) \u00b6 Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_acute.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) ClusteringMethod \u00b6 Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_acute.py 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort2\" , \"kilosort2 clustering method\" ), ( \"kilosort2.5\" , \"kilosort2.5 clustering method\" ), ( \"kilosort3\" , \"kilosort3 clustering method\" ), ] ClusteringParamSet \u00b6 Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_acute.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict ) insert_new_params ( clustering_method , paramset_desc , params , paramset_idx = None ) classmethod \u00b6 Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. None Source code in element_array_ephys/ephys_acute.py 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict ) ClusteringTask \u00b6 Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_acute.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> ClusteringParamSet --- clustering_output_dir='': varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } ) auto_generate_entries ( ephys_recording_key , paramset_idx = 0 ) classmethod \u00b6 Autogenerate entries based on a particular ephys recording. Parameters: Name Type Description Default ephys_recording_key dict EphysRecording primary key. required paramset_idx int Parameter index to use for clustering task. Defaults to 0. 0 Source code in element_array_ephys/ephys_acute.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Infer output directory if it is not provided. Parameters: Name Type Description Default key dict ClusteringTask primary key. required Returns: Type Description Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 Source code in element_array_ephys/ephys_acute.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir CuratedClustering \u00b6 Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_acute.py 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of a curation. -> Curation \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Unit \u00b6 Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_acute.py 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" make ( key ) \u00b6 Automated population of Unit information. Source code in element_array_ephys/ephys_acute.py 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Curation \u00b6 Bases: dj . Manual Curation procedure table. Attributes: Name Type Description Clustering foreign key Clustering primary key. curation_id foreign key, int Unique curation ID. curation_time datetime Time when curation results are generated. curation_output_dir varchar (255) Output directory of the curated results. quality_control bool If True, this clustering result has undergone quality control. manual_curation bool If True, manual curation has been performed on this clustering result. curation_note varchar (2000) Notes about the curation task. Source code in element_array_ephys/ephys_acute.py 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 @schema class Curation ( dj . Manual ): \"\"\"Curation procedure table. Attributes: Clustering (foreign key): Clustering primary key. curation_id (foreign key, int): Unique curation ID. curation_time (datetime): Time when curation results are generated. curation_output_dir (varchar(255) ): Output directory of the curated results. quality_control (bool): If True, this clustering result has undergone quality control. manual_curation (bool): If True, manual curation has been performed on this clustering result. curation_note (varchar(2000) ): Notes about the curation task. \"\"\" definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) create1_from_clustering_task ( key , curation_note = '' ) \u00b6 A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_acute.py 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 def create1_from_clustering_task ( self , key , curation_note = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) EphysRecording \u00b6 Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_acute.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) EphysFile \u00b6 Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_acute.py 279 280 281 282 283 284 285 286 287 288 289 290 291 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" make ( key ) \u00b6 Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_acute.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) InsertionLocation \u00b6 Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_acute.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\" LFP \u00b6 Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_acute.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> EphysRecording --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) Electrode \u00b6 Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_acute.py 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" make ( key ) \u00b6 Populates the LFP tables. Source code in element_array_ephys/ephys_acute.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) ProbeInsertion \u00b6 Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_acute.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion implanted into an animal for a given session. -> Session insertion_number: tinyint unsigned --- -> probe.Probe \"\"\" @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list , skip_duplicates = True ) cls . insert ( probe_insertion_list , skip_duplicates = True ) auto_generate_entries ( session_key ) classmethod \u00b6 Automatically populate entries in ProbeInsertion table for a session. Source code in element_array_ephys/ephys_acute.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list , skip_duplicates = True ) cls . insert ( probe_insertion_list , skip_duplicates = True ) QualityMetrics \u00b6 Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_acute.py 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) Cluster \u00b6 Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_acute.py 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" Waveform \u00b6 Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_acute.py 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" make ( key ) \u00b6 Populates tables with quality metrics data. Source code in element_array_ephys/ephys_acute.py 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) WaveformSet \u00b6 Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_acute.py 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) PeakWaveform \u00b6 Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_acute.py 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" Waveform \u00b6 Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_acute.py 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" make ( key ) \u00b6 Populates waveform tables. Source code in element_array_ephys/ephys_acute.py 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. Source code in element_array_ephys/ephys_acute.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None , ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_acute.py 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_acute.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" root_directories = _linking_module . get_ephys_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories get_neuropixels_channel2electrode_map ( ephys_recording_key , acq_software ) \u00b6 Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_acute.py 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : probe_dataset = get_openephys_probe_data ( ephys_recording_key ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_indices\" ] } return channel2electrode_map get_openephys_probe_data ( ephys_recording_key ) \u00b6 Get OpenEphys probe data from file. Source code in element_array_ephys/ephys_acute.py 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 def get_openephys_probe_data ( ephys_recording_key : dict ) -> list : \"\"\"Get OpenEphys probe data from file. \"\"\" inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) probe_data = loaded_oe . probes [ inserted_probe_serial_number ] # explicitly garbage collect \"loaded_oe\" # as these may have large memory footprint and may not be cleared fast enough del loaded_oe gc . collect () return probe_data get_processed_root_data_dir () \u00b6 Retrieve the root directory for all processed data. Returns: Type Description str A string for the full path to the root directory for processed data. Source code in element_array_ephys/ephys_acute.py 105 106 107 108 109 110 111 112 113 114 115 def get_processed_root_data_dir () -> str : \"\"\"Retrieve the root directory for all processed data. Returns: A string for the full path to the root directory for processed data. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_ephys_root_data_dir ()[ 0 ] get_recording_channels_details ( ephys_recording_key ) \u00b6 Get details of recording channels for a given recording. Source code in element_array_ephys/ephys_acute.py 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 def get_recording_channels_details ( ephys_recording_key : dict ) -> np . array : \"\"\"Get details of recording channels for a given recording. \"\"\" channels_details = {} acq_software , sample_rate = ( EphysRecording & ephys_recording_key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) probe_type = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe_type\" ) channels_details [ \"probe_type\" ] = { \"neuropixels 1.0 - 3A\" : \"3A\" , \"neuropixels 1.0 - 3B\" : \"NP1\" , \"neuropixels UHD\" : \"NP1100\" , \"neuropixels 2.0 - SS\" : \"NP21\" , \"neuropixels 2.0 - MS\" : \"NP24\" , }[ probe_type ] electrode_config_key = ( probe . ElectrodeConfig * EphysRecording & ephys_recording_key ) . fetch1 ( \"KEY\" ) ( channels_details [ \"channel_ind\" ], channels_details [ \"x_coords\" ], channels_details [ \"y_coords\" ], channels_details [ \"shank_ind\" ], ) = ( probe . ElectrodeConfig . Electrode * probe . ProbeType . Electrode & electrode_config_key ) . fetch ( \"electrode\" , \"x_coord\" , \"y_coord\" , \"shank\" ) channels_details [ \"sample_rate\" ] = sample_rate channels_details [ \"num_channels\" ] = len ( channels_details [ \"channel_ind\" ]) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) channels_details [ \"uVPerBit\" ] = spikeglx_recording . get_channel_bit_volts ( \"ap\" )[ 0 ] channels_details [ \"connected\" ] = np . array ( [ v for * _ , v in spikeglx_recording . apmeta . shankmap [ \"data\" ]] ) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( ephys_recording_key ) channels_details [ \"uVPerBit\" ] = oe_probe . ap_meta [ \"channels_gains\" ][ 0 ] channels_details [ \"connected\" ] = np . array ( [ int ( v == 1 ) for c , v in oe_probe . channels_connected . items () if c in channels_details [ \"channel_ind\" ] ] ) return channels_details get_session_directory ( session_key ) \u00b6 Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_acute.py 93 94 95 96 97 98 99 100 101 102 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key ) get_spikeglx_meta_filepath ( ephys_recording_key ) \u00b6 Get spikeGLX data filepath. Source code in element_array_ephys/ephys_acute.py 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = pathlib . Path ( ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "ephys_acute.py"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.AcquisitionSoftware", "text": "Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_acute.py 121 122 123 124 125 126 127 128 129 130 131 132 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Software used for recording of neuropixels probes acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ])", "title": "AcquisitionSoftware"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusterQualityLabel", "text": "Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_acute.py 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) # cluster quality type - e.g. 'good', 'MUA', 'noise', etc. --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ]", "title": "ClusterQualityLabel"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering", "text": "Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_acute.py 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering.make", "text": "Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_acute.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringMethod", "text": "Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_acute.py 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort2\" , \"kilosort2 clustering method\" ), ( \"kilosort2.5\" , \"kilosort2.5 clustering method\" ), ( \"kilosort3\" , \"kilosort3 clustering method\" ), ]", "title": "ClusteringMethod"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringParamSet", "text": "Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_acute.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict )", "title": "ClusteringParamSet"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringParamSet.insert_new_params", "text": "Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. None Source code in element_array_ephys/ephys_acute.py 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask", "text": "Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_acute.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> ClusteringParamSet --- clustering_output_dir='': varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } )", "title": "ClusteringTask"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask.auto_generate_entries", "text": "Autogenerate entries based on a particular ephys recording. Parameters: Name Type Description Default ephys_recording_key dict EphysRecording primary key. required paramset_idx int Parameter index to use for clustering task. Defaults to 0. 0 Source code in element_array_ephys/ephys_acute.py 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } )", "title": "auto_generate_entries()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask.infer_output_dir", "text": "Infer output directory if it is not provided. Parameters: Name Type Description Default key dict ClusteringTask primary key. required Returns: Type Description Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 Source code in element_array_ephys/ephys_acute.py 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 @classmethod def infer_output_dir ( cls , key : dict , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering", "text": "Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_acute.py 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of a curation. -> Curation \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "CuratedClustering"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering.Unit", "text": "Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_acute.py 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\"", "title": "Unit"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering.make", "text": "Automated population of Unit information. Source code in element_array_ephys/ephys_acute.py 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "make()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation", "text": "Bases: dj . Manual Curation procedure table. Attributes: Name Type Description Clustering foreign key Clustering primary key. curation_id foreign key, int Unique curation ID. curation_time datetime Time when curation results are generated. curation_output_dir varchar (255) Output directory of the curated results. quality_control bool If True, this clustering result has undergone quality control. manual_curation bool If True, manual curation has been performed on this clustering result. curation_note varchar (2000) Notes about the curation task. Source code in element_array_ephys/ephys_acute.py 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 @schema class Curation ( dj . Manual ): \"\"\"Curation procedure table. Attributes: Clustering (foreign key): Clustering primary key. curation_id (foreign key, int): Unique curation ID. curation_time (datetime): Time when curation results are generated. curation_output_dir (varchar(255) ): Output directory of the curated results. quality_control (bool): If True, this clustering result has undergone quality control. manual_curation (bool): If True, manual curation has been performed on this clustering result. curation_note (varchar(2000) ): Notes about the curation task. \"\"\" definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "Curation"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation.create1_from_clustering_task", "text": "A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_acute.py 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 def create1_from_clustering_task ( self , key , curation_note = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "create1_from_clustering_task()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording", "text": "Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_acute.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "EphysRecording"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording.EphysFile", "text": "Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_acute.py 279 280 281 282 283 284 285 286 287 288 289 290 291 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\"", "title": "EphysFile"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording.make", "text": "Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_acute.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.InsertionLocation", "text": "Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_acute.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\"", "title": "InsertionLocation"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP", "text": "Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_acute.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> EphysRecording --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "LFP"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP.Electrode", "text": "Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_acute.py 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\"", "title": "Electrode"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP.make", "text": "Populates the LFP tables. Source code in element_array_ephys/ephys_acute.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ProbeInsertion", "text": "Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_acute.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion implanted into an animal for a given session. -> Session insertion_number: tinyint unsigned --- -> probe.Probe \"\"\" @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list , skip_duplicates = True ) cls . insert ( probe_insertion_list , skip_duplicates = True )", "title": "ProbeInsertion"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ProbeInsertion.auto_generate_entries", "text": "Automatically populate entries in ProbeInsertion table for a session. Source code in element_array_ephys/ephys_acute.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list , skip_duplicates = True ) cls . insert ( probe_insertion_list , skip_duplicates = True )", "title": "auto_generate_entries()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics", "text": "Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_acute.py 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "QualityMetrics"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.Cluster", "text": "Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_acute.py 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\"", "title": "Cluster"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.Waveform", "text": "Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_acute.py 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.make", "text": "Populates tables with quality metrics data. Source code in element_array_ephys/ephys_acute.py 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet", "text": "Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_acute.py 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "WaveformSet"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.PeakWaveform", "text": "Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_acute.py 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\"", "title": "PeakWaveform"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.Waveform", "text": "Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_acute.py 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.make", "text": "Populates waveform tables. Source code in element_array_ephys/ephys_acute.py 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.activate", "text": "Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. Source code in element_array_ephys/ephys_acute.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None , ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_acute.py 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_ephys_root_data_dir", "text": "Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_acute.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" root_directories = _linking_module . get_ephys_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_neuropixels_channel2electrode_map", "text": "Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_acute.py 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : probe_dataset = get_openephys_probe_data ( ephys_recording_key ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_indices\" ] } return channel2electrode_map", "title": "get_neuropixels_channel2electrode_map()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_openephys_probe_data", "text": "Get OpenEphys probe data from file. Source code in element_array_ephys/ephys_acute.py 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 def get_openephys_probe_data ( ephys_recording_key : dict ) -> list : \"\"\"Get OpenEphys probe data from file. \"\"\" inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) probe_data = loaded_oe . probes [ inserted_probe_serial_number ] # explicitly garbage collect \"loaded_oe\" # as these may have large memory footprint and may not be cleared fast enough del loaded_oe gc . collect () return probe_data", "title": "get_openephys_probe_data()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_processed_root_data_dir", "text": "Retrieve the root directory for all processed data. Returns: Type Description str A string for the full path to the root directory for processed data. Source code in element_array_ephys/ephys_acute.py 105 106 107 108 109 110 111 112 113 114 115 def get_processed_root_data_dir () -> str : \"\"\"Retrieve the root directory for all processed data. Returns: A string for the full path to the root directory for processed data. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_ephys_root_data_dir ()[ 0 ]", "title": "get_processed_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_recording_channels_details", "text": "Get details of recording channels for a given recording. Source code in element_array_ephys/ephys_acute.py 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 def get_recording_channels_details ( ephys_recording_key : dict ) -> np . array : \"\"\"Get details of recording channels for a given recording. \"\"\" channels_details = {} acq_software , sample_rate = ( EphysRecording & ephys_recording_key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) probe_type = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe_type\" ) channels_details [ \"probe_type\" ] = { \"neuropixels 1.0 - 3A\" : \"3A\" , \"neuropixels 1.0 - 3B\" : \"NP1\" , \"neuropixels UHD\" : \"NP1100\" , \"neuropixels 2.0 - SS\" : \"NP21\" , \"neuropixels 2.0 - MS\" : \"NP24\" , }[ probe_type ] electrode_config_key = ( probe . ElectrodeConfig * EphysRecording & ephys_recording_key ) . fetch1 ( \"KEY\" ) ( channels_details [ \"channel_ind\" ], channels_details [ \"x_coords\" ], channels_details [ \"y_coords\" ], channels_details [ \"shank_ind\" ], ) = ( probe . ElectrodeConfig . Electrode * probe . ProbeType . Electrode & electrode_config_key ) . fetch ( \"electrode\" , \"x_coord\" , \"y_coord\" , \"shank\" ) channels_details [ \"sample_rate\" ] = sample_rate channels_details [ \"num_channels\" ] = len ( channels_details [ \"channel_ind\" ]) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) channels_details [ \"uVPerBit\" ] = spikeglx_recording . get_channel_bit_volts ( \"ap\" )[ 0 ] channels_details [ \"connected\" ] = np . array ( [ v for * _ , v in spikeglx_recording . apmeta . shankmap [ \"data\" ]] ) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( ephys_recording_key ) channels_details [ \"uVPerBit\" ] = oe_probe . ap_meta [ \"channels_gains\" ][ 0 ] channels_details [ \"connected\" ] = np . array ( [ int ( v == 1 ) for c , v in oe_probe . channels_connected . items () if c in channels_details [ \"channel_ind\" ] ] ) return channels_details", "title": "get_recording_channels_details()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_session_directory", "text": "Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_acute.py 93 94 95 96 97 98 99 100 101 102 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_spikeglx_meta_filepath", "text": "Get spikeGLX data filepath. Source code in element_array_ephys/ephys_acute.py 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = pathlib . Path ( ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "get_spikeglx_meta_filepath()"}, {"location": "api/element_array_ephys/ephys_chronic/", "text": "AcquisitionSoftware \u00b6 Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_chronic.py 121 122 123 124 125 126 127 128 129 130 131 132 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Software used for recording of neuropixels probes acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ]) ClusterQualityLabel \u00b6 Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_chronic.py 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) # cluster quality type - e.g. 'good', 'MUA', 'noise', etc. --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ] Clustering \u00b6 Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_chronic.py 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) make ( key ) \u00b6 Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_chronic.py 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) ClusteringMethod \u00b6 Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_chronic.py 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort2\" , \"kilosort2 clustering method\" ), ( \"kilosort2.5\" , \"kilosort2.5 clustering method\" ), ( \"kilosort3\" , \"kilosort3 clustering method\" ), ] ClusteringParamSet \u00b6 Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_chronic.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict ) insert_new_params ( clustering_method , paramset_desc , params , paramset_idx = None ) classmethod \u00b6 Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. None Source code in element_array_ephys/ephys_chronic.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict ) ClusteringTask \u00b6 Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_chronic.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> ClusteringParamSet --- clustering_output_dir='': varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) sess_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), sess_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / sess_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } ) auto_generate_entries ( ephys_recording_key , paramset_idx = 0 ) classmethod \u00b6 Autogenerate entries based on a particular ephys recording. Parameters: Name Type Description Default ephys_recording_key dict EphysRecording primary key. required paramset_idx int Parameter index to use for clustering task. Defaults to 0. 0 Source code in element_array_ephys/ephys_chronic.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Infer output directory if it is not provided. Parameters: Name Type Description Default key dict ClusteringTask primary key. required Returns: Type Description Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 Source code in element_array_ephys/ephys_chronic.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) sess_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), sess_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / sess_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir CuratedClustering \u00b6 Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_chronic.py 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of a curation. -> Curation \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Unit \u00b6 Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_chronic.py 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" make ( key ) \u00b6 Automated population of Unit information. Source code in element_array_ephys/ephys_chronic.py 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Curation \u00b6 Bases: dj . Manual Curation procedure table. Attributes: Name Type Description Clustering foreign key Clustering primary key. curation_id foreign key, int Unique curation ID. curation_time datetime Time when curation results are generated. curation_output_dir varchar (255) Output directory of the curated results. quality_control bool If True, this clustering result has undergone quality control. manual_curation bool If True, manual curation has been performed on this clustering result. curation_note varchar (2000) Notes about the curation task. Source code in element_array_ephys/ephys_chronic.py 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 @schema class Curation ( dj . Manual ): \"\"\"Curation procedure table. Attributes: Clustering (foreign key): Clustering primary key. curation_id (foreign key, int): Unique curation ID. curation_time (datetime): Time when curation results are generated. curation_output_dir (varchar(255) ): Output directory of the curated results. quality_control (bool): If True, this clustering result has undergone quality control. manual_curation (bool): If True, manual curation has been performed on this clustering result. curation_note (varchar(2000) ): Notes about the curation task. \"\"\" definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) create1_from_clustering_task ( key , curation_note = '' ) \u00b6 A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_chronic.py 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) EphysRecording \u00b6 Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_chronic.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> Session -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( f \"No SpikeGLX data found for probe insertion: { key } \" + \" The probe serial number does not match.\" ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) EphysFile \u00b6 Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_chronic.py 209 210 211 212 213 214 215 216 217 218 219 220 221 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" make ( key ) \u00b6 Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_chronic.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( f \"No SpikeGLX data found for probe insertion: { key } \" + \" The probe serial number does not match.\" ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) InsertionLocation \u00b6 Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_chronic.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\" LFP \u00b6 Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_chronic.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> EphysRecording --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) Electrode \u00b6 Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_chronic.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" make ( key ) \u00b6 Populates the LFP tables. Source code in element_array_ephys/ephys_chronic.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) ProbeInsertion \u00b6 Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_chronic.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion chronically implanted into an animal. -> Subject insertion_number: tinyint unsigned --- -> probe.Probe insertion_datetime=null: datetime \"\"\" QualityMetrics \u00b6 Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_chronic.py 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) Cluster \u00b6 Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_chronic.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" Waveform \u00b6 Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_chronic.py 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" make ( key ) \u00b6 Populates tables with quality metrics data. Source code in element_array_ephys/ephys_chronic.py 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) WaveformSet \u00b6 Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_chronic.py 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) PeakWaveform \u00b6 Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_chronic.py 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" Waveform \u00b6 Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_chronic.py 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" make ( key ) \u00b6 Populates waveform tables. Source code in element_array_ephys/ephys_chronic.py 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. Source code in element_array_ephys/ephys_chronic.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None , ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_chronic.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_chronic.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" root_directories = _linking_module . get_ephys_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories get_neuropixels_channel2electrode_map ( ephys_recording_key , acq_software ) \u00b6 Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_chronic.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : probe_dataset = get_openephys_probe_data ( ephys_recording_key ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_indices\" ] } return channel2electrode_map get_openephys_probe_data ( ephys_recording_key ) \u00b6 Get OpenEphys probe data from file. Source code in element_array_ephys/ephys_chronic.py 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 def get_openephys_probe_data ( ephys_recording_key : dict ) -> list : \"\"\"Get OpenEphys probe data from file. \"\"\" inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) probe_data = loaded_oe . probes [ inserted_probe_serial_number ] # explicitly garbage collect \"loaded_oe\" # as these may have large memory footprint and may not be cleared fast enough del loaded_oe gc . collect () return probe_data get_processed_root_data_dir () \u00b6 Retrieve the root directory for all processed data. Returns: Type Description str A string for the full path to the root directory for processed data. Source code in element_array_ephys/ephys_chronic.py 105 106 107 108 109 110 111 112 113 114 115 def get_processed_root_data_dir () -> str : \"\"\"Retrieve the root directory for all processed data. Returns: A string for the full path to the root directory for processed data. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_ephys_root_data_dir ()[ 0 ] get_recording_channels_details ( ephys_recording_key ) \u00b6 Get details of recording channels for a given recording. Source code in element_array_ephys/ephys_chronic.py 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 def get_recording_channels_details ( ephys_recording_key : dict ) -> np . array : \"\"\"Get details of recording channels for a given recording. \"\"\" channels_details = {} acq_software , sample_rate = ( EphysRecording & ephys_recording_key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) probe_type = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe_type\" ) channels_details [ \"probe_type\" ] = { \"neuropixels 1.0 - 3A\" : \"3A\" , \"neuropixels 1.0 - 3B\" : \"NP1\" , \"neuropixels UHD\" : \"NP1100\" , \"neuropixels 2.0 - SS\" : \"NP21\" , \"neuropixels 2.0 - MS\" : \"NP24\" , }[ probe_type ] electrode_config_key = ( probe . ElectrodeConfig * EphysRecording & ephys_recording_key ) . fetch1 ( \"KEY\" ) ( channels_details [ \"channel_ind\" ], channels_details [ \"x_coords\" ], channels_details [ \"y_coords\" ], channels_details [ \"shank_ind\" ], ) = ( probe . ElectrodeConfig . Electrode * probe . ProbeType . Electrode & electrode_config_key ) . fetch ( \"electrode\" , \"x_coord\" , \"y_coord\" , \"shank\" ) channels_details [ \"sample_rate\" ] = sample_rate channels_details [ \"num_channels\" ] = len ( channels_details [ \"channel_ind\" ]) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) channels_details [ \"uVPerBit\" ] = spikeglx_recording . get_channel_bit_volts ( \"ap\" )[ 0 ] channels_details [ \"connected\" ] = np . array ( [ v for * _ , v in spikeglx_recording . apmeta . shankmap [ \"data\" ]] ) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( ephys_recording_key ) channels_details [ \"uVPerBit\" ] = oe_probe . ap_meta [ \"channels_gains\" ][ 0 ] channels_details [ \"connected\" ] = np . array ( [ int ( v == 1 ) for c , v in oe_probe . channels_connected . items () if c in channels_details [ \"channel_ind\" ] ] ) return channels_details get_session_directory ( session_key ) \u00b6 Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_chronic.py 93 94 95 96 97 98 99 100 101 102 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key ) get_spikeglx_meta_filepath ( ephys_recording_key ) \u00b6 Get spikeGLX data filepath. Source code in element_array_ephys/ephys_chronic.py 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = pathlib . Path ( ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "ephys_chronic.py"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.AcquisitionSoftware", "text": "Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_chronic.py 121 122 123 124 125 126 127 128 129 130 131 132 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Software used for recording of neuropixels probes acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ])", "title": "AcquisitionSoftware"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusterQualityLabel", "text": "Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_chronic.py 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) # cluster quality type - e.g. 'good', 'MUA', 'noise', etc. --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ]", "title": "ClusterQualityLabel"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering", "text": "Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_chronic.py 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering.make", "text": "Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_chronic.py 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringMethod", "text": "Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_chronic.py 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort2\" , \"kilosort2 clustering method\" ), ( \"kilosort2.5\" , \"kilosort2.5 clustering method\" ), ( \"kilosort3\" , \"kilosort3 clustering method\" ), ]", "title": "ClusteringMethod"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringParamSet", "text": "Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_chronic.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict )", "title": "ClusteringParamSet"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringParamSet.insert_new_params", "text": "Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. None Source code in element_array_ephys/ephys_chronic.py 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask", "text": "Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_chronic.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> ClusteringParamSet --- clustering_output_dir='': varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) sess_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), sess_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / sess_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } )", "title": "ClusteringTask"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask.auto_generate_entries", "text": "Autogenerate entries based on a particular ephys recording. Parameters: Name Type Description Default ephys_recording_key dict EphysRecording primary key. required paramset_idx int Parameter index to use for clustering task. Defaults to 0. 0 Source code in element_array_ephys/ephys_chronic.py 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } )", "title": "auto_generate_entries()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask.infer_output_dir", "text": "Infer output directory if it is not provided. Parameters: Name Type Description Default key dict ClusteringTask primary key. required Returns: Type Description Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 Source code in element_array_ephys/ephys_chronic.py 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 @classmethod def infer_output_dir ( cls , key , relative = False , mkdir = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) sess_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key )) root_dir = find_root_directory ( get_ephys_root_data_dir (), sess_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / sess_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering", "text": "Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_chronic.py 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of a curation. -> Curation \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "CuratedClustering"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering.Unit", "text": "Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_chronic.py 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\"", "title": "Unit"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering.make", "text": "Automated population of Unit information. Source code in element_array_ephys/ephys_chronic.py 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "make()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation", "text": "Bases: dj . Manual Curation procedure table. Attributes: Name Type Description Clustering foreign key Clustering primary key. curation_id foreign key, int Unique curation ID. curation_time datetime Time when curation results are generated. curation_output_dir varchar (255) Output directory of the curated results. quality_control bool If True, this clustering result has undergone quality control. manual_curation bool If True, manual curation has been performed on this clustering result. curation_note varchar (2000) Notes about the curation task. Source code in element_array_ephys/ephys_chronic.py 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 @schema class Curation ( dj . Manual ): \"\"\"Curation procedure table. Attributes: Clustering (foreign key): Clustering primary key. curation_id (foreign key, int): Unique curation ID. curation_time (datetime): Time when curation results are generated. curation_output_dir (varchar(255) ): Output directory of the curated results. quality_control (bool): If True, this clustering result has undergone quality control. manual_curation (bool): If True, manual curation has been performed on this clustering result. curation_note (varchar(2000) ): Notes about the curation task. \"\"\" definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "Curation"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation.create1_from_clustering_task", "text": "A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_chronic.py 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "create1_from_clustering_task()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording", "text": "Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_chronic.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> Session -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( f \"No SpikeGLX data found for probe insertion: { key } \" + \" The probe serial number does not match.\" ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "EphysRecording"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording.EphysFile", "text": "Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_chronic.py 209 210 211 212 213 214 215 216 217 218 219 220 221 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\"", "title": "EphysFile"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording.make", "text": "Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_chronic.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ) ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( f \"No SpikeGLX data found for probe insertion: { key } \" + \" The probe serial number does not match.\" ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.InsertionLocation", "text": "Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_chronic.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\"", "title": "InsertionLocation"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP", "text": "Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_chronic.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> EphysRecording --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "LFP"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP.Electrode", "text": "Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_chronic.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\"", "title": "Electrode"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP.make", "text": "Populates the LFP tables. Source code in element_array_ephys/ephys_chronic.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ProbeInsertion", "text": "Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_chronic.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion chronically implanted into an animal. -> Subject insertion_number: tinyint unsigned --- -> probe.Probe insertion_datetime=null: datetime \"\"\"", "title": "ProbeInsertion"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics", "text": "Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_chronic.py 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "QualityMetrics"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.Cluster", "text": "Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_chronic.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\"", "title": "Cluster"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.Waveform", "text": "Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_chronic.py 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.make", "text": "Populates tables with quality metrics data. Source code in element_array_ephys/ephys_chronic.py 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet", "text": "Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_chronic.py 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "WaveformSet"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.PeakWaveform", "text": "Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_chronic.py 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\"", "title": "PeakWaveform"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.Waveform", "text": "Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_chronic.py 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.make", "text": "Populates waveform tables. Source code in element_array_ephys/ephys_chronic.py 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.activate", "text": "Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. Source code in element_array_ephys/ephys_chronic.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None , ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_chronic.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_ephys_root_data_dir", "text": "Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_chronic.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" root_directories = _linking_module . get_ephys_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_neuropixels_channel2electrode_map", "text": "Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_chronic.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : probe_dataset = get_openephys_probe_data ( ephys_recording_key ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_indices\" ] } return channel2electrode_map", "title": "get_neuropixels_channel2electrode_map()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_openephys_probe_data", "text": "Get OpenEphys probe data from file. Source code in element_array_ephys/ephys_chronic.py 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 def get_openephys_probe_data ( ephys_recording_key : dict ) -> list : \"\"\"Get OpenEphys probe data from file. \"\"\" inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) probe_data = loaded_oe . probes [ inserted_probe_serial_number ] # explicitly garbage collect \"loaded_oe\" # as these may have large memory footprint and may not be cleared fast enough del loaded_oe gc . collect () return probe_data", "title": "get_openephys_probe_data()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_processed_root_data_dir", "text": "Retrieve the root directory for all processed data. Returns: Type Description str A string for the full path to the root directory for processed data. Source code in element_array_ephys/ephys_chronic.py 105 106 107 108 109 110 111 112 113 114 115 def get_processed_root_data_dir () -> str : \"\"\"Retrieve the root directory for all processed data. Returns: A string for the full path to the root directory for processed data. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_ephys_root_data_dir ()[ 0 ]", "title": "get_processed_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_recording_channels_details", "text": "Get details of recording channels for a given recording. Source code in element_array_ephys/ephys_chronic.py 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 def get_recording_channels_details ( ephys_recording_key : dict ) -> np . array : \"\"\"Get details of recording channels for a given recording. \"\"\" channels_details = {} acq_software , sample_rate = ( EphysRecording & ephys_recording_key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) probe_type = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe_type\" ) channels_details [ \"probe_type\" ] = { \"neuropixels 1.0 - 3A\" : \"3A\" , \"neuropixels 1.0 - 3B\" : \"NP1\" , \"neuropixels UHD\" : \"NP1100\" , \"neuropixels 2.0 - SS\" : \"NP21\" , \"neuropixels 2.0 - MS\" : \"NP24\" , }[ probe_type ] electrode_config_key = ( probe . ElectrodeConfig * EphysRecording & ephys_recording_key ) . fetch1 ( \"KEY\" ) ( channels_details [ \"channel_ind\" ], channels_details [ \"x_coords\" ], channels_details [ \"y_coords\" ], channels_details [ \"shank_ind\" ], ) = ( probe . ElectrodeConfig . Electrode * probe . ProbeType . Electrode & electrode_config_key ) . fetch ( \"electrode\" , \"x_coord\" , \"y_coord\" , \"shank\" ) channels_details [ \"sample_rate\" ] = sample_rate channels_details [ \"num_channels\" ] = len ( channels_details [ \"channel_ind\" ]) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) channels_details [ \"uVPerBit\" ] = spikeglx_recording . get_channel_bit_volts ( \"ap\" )[ 0 ] channels_details [ \"connected\" ] = np . array ( [ v for * _ , v in spikeglx_recording . apmeta . shankmap [ \"data\" ]] ) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( ephys_recording_key ) channels_details [ \"uVPerBit\" ] = oe_probe . ap_meta [ \"channels_gains\" ][ 0 ] channels_details [ \"connected\" ] = np . array ( [ int ( v == 1 ) for c , v in oe_probe . channels_connected . items () if c in channels_details [ \"channel_ind\" ] ] ) return channels_details", "title": "get_recording_channels_details()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_session_directory", "text": "Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_chronic.py 93 94 95 96 97 98 99 100 101 102 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_spikeglx_meta_filepath", "text": "Get spikeGLX data filepath. Source code in element_array_ephys/ephys_chronic.py 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = pathlib . Path ( ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "get_spikeglx_meta_filepath()"}, {"location": "api/element_array_ephys/ephys_no_curation/", "text": "AcquisitionSoftware \u00b6 Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_no_curation.py 123 124 125 126 127 128 129 130 131 132 133 134 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ]) ClusterQualityLabel \u00b6 Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_no_curation.py 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) # cluster quality type - e.g. 'good', 'MUA', 'noise', etc. --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ] Clustering \u00b6 Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_no_curation.py 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) make ( key ) \u00b6 Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_no_curation.py 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) ClusteringMethod \u00b6 Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_no_curation.py 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort2\" , \"kilosort2 clustering method\" ), ( \"kilosort2.5\" , \"kilosort2.5 clustering method\" ), ( \"kilosort3\" , \"kilosort3 clustering method\" ), ] ClusteringParamSet \u00b6 Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_no_curation.py 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict ) insert_new_params ( clustering_method , paramset_desc , params , paramset_idx = None ) classmethod \u00b6 Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. None Source code in element_array_ephys/ephys_no_curation.py 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict ) ClusteringTask \u00b6 Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_no_curation.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> ClusteringParamSet --- clustering_output_dir='': varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } ) auto_generate_entries ( ephys_recording_key , paramset_idx = 0 ) classmethod \u00b6 Autogenerate entries based on a particular ephys recording. Parameters: Name Type Description Default ephys_recording_key dict EphysRecording primary key. required paramset_idx int Parameter index to use for clustering task. Defaults to 0. 0 Source code in element_array_ephys/ephys_no_curation.py 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } ) infer_output_dir ( key , relative = False , mkdir = False ) classmethod \u00b6 Infer output directory if it is not provided. Parameters: Name Type Description Default key dict ClusteringTask primary key. required Returns: Type Description Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 Source code in element_array_ephys/ephys_no_curation.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 @classmethod def infer_output_dir ( cls , key , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir CuratedClustering \u00b6 Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_no_curation.py 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of the spike sorting step. -> Clustering \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Unit \u00b6 Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_no_curation.py 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" make ( key ) \u00b6 Automated population of Unit information. Source code in element_array_ephys/ephys_no_curation.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) EphysRecording \u00b6 Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_no_curation.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) EphysFile \u00b6 Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_no_curation.py 284 285 286 287 288 289 290 291 292 293 294 295 296 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" make ( key ) \u00b6 Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_no_curation.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) InsertionLocation \u00b6 Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_no_curation.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\" LFP \u00b6 Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_no_curation.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> EphysRecording --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) Electrode \u00b6 Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_no_curation.py 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" make ( key ) \u00b6 Populates the LFP tables. Source code in element_array_ephys/ephys_no_curation.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) ProbeInsertion \u00b6 Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_no_curation.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion implanted into an animal for a given session. -> Session insertion_number: tinyint unsigned --- -> probe.Probe \"\"\" @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list ) cls . insert ( probe_insertion_list , skip_duplicates = True ) auto_generate_entries ( session_key ) classmethod \u00b6 Automatically populate entries in ProbeInsertion table for a session. Source code in element_array_ephys/ephys_no_curation.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list ) cls . insert ( probe_insertion_list , skip_duplicates = True ) QualityMetrics \u00b6 Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_no_curation.py 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) Cluster \u00b6 Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_no_curation.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" Waveform \u00b6 Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_no_curation.py 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" make ( key ) \u00b6 Populates tables with quality metrics data. Source code in element_array_ephys/ephys_no_curation.py 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) WaveformSet \u00b6 Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_no_curation.py 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if ( kilosort_dir / \"mean_waveforms.npy\" ) . exists (): unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) PeakWaveform \u00b6 Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_no_curation.py 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" Waveform \u00b6 Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_no_curation.py 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" make ( key ) \u00b6 Populates waveform tables. Source code in element_array_ephys/ephys_no_curation.py 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if ( kilosort_dir / \"mean_waveforms.npy\" ) . exists (): unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. Source code in element_array_ephys/ephys_no_curation.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None , ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module # activate probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_no_curation.py 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_no_curation.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" root_directories = _linking_module . get_ephys_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories get_neuropixels_channel2electrode_map ( ephys_recording_key , acq_software ) \u00b6 Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_no_curation.py 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : probe_dataset = get_openephys_probe_data ( ephys_recording_key ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_indices\" ] } return channel2electrode_map get_openephys_probe_data ( ephys_recording_key ) \u00b6 Get OpenEphys probe data from file. Source code in element_array_ephys/ephys_no_curation.py 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 def get_openephys_probe_data ( ephys_recording_key : dict ) -> list : \"\"\"Get OpenEphys probe data from file. \"\"\" inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) probe_data = loaded_oe . probes [ inserted_probe_serial_number ] # explicitly garbage collect \"loaded_oe\" # as these may have large memory footprint and may not be cleared fast enough del loaded_oe gc . collect () return probe_data get_processed_root_data_dir () \u00b6 Retrieve the root directory for all processed data. Returns: Type Description str A string for the full path to the root directory for processed data. Source code in element_array_ephys/ephys_no_curation.py 107 108 109 110 111 112 113 114 115 116 117 def get_processed_root_data_dir () -> str : \"\"\"Retrieve the root directory for all processed data. Returns: A string for the full path to the root directory for processed data. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_ephys_root_data_dir ()[ 0 ] get_recording_channels_details ( ephys_recording_key ) \u00b6 Get details of recording channels for a givenn recording. Source code in element_array_ephys/ephys_no_curation.py 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 def get_recording_channels_details ( ephys_recording_key : dict ) -> np . array : \"\"\"Get details of recording channels for a givenn recording. \"\"\" channels_details = {} acq_software , sample_rate = ( EphysRecording & ephys_recording_key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) probe_type = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe_type\" ) channels_details [ \"probe_type\" ] = { \"neuropixels 1.0 - 3A\" : \"3A\" , \"neuropixels 1.0 - 3B\" : \"NP1\" , \"neuropixels UHD\" : \"NP1100\" , \"neuropixels 2.0 - SS\" : \"NP21\" , \"neuropixels 2.0 - MS\" : \"NP24\" , }[ probe_type ] electrode_config_key = ( probe . ElectrodeConfig * EphysRecording & ephys_recording_key ) . fetch1 ( \"KEY\" ) ( channels_details [ \"channel_ind\" ], channels_details [ \"x_coords\" ], channels_details [ \"y_coords\" ], channels_details [ \"shank_ind\" ], ) = ( probe . ElectrodeConfig . Electrode * probe . ProbeType . Electrode & electrode_config_key ) . fetch ( \"electrode\" , \"x_coord\" , \"y_coord\" , \"shank\" ) channels_details [ \"sample_rate\" ] = sample_rate channels_details [ \"num_channels\" ] = len ( channels_details [ \"channel_ind\" ]) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) channels_details [ \"uVPerBit\" ] = spikeglx_recording . get_channel_bit_volts ( \"ap\" )[ 0 ] channels_details [ \"connected\" ] = np . array ( [ v for * _ , v in spikeglx_recording . apmeta . shankmap [ \"data\" ]] ) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( ephys_recording_key ) channels_details [ \"uVPerBit\" ] = oe_probe . ap_meta [ \"channels_gains\" ][ 0 ] channels_details [ \"connected\" ] = np . array ( [ int ( v == 1 ) for c , v in oe_probe . channels_connected . items () if c in channels_details [ \"channel_ind\" ] ] ) return channels_details get_session_directory ( session_key ) \u00b6 Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_no_curation.py 95 96 97 98 99 100 101 102 103 104 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key ) get_spikeglx_meta_filepath ( ephys_recording_key ) \u00b6 Get spikeGLX data filepath. Source code in element_array_ephys/ephys_no_curation.py 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = pathlib . Path ( ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "ephys_no_curation.py"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.AcquisitionSoftware", "text": "Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_no_curation.py 123 124 125 126 127 128 129 130 131 132 133 134 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ])", "title": "AcquisitionSoftware"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusterQualityLabel", "text": "Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_no_curation.py 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) # cluster quality type - e.g. 'good', 'MUA', 'noise', etc. --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ]", "title": "ClusterQualityLabel"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.Clustering", "text": "Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_no_curation.py 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.Clustering.make", "text": "Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_no_curation.py 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) if not output_dir : output_dir = ClusteringTask . infer_output_dir ( key , relative = True , mkdir = True ) # update clustering_output_dir ClusteringTask . update1 ( { ** key , \"clustering_output_dir\" : output_dir . as_posix ()} ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output elif task_mode == \"trigger\" : acq_software , clustering_method , params = ( ClusteringTask * EphysRecording * ClusteringParamSet & key ) . fetch1 ( \"acq_software\" , \"clustering_method\" , \"params\" ) if \"kilosort\" in clustering_method : from element_array_ephys.readers import kilosort_triggering # add additional probe-recording and channels details into `params` params = { ** params , ** get_recording_channels_details ( key )} params [ \"fs\" ] = params [ \"sample_rate\" ] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) spikeglx_recording . validate_file ( \"ap\" ) if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = spikeglx_recording . root_dir / ( spikeglx_recording . root_name + \".ap.bin\" ), kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . SGLXKilosortPipeline ( npx_input_dir = spikeglx_meta_filepath . parent , ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , run_CatGT = True , ) run_kilosort . run_modules () elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) assert len ( oe_probe . recording_info [ \"recording_files\" ]) == 1 # run kilosort if clustering_method . startswith ( \"pykilosort\" ): kilosort_triggering . run_pykilosort ( continuous_file = pathlib . Path ( oe_probe . recording_info [ \"recording_files\" ][ 0 ] ) / \"continuous.dat\" , kilosort_output_directory = kilosort_dir , channel_ind = params . pop ( \"channel_ind\" ), x_coords = params . pop ( \"x_coords\" ), y_coords = params . pop ( \"y_coords\" ), shank_ind = params . pop ( \"shank_ind\" ), connected = params . pop ( \"connected\" ), sample_rate = params . pop ( \"sample_rate\" ), params = params , ) else : run_kilosort = kilosort_triggering . OpenEphysKilosortPipeline ( npx_input_dir = oe_probe . recording_info [ \"recording_files\" ][ 0 ], ks_output_dir = kilosort_dir , params = params , KS2ver = f ' { Decimal ( clustering_method . replace ( \"kilosort\" , \"\" )) : .1f } ' , ) run_kilosort . run_modules () else : raise NotImplementedError ( f \"Automatic triggering of { clustering_method } \" f \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringMethod", "text": "Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_no_curation.py 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort2\" , \"kilosort2 clustering method\" ), ( \"kilosort2.5\" , \"kilosort2.5 clustering method\" ), ( \"kilosort3\" , \"kilosort3 clustering method\" ), ]", "title": "ClusteringMethod"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringParamSet", "text": "Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_no_curation.py 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict )", "title": "ClusteringParamSet"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringParamSet.insert_new_params", "text": "Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. None Source code in element_array_ephys/ephys_no_curation.py 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 @classmethod def insert_new_params ( cls , clustering_method : str , paramset_desc : str , params : dict , paramset_idx : int = None , ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" if paramset_idx is None : paramset_idx = ( dj . U () . aggr ( cls , n = \"max(paramset_idx)\" ) . fetch1 ( \"n\" ) or 0 ) + 1 param_dict = { \"clustering_method\" : clustering_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( { ** params , \"clustering_method\" : clustering_method } ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( f \"The specified param-set already exists\" f \" - with paramset_idx: { existing_paramset_idx } \" ) else : if { \"paramset_idx\" : paramset_idx } in cls . proj (): raise dj . DataJointError ( f \"The specified paramset_idx { paramset_idx } already exists,\" f \" please pick a different one.\" ) cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask", "text": "Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_no_curation.py 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> ClusteringParamSet --- clustering_output_dir='': varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" @classmethod def infer_output_dir ( cls , key , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } )", "title": "ClusteringTask"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask.auto_generate_entries", "text": "Autogenerate entries based on a particular ephys recording. Parameters: Name Type Description Default ephys_recording_key dict EphysRecording primary key. required paramset_idx int Parameter index to use for clustering task. Defaults to 0. 0 Source code in element_array_ephys/ephys_no_curation.py 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 @classmethod def auto_generate_entries ( cls , ephys_recording_key : dict , paramset_idx : int = 0 ): \"\"\"Autogenerate entries based on a particular ephys recording. Args: ephys_recording_key (dict): EphysRecording primary key. paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0. \"\"\" key = { ** ephys_recording_key , \"paramset_idx\" : paramset_idx } processed_dir = get_processed_root_data_dir () output_dir = ClusteringTask . infer_output_dir ( key , relative = False , mkdir = True ) try : kilosort . Kilosort ( output_dir ) # check if the directory is a valid Kilosort output except FileNotFoundError : task_mode = \"trigger\" else : task_mode = \"load\" cls . insert1 ( { ** key , \"clustering_output_dir\" : output_dir . relative_to ( processed_dir ) . as_posix (), \"task_mode\" : task_mode , } )", "title": "auto_generate_entries()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask.infer_output_dir", "text": "Infer output directory if it is not provided. Parameters: Name Type Description Default key dict ClusteringTask primary key. required Returns: Type Description Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 Source code in element_array_ephys/ephys_no_curation.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 @classmethod def infer_output_dir ( cls , key , relative : bool = False , mkdir : bool = False ): \"\"\"Infer output directory if it is not provided. Args: key (dict): ClusteringTask primary key. Returns: Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx} e.g.: sub4/sess1/probe_2/kilosort2_0 \"\"\" processed_dir = pathlib . Path ( get_processed_root_data_dir ()) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) method = ( ( ClusteringParamSet * ClusteringMethod & key ) . fetch1 ( \"clustering_method\" ) . replace ( \".\" , \"-\" ) ) output_dir = ( processed_dir / session_dir . relative_to ( root_dir ) / f 'probe_ { key [ \"insertion_number\" ] } ' / f ' { method } _ { key [ \"paramset_idx\" ] } ' ) if mkdir : output_dir . mkdir ( parents = True , exist_ok = True ) log . info ( f \" { output_dir } created!\" ) return output_dir . relative_to ( processed_dir ) if relative else output_dir", "title": "infer_output_dir()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering", "text": "Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_no_curation.py 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of the spike sorting step. -> Clustering \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "CuratedClustering"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering.Unit", "text": "Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_no_curation.py 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\"", "title": "Unit"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering.make", "text": "Automated population of Unit information. Source code in element_array_ephys/ephys_no_curation.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , sample_rate = ( EphysRecording & key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) sample_rate = kilosort_dataset . data [ \"params\" ] . get ( \"sample_rate\" , sample_rate ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / sample_rate ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "make()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording", "text": "Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_no_curation.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "EphysRecording"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording.EphysFile", "text": "Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_no_curation.py 284 285 286 287 288 289 290 291 292 293 294 295 296 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\"", "title": "EphysFile"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording.make", "text": "Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_no_curation.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" ) supported_probe_types = probe . ProbeType . fetch ( \"probe_type\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if spikeglx_meta . probe_model in supported_probe_types : probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if not probe_data . ap_meta : raise IOError ( 'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory' ) if probe_data . probe_model in supported_probe_types : probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_indices\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) # explicitly garbage collect \"dataset\" # as these may have large memory footprint and may not be cleared fast enough del probe_data , dataset gc . collect () else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.InsertionLocation", "text": "Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_no_curation.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\"", "title": "InsertionLocation"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP", "text": "Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_no_curation.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> EphysRecording --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "LFP"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP.Electrode", "text": "Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_no_curation.py 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\"", "title": "Electrode"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP.make", "text": "Populates the LFP tables. Source code in element_array_ephys/ephys_no_curation.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( key ) lfp_channel_ind = np . r_ [ len ( oe_probe . lfp_meta [ \"channels_indices\" ]) - 1 : 0 : - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_keys . extend ( probe_electrodes [ channel_idx ] for channel_idx in lfp_channel_ind ) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ProbeInsertion", "text": "Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_no_curation.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion implanted into an animal for a given session. -> Session insertion_number: tinyint unsigned --- -> probe.Probe \"\"\" @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list ) cls . insert ( probe_insertion_list , skip_duplicates = True )", "title": "ProbeInsertion"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ProbeInsertion.auto_generate_entries", "text": "Automatically populate entries in ProbeInsertion table for a session. Source code in element_array_ephys/ephys_no_curation.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 @classmethod def auto_generate_entries ( cls , session_key ): \"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( session_key ) ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = list ( session_dir . rglob ( ephys_pattern )) if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found in: { session_dir } \" ) probe_list , probe_insertion_list = [], [] if acq_software == \"SpikeGLX\" : for meta_fp_idx , meta_filepath in enumerate ( ephys_meta_filepaths ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent try : probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) except AttributeError : probe_number = meta_fp_idx probe_insertion_list . append ( { ** session_key , \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) elif acq_software == \"Open Ephys\" : loaded_oe = openephys . OpenEphys ( session_dir ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_insertion_list . append ( { ** session_key , \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx , } ) else : raise NotImplementedError ( f \"Unknown acquisition software: { acq_software } \" ) probe . Probe . insert ( probe_list ) cls . insert ( probe_insertion_list , skip_duplicates = True )", "title": "auto_generate_entries()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics", "text": "Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_no_curation.py 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "QualityMetrics"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.Cluster", "text": "Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_no_curation.py 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\"", "title": "Cluster"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.Waveform", "text": "Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_no_curation.py 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.make", "text": "Populates tables with quality metrics data. Source code in element_array_ephys/ephys_no_curation.py 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_df . replace ([ np . inf , - np . inf ], np . nan , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet", "text": "Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_no_curation.py 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if ( kilosort_dir / \"mean_waveforms.npy\" ) . exists (): unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "WaveformSet"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.PeakWaveform", "text": "Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_no_curation.py 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\"", "title": "PeakWaveform"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.Waveform", "text": "Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_no_curation.py 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.make", "text": "Populates waveform tables. Source code in element_array_ephys/ephys_no_curation.py 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if ( kilosort_dir / \"mean_waveforms.npy\" ) . exists (): unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): if unit_peak_waveform : self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) if unit_electrode_waveforms : self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.activate", "text": "Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. Source code in element_array_ephys/ephys_no_curation.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None , ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module # activate probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_no_curation.py 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_ephys_root_data_dir", "text": "Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_no_curation.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" root_directories = _linking_module . get_ephys_root_data_dir () if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ root_directories ] if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): root_directories . append ( _linking_module . get_processed_root_data_dir ()) return root_directories", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_neuropixels_channel2electrode_map", "text": "Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_no_curation.py 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : probe_dataset = get_openephys_probe_data ( ephys_recording_key ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_indices\" ] } return channel2electrode_map", "title": "get_neuropixels_channel2electrode_map()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_openephys_probe_data", "text": "Get OpenEphys probe data from file. Source code in element_array_ephys/ephys_no_curation.py 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 def get_openephys_probe_data ( ephys_recording_key : dict ) -> list : \"\"\"Get OpenEphys probe data from file. \"\"\" inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) probe_data = loaded_oe . probes [ inserted_probe_serial_number ] # explicitly garbage collect \"loaded_oe\" # as these may have large memory footprint and may not be cleared fast enough del loaded_oe gc . collect () return probe_data", "title": "get_openephys_probe_data()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_processed_root_data_dir", "text": "Retrieve the root directory for all processed data. Returns: Type Description str A string for the full path to the root directory for processed data. Source code in element_array_ephys/ephys_no_curation.py 107 108 109 110 111 112 113 114 115 116 117 def get_processed_root_data_dir () -> str : \"\"\"Retrieve the root directory for all processed data. Returns: A string for the full path to the root directory for processed data. \"\"\" if hasattr ( _linking_module , \"get_processed_root_data_dir\" ): return _linking_module . get_processed_root_data_dir () else : return get_ephys_root_data_dir ()[ 0 ]", "title": "get_processed_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_recording_channels_details", "text": "Get details of recording channels for a givenn recording. Source code in element_array_ephys/ephys_no_curation.py 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 def get_recording_channels_details ( ephys_recording_key : dict ) -> np . array : \"\"\"Get details of recording channels for a givenn recording. \"\"\" channels_details = {} acq_software , sample_rate = ( EphysRecording & ephys_recording_key ) . fetch1 ( \"acq_software\" , \"sampling_rate\" ) probe_type = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe_type\" ) channels_details [ \"probe_type\" ] = { \"neuropixels 1.0 - 3A\" : \"3A\" , \"neuropixels 1.0 - 3B\" : \"NP1\" , \"neuropixels UHD\" : \"NP1100\" , \"neuropixels 2.0 - SS\" : \"NP21\" , \"neuropixels 2.0 - MS\" : \"NP24\" , }[ probe_type ] electrode_config_key = ( probe . ElectrodeConfig * EphysRecording & ephys_recording_key ) . fetch1 ( \"KEY\" ) ( channels_details [ \"channel_ind\" ], channels_details [ \"x_coords\" ], channels_details [ \"y_coords\" ], channels_details [ \"shank_ind\" ], ) = ( probe . ElectrodeConfig . Electrode * probe . ProbeType . Electrode & electrode_config_key ) . fetch ( \"electrode\" , \"x_coord\" , \"y_coord\" , \"shank\" ) channels_details [ \"sample_rate\" ] = sample_rate channels_details [ \"num_channels\" ] = len ( channels_details [ \"channel_ind\" ]) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) channels_details [ \"uVPerBit\" ] = spikeglx_recording . get_channel_bit_volts ( \"ap\" )[ 0 ] channels_details [ \"connected\" ] = np . array ( [ v for * _ , v in spikeglx_recording . apmeta . shankmap [ \"data\" ]] ) elif acq_software == \"Open Ephys\" : oe_probe = get_openephys_probe_data ( ephys_recording_key ) channels_details [ \"uVPerBit\" ] = oe_probe . ap_meta [ \"channels_gains\" ][ 0 ] channels_details [ \"connected\" ] = np . array ( [ int ( v == 1 ) for c , v in oe_probe . channels_connected . items () if c in channels_details [ \"channel_ind\" ] ] ) return channels_details", "title": "get_recording_channels_details()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_session_directory", "text": "Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_no_curation.py 95 96 97 98 99 100 101 102 103 104 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_spikeglx_meta_filepath", "text": "Get spikeGLX data filepath. Source code in element_array_ephys/ephys_no_curation.py 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = pathlib . Path ( ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "get_spikeglx_meta_filepath()"}, {"location": "api/element_array_ephys/ephys_precluster/", "text": "AcquisitionSoftware \u00b6 Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_precluster.py 96 97 98 99 100 101 102 103 104 105 106 107 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ]) ClusterQualityLabel \u00b6 Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_precluster.py 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ] Clustering \u00b6 Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_precluster.py 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) make ( key ) \u00b6 Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_precluster.py 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"clustering_time\" : creation_time }) ClusteringMethod \u00b6 Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_precluster.py 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort\" , \"kilosort clustering method\" ), ( \"kilosort2\" , \"kilosort2 clustering method\" ), ] ClusteringParamSet \u00b6 Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_precluster.py 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" param_dict = { \"clustering_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set\" \" already exists - paramset_idx: {} \" . format ( existing_paramset_idx ) ) else : cls . insert1 ( param_dict ) insert_new_params ( processing_method , paramset_idx , paramset_desc , params ) classmethod \u00b6 Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. required Source code in element_array_ephys/ephys_precluster.py 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" param_dict = { \"clustering_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set\" \" already exists - paramset_idx: {} \" . format ( existing_paramset_idx ) ) else : cls . insert1 ( param_dict ) ClusteringTask \u00b6 Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_precluster.py 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> PreCluster -> ClusteringParamSet --- clustering_output_dir: varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\" CuratedClustering \u00b6 Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_precluster.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of a curation. -> Curation \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / kilosort_dataset . data [ \"params\" ][ \"sample_rate\" ] ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Unit \u00b6 Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_precluster.py 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" make ( key ) \u00b6 Automated population of Unit information. Source code in element_array_ephys/ephys_precluster.py 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / kilosort_dataset . data [ \"params\" ][ \"sample_rate\" ] ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ]) Curation \u00b6 Bases: dj . Manual Curation procedure table. Attributes: Name Type Description Clustering foreign key Clustering primary key. curation_id foreign key, int Unique curation ID. curation_time datetime Time when curation results are generated. curation_output_dir varchar (255) Output directory of the curated results. quality_control bool If True, this clustering result has undergone quality control. manual_curation bool If True, manual curation has been performed on this clustering result. curation_note varchar (2000) Notes about the curation task. Source code in element_array_ephys/ephys_precluster.py 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 @schema class Curation ( dj . Manual ): \"\"\"Curation procedure table. Attributes: Clustering (foreign key): Clustering primary key. curation_id (foreign key, int): Unique curation ID. curation_time (datetime): Time when curation results are generated. curation_output_dir (varchar(255) ): Output directory of the curated results. quality_control (bool): If True, this clustering result has undergone quality control. manual_curation (bool): If True, manual curation has been performed on this clustering result. curation_note (varchar(2000) ): Notes about the curation task. \"\"\" definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) create1_from_clustering_task ( key , curation_note = '' ) \u00b6 A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_precluster.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } ) EphysRecording \u00b6 Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_precluster.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = [ fp for fp in session_dir . rglob ( ephys_pattern )] if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , spikeglx_meta . probe_model ): probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , probe_data . probe_model ): probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_ids\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) EphysFile \u00b6 Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_precluster.py 182 183 184 185 186 187 188 189 190 191 192 193 194 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" make ( key ) \u00b6 Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_precluster.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = [ fp for fp in session_dir . rglob ( ephys_pattern )] if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , spikeglx_meta . probe_model ): probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , probe_data . probe_model ): probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_ids\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" ) InsertionLocation \u00b6 Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_precluster.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\" LFP \u00b6 Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_precluster.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> PreCluster --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software , probe_sn = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) oe_probe = loaded_oe . probes [ probe_sn ] lfp_channel_ind = np . arange ( len ( oe_probe . lfp_meta [ \"channels_ids\" ]))[ - 1 :: - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } for channel_idx in np . array ( oe_probe . lfp_meta [ \"channels_ids\" ])[ lfp_channel_ind ]: electrode_keys . append ( probe_electrodes [ channel_idx ]) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) Electrode \u00b6 Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_precluster.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" make ( key ) \u00b6 Populates the LFP tables. Source code in element_array_ephys/ephys_precluster.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software , probe_sn = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) oe_probe = loaded_oe . probes [ probe_sn ] lfp_channel_ind = np . arange ( len ( oe_probe . lfp_meta [ \"channels_ids\" ]))[ - 1 :: - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } for channel_idx in np . array ( oe_probe . lfp_meta [ \"channels_ids\" ])[ lfp_channel_ind ]: electrode_keys . append ( probe_electrodes [ channel_idx ]) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace }) PreCluster \u00b6 Bases: dj . Imported A processing table to handle each PreClusterTask: Attributes: Name Type Description PreClusterTask foreign key PreClusterTask primary key. precluster_time datetime Time of generation of this set of pre-clustering results. package_version varchar (16) Package version used for performing pre-clustering. Source code in element_array_ephys/ephys_precluster.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 @schema class PreCluster ( dj . Imported ): \"\"\" A processing table to handle each PreClusterTask: Attributes: PreClusterTask (foreign key): PreClusterTask primary key. precluster_time (datetime): Time of generation of this set of pre-clustering results. package_version (varchar(16) ): Package version used for performing pre-clustering. \"\"\" definition = \"\"\" -> PreClusterTask --- precluster_time: datetime # time of generation of this set of pre-clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Populate pre-clustering tables.\"\"\" task_mode , output_dir = ( PreClusterTask & key ) . fetch1 ( \"task_mode\" , \"precluster_output_dir\" ) precluster_output_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"none\" : if len (( PreClusterParamSteps . Step & key ) . fetch ()) > 0 : raise ValueError ( \"There are entries in the PreClusterParamSteps.Step \" \"table and task_mode=none\" ) creation_time = ( EphysRecording & key ) . fetch1 ( \"recording_datetime\" ) elif task_mode == \"load\" : acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in precluster_output_dir . rglob ( \"*.ap.meta\" ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : creation_time = spikeglx_meta . recording_time break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) else : raise NotImplementedError ( f \"Pre-clustering analysis of { acq_software } \" \"is not yet supported.\" ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" pre-clustering analysis is not yet supported.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"precluster_time\" : creation_time }) make ( key ) \u00b6 Populate pre-clustering tables. Source code in element_array_ephys/ephys_precluster.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def make ( self , key ): \"\"\"Populate pre-clustering tables.\"\"\" task_mode , output_dir = ( PreClusterTask & key ) . fetch1 ( \"task_mode\" , \"precluster_output_dir\" ) precluster_output_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"none\" : if len (( PreClusterParamSteps . Step & key ) . fetch ()) > 0 : raise ValueError ( \"There are entries in the PreClusterParamSteps.Step \" \"table and task_mode=none\" ) creation_time = ( EphysRecording & key ) . fetch1 ( \"recording_datetime\" ) elif task_mode == \"load\" : acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in precluster_output_dir . rglob ( \"*.ap.meta\" ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : creation_time = spikeglx_meta . recording_time break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) else : raise NotImplementedError ( f \"Pre-clustering analysis of { acq_software } \" \"is not yet supported.\" ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" pre-clustering analysis is not yet supported.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"precluster_time\" : creation_time }) PreClusterMethod \u00b6 Bases: dj . Lookup Pre-clustering method Attributes: precluster_method (foreign key, varchar(16) ): Pre-clustering method for the dataset. precluster_method_desc(varchar(1000) ): Pre-clustering method description. Source code in element_array_ephys/ephys_precluster.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 @schema class PreClusterMethod ( dj . Lookup ): \"\"\"Pre-clustering method Attributes: precluster_method (foreign key, varchar(16) ): Pre-clustering method for the dataset. precluster_method_desc(varchar(1000) ): Pre-clustering method description. \"\"\" definition = \"\"\" # Method for pre-clustering precluster_method: varchar(16) --- precluster_method_desc: varchar(1000) \"\"\" contents = [( \"catgt\" , \"Time shift, Common average referencing, Zeroing\" )] PreClusterParamSet \u00b6 Bases: dj . Lookup Parameters for the pre-clustering method. Attributes: Name Type Description paramset_idx foreign key Unique parameter set ID. PreClusterMethod dict PreClusterMethod query for this dataset. paramset_desc varchar (128) Description for the pre-clustering parameter set. param_set_hash uuid Unique hash for parameter set. params longblob All parameters for the pre-clustering method. Source code in element_array_ephys/ephys_precluster.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 @schema class PreClusterParamSet ( dj . Lookup ): \"\"\"Parameters for the pre-clustering method. Attributes: paramset_idx (foreign key): Unique parameter set ID. PreClusterMethod (dict): PreClusterMethod query for this dataset. paramset_desc (varchar(128) ): Description for the pre-clustering parameter set. param_set_hash (uuid): Unique hash for parameter set. params (longblob): All parameters for the pre-clustering method. \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> PreClusterMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , precluster_method : str , paramset_idx : int , paramset_desc : str , params : dict ): param_dict = { \"precluster_method\" : precluster_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set\" \" already exists - paramset_idx: {} \" . format ( existing_paramset_idx ) ) else : cls . insert1 ( param_dict ) PreClusterParamSteps \u00b6 Bases: dj . Manual Ordered list of parameter sets that will be run. Attributes: Name Type Description precluster_param_steps_id foreign key Unique ID for the pre-clustering parameter sets to be run. precluster_param_steps_name varchar (32) User-friendly name for the parameter steps. precluster_param_steps_desc varchar (128) Description of the parameter steps. Source code in element_array_ephys/ephys_precluster.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 @schema class PreClusterParamSteps ( dj . Manual ): \"\"\"Ordered list of parameter sets that will be run. Attributes: precluster_param_steps_id (foreign key): Unique ID for the pre-clustering parameter sets to be run. precluster_param_steps_name (varchar(32) ): User-friendly name for the parameter steps. precluster_param_steps_desc (varchar(128) ): Description of the parameter steps. \"\"\" definition = \"\"\" # Ordered list of paramset_idx that are to be run # When pre-clustering is not performed, do not create an entry in `Step` Part table precluster_param_steps_id: smallint --- precluster_param_steps_name: varchar(32) precluster_param_steps_desc: varchar(128) \"\"\" class Step ( dj . Part ): \"\"\"Define the order of operations for parameter sets. Attributes: PreClusterParamSteps (foreign key): PreClusterParamSteps primary key. step_number (foreign key, smallint): Order of operations. PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreClusterParamSet \"\"\" Step \u00b6 Bases: dj . Part Define the order of operations for parameter sets. Attributes: Name Type Description PreClusterParamSteps foreign key PreClusterParamSteps primary key. step_number foreign key, smallint Order of operations. PreClusterParamSet dict PreClusterParamSet to be used in pre-clustering. Source code in element_array_ephys/ephys_precluster.py 422 423 424 425 426 427 428 429 430 431 432 433 434 435 class Step ( dj . Part ): \"\"\"Define the order of operations for parameter sets. Attributes: PreClusterParamSteps (foreign key): PreClusterParamSteps primary key. step_number (foreign key, smallint): Order of operations. PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreClusterParamSet \"\"\" PreClusterTask \u00b6 Bases: dj . Manual Defines a pre-clusting task ready to be run. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. PreclusterParamSteps foreign key PreClusterParam Steps primary key. precluster_output_dir varchar (255) relative path to directory for storing results of pre-clustering. task_mode enum none (no pre-clustering), load results from file, or trigger automated pre-clustering. Source code in element_array_ephys/ephys_precluster.py 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 @schema class PreClusterTask ( dj . Manual ): \"\"\"Defines a pre-clusting task ready to be run. Attributes: EphysRecording (foreign key): EphysRecording primary key. PreclusterParamSteps (foreign key): PreClusterParam Steps primary key. precluster_output_dir (varchar(255) ): relative path to directory for storing results of pre-clustering. task_mode (enum ): `none` (no pre-clustering), `load` results from file, or `trigger` automated pre-clustering. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> PreClusterParamSteps --- precluster_output_dir='': varchar(255) # pre-clustering output directory relative to the root data directory task_mode='none': enum('none','load', 'trigger') # 'none': no pre-clustering analysis # 'load': load analysis results # 'trigger': trigger computation \"\"\" ProbeInsertion \u00b6 Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_precluster.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion implanted into an animal for a given session. -> Session insertion_number: tinyint unsigned --- -> probe.Probe \"\"\" QualityMetrics \u00b6 Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_precluster.py 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) Cluster \u00b6 Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_precluster.py 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" Waveform \u00b6 Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_precluster.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" make ( key ) \u00b6 Populates tables with quality metrics data. Source code in element_array_ephys/ephys_precluster.py 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True ) WaveformSet \u00b6 Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_precluster.py 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) PeakWaveform \u00b6 Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_precluster.py 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" Waveform \u00b6 Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_precluster.py 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" make ( key ) \u00b6 Populates waveform tables. Source code in element_array_ephys/ephys_precluster.py 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True ) activate ( ephys_schema_name , probe_schema_name = None , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. Source code in element_array_ephys/ephys_precluster.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name ) generate_electrode_config ( probe_type , electrodes ) \u00b6 Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_precluster.py 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key get_ephys_root_data_dir () \u00b6 Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_precluster.py 70 71 72 73 74 75 76 77 78 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" return _linking_module . get_ephys_root_data_dir () get_neuropixels_channel2electrode_map ( ephys_recording_key , acq_software ) \u00b6 Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_precluster.py 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) probe_serial_number = ( ProbeInsertion & ephys_recording_key ) . fetch1 ( \"probe\" ) probe_dataset = openephys_dataset . probes [ probe_serial_number ] electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_ids\" ] } return channel2electrode_map get_session_directory ( session_key ) \u00b6 Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_precluster.py 81 82 83 84 85 86 87 88 89 90 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key ) get_spikeglx_meta_filepath ( ephys_recording_key ) \u00b6 Get spikeGLX data filepath. Source code in element_array_ephys/ephys_precluster.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "ephys_precluster.py"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.AcquisitionSoftware", "text": "Bases: dj . Lookup Name of software used for recording electrophysiological data. Attributes: Name Type Description acq_software varchar(24) Acquisition software, e.g,. SpikeGLX, OpenEphys Source code in element_array_ephys/ephys_precluster.py 96 97 98 99 100 101 102 103 104 105 106 107 @schema class AcquisitionSoftware ( dj . Lookup ): \"\"\"Name of software used for recording electrophysiological data. Attributes: acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys \"\"\" definition = \"\"\" # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys acq_software: varchar(24) \"\"\" contents = zip ([ \"SpikeGLX\" , \"Open Ephys\" ])", "title": "AcquisitionSoftware"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusterQualityLabel", "text": "Bases: dj . Lookup Quality label for each spike sorted cluster. Attributes: Name Type Description cluster_quality_label foreign key, varchar(100) Cluster quality type. cluster_quality_description varchar (4000) Description of the cluster quality type. Source code in element_array_ephys/ephys_precluster.py 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 @schema class ClusterQualityLabel ( dj . Lookup ): \"\"\"Quality label for each spike sorted cluster. Attributes: cluster_quality_label (foreign key, varchar(100) ): Cluster quality type. cluster_quality_description (varchar(4000) ): Description of the cluster quality type. \"\"\" definition = \"\"\" # Quality cluster_quality_label: varchar(100) --- cluster_quality_description: varchar(4000) \"\"\" contents = [ ( \"good\" , \"single unit\" ), ( \"ok\" , \"probably a single unit, but could be contaminated\" ), ( \"mua\" , \"multi-unit activity\" ), ( \"noise\" , \"bad unit\" ), ]", "title": "ClusterQualityLabel"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering", "text": "Bases: dj . Imported A processing table to handle each clustering task. Attributes: Name Type Description ClusteringTask foreign key ClusteringTask primary key. clustering_time datetime Time when clustering results are generated. package_version varchar (16) Package version used for a clustering analysis. Source code in element_array_ephys/ephys_precluster.py 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 @schema class Clustering ( dj . Imported ): \"\"\"A processing table to handle each clustering task. Attributes: ClusteringTask (foreign key): ClusteringTask primary key. clustering_time (datetime): Time when clustering results are generated. package_version (varchar(16) ): Package version used for a clustering analysis. \"\"\" definition = \"\"\" # Clustering Procedure -> ClusteringTask --- clustering_time: datetime # time of generation of this set of clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "Clustering"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering.make", "text": "Triggers or imports clustering analysis. Source code in element_array_ephys/ephys_precluster.py 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 def make ( self , key ): \"\"\"Triggers or imports clustering analysis.\"\"\" task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"load\" : kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) # check if the directory is a valid Kilosort output creation_time , _ , _ = kilosort . extract_clustering_info ( kilosort_dir ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" clustering analysis is not yet supported\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"clustering_time\" : creation_time })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringMethod", "text": "Bases: dj . Lookup Kilosort clustering method. Attributes: Name Type Description clustering_method foreign key, varchar(16) Kilosort clustering method. clustering_methods_desc varchar (1000) Additional description of the clustering method. Source code in element_array_ephys/ephys_precluster.py 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 @schema class ClusteringMethod ( dj . Lookup ): \"\"\"Kilosort clustering method. Attributes: clustering_method (foreign key, varchar(16) ): Kilosort clustering method. clustering_methods_desc (varchar(1000) ): Additional description of the clustering method. \"\"\" definition = \"\"\" # Method for clustering clustering_method: varchar(16) --- clustering_method_desc: varchar(1000) \"\"\" contents = [ ( \"kilosort\" , \"kilosort clustering method\" ), ( \"kilosort2\" , \"kilosort2 clustering method\" ), ]", "title": "ClusteringMethod"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringParamSet", "text": "Bases: dj . Lookup Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) Source code in element_array_ephys/ephys_precluster.py 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 @schema class ClusteringParamSet ( dj . Lookup ): \"\"\"Parameters to be used in clustering procedure for spike sorting. Attributes: paramset_idx (foreign key): Unique ID for the clustering parameter set. ClusteringMethod (dict): ClusteringMethod primary key. paramset_desc (varchar(128) ): Description of the clustering parameter set. param_set_hash (uuid): UUID hash for the parameter set. params (longblob) \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> ClusteringMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" param_dict = { \"clustering_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set\" \" already exists - paramset_idx: {} \" . format ( existing_paramset_idx ) ) else : cls . insert1 ( param_dict )", "title": "ClusteringParamSet"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringParamSet.insert_new_params", "text": "Inserts new parameters into the ClusteringParamSet table. Parameters: Name Type Description Default clustering_method str name of the clustering method. required paramset_desc str description of the parameter set required params dict clustering parameters required paramset_idx int Unique parameter set ID. Defaults to None. required Source code in element_array_ephys/ephys_precluster.py 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 @classmethod def insert_new_params ( cls , processing_method : str , paramset_idx : int , paramset_desc : str , params : dict ): \"\"\"Inserts new parameters into the ClusteringParamSet table. Args: clustering_method (str): name of the clustering method. paramset_desc (str): description of the parameter set params (dict): clustering parameters paramset_idx (int, optional): Unique parameter set ID. Defaults to None. \"\"\" param_dict = { \"clustering_method\" : processing_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set\" \" already exists - paramset_idx: {} \" . format ( existing_paramset_idx ) ) else : cls . insert1 ( param_dict )", "title": "insert_new_params()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringTask", "text": "Bases: dj . Manual A clustering task to spike sort electrophysiology datasets. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. ClusteringParamSet foreign key ClusteringParamSet primary key. clustering_outdir_dir varchar (255) Relative path to output clustering results. task_mode enum Trigger computes clustering or and load imports existing data. Source code in element_array_ephys/ephys_precluster.py 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 @schema class ClusteringTask ( dj . Manual ): \"\"\"A clustering task to spike sort electrophysiology datasets. Attributes: EphysRecording (foreign key): EphysRecording primary key. ClusteringParamSet (foreign key): ClusteringParamSet primary key. clustering_outdir_dir (varchar (255) ): Relative path to output clustering results. task_mode (enum): `Trigger` computes clustering or and `load` imports existing data. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> PreCluster -> ClusteringParamSet --- clustering_output_dir: varchar(255) # clustering output directory relative to the clustering root data directory task_mode='load': enum('load', 'trigger') # 'load': load computed analysis results, 'trigger': trigger computation \"\"\"", "title": "ClusteringTask"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering", "text": "Bases: dj . Imported Clustering results after curation. Attributes: Name Type Description Curation foreign key Curation primary key. Source code in element_array_ephys/ephys_precluster.py 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 @schema class CuratedClustering ( dj . Imported ): \"\"\"Clustering results after curation. Attributes: Curation (foreign key): Curation primary key. \"\"\" definition = \"\"\" # Clustering results of a curation. -> Curation \"\"\" class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\" def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / kilosort_dataset . data [ \"params\" ][ \"sample_rate\" ] ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "CuratedClustering"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering.Unit", "text": "Bases: dj . Part Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. Source code in element_array_ephys/ephys_precluster.py 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 class Unit ( dj . Part ): \"\"\"Single unit properties after clustering and curation. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. unit (foreign key, int): Unique integer identifying a single unit. probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key. ClusteringQualityLabel (dict): CLusteringQualityLabel primary key. spike_count (int): Number of spikes in this recording for this unit. spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording. spike_sites (longblob): Array of electrode associated with each spike. spike_depths (longblob): Array of depths associated with each spike, relative to each spike. \"\"\" definition = \"\"\" # Properties of a given unit from a round of clustering (and curation) -> master unit: int --- -> probe.ElectrodeConfig.Electrode # electrode with highest waveform amplitude for this unit -> ClusterQualityLabel spike_count: int # how many spikes in this recording for this unit spike_times: longblob # (s) spike times of this unit, relative to the start of the EphysRecording spike_sites : longblob # array of electrode associated with each spike spike_depths=null : longblob # (um) array of depths associated with each spike, relative to the (0, 0) of the probe \"\"\"", "title": "Unit"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering.make", "text": "Automated population of Unit information. Source code in element_array_ephys/ephys_precluster.py 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 def make ( self , key ): \"\"\"Automated population of Unit information. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) # ---------- Unit ---------- # -- Remove 0-spike units withspike_idx = [ i for i , u in enumerate ( kilosort_dataset . data [ \"cluster_ids\" ]) if ( kilosort_dataset . data [ \"spike_clusters\" ] == u ) . any () ] valid_units = kilosort_dataset . data [ \"cluster_ids\" ][ withspike_idx ] valid_unit_labels = kilosort_dataset . data [ \"cluster_groups\" ][ withspike_idx ] # -- Get channel and electrode-site mapping channel2electrodes = get_neuropixels_channel2electrode_map ( key , acq_software ) # -- Spike-times -- # spike_times_sec_adj > spike_times_sec > spike_times spike_time_key = ( \"spike_times_sec_adj\" if \"spike_times_sec_adj\" in kilosort_dataset . data else \"spike_times_sec\" if \"spike_times_sec\" in kilosort_dataset . data else \"spike_times\" ) spike_times = kilosort_dataset . data [ spike_time_key ] kilosort_dataset . extract_spike_depths () # -- Spike-sites and Spike-depths -- spike_sites = np . array ( [ channel2electrodes [ s ][ \"electrode\" ] for s in kilosort_dataset . data [ \"spike_sites\" ] ] ) spike_depths = kilosort_dataset . data [ \"spike_depths\" ] # -- Insert unit, label, peak-chn units = [] for unit , unit_lbl in zip ( valid_units , valid_unit_labels ): if ( kilosort_dataset . data [ \"spike_clusters\" ] == unit ) . any (): unit_channel , _ = kilosort_dataset . get_best_channel ( unit ) unit_spike_times = ( spike_times [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] / kilosort_dataset . data [ \"params\" ][ \"sample_rate\" ] ) spike_count = len ( unit_spike_times ) units . append ( { \"unit\" : unit , \"cluster_quality_label\" : unit_lbl , ** channel2electrodes [ unit_channel ], \"spike_times\" : unit_spike_times , \"spike_count\" : spike_count , \"spike_sites\" : spike_sites [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ], \"spike_depths\" : spike_depths [ kilosort_dataset . data [ \"spike_clusters\" ] == unit ] if spike_depths is not None else None , } ) self . insert1 ( key ) self . Unit . insert ([{ ** key , ** u } for u in units ])", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation", "text": "Bases: dj . Manual Curation procedure table. Attributes: Name Type Description Clustering foreign key Clustering primary key. curation_id foreign key, int Unique curation ID. curation_time datetime Time when curation results are generated. curation_output_dir varchar (255) Output directory of the curated results. quality_control bool If True, this clustering result has undergone quality control. manual_curation bool If True, manual curation has been performed on this clustering result. curation_note varchar (2000) Notes about the curation task. Source code in element_array_ephys/ephys_precluster.py 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 @schema class Curation ( dj . Manual ): \"\"\"Curation procedure table. Attributes: Clustering (foreign key): Clustering primary key. curation_id (foreign key, int): Unique curation ID. curation_time (datetime): Time when curation results are generated. curation_output_dir (varchar(255) ): Output directory of the curated results. quality_control (bool): If True, this clustering result has undergone quality control. manual_curation (bool): If True, manual curation has been performed on this clustering result. curation_note (varchar(2000) ): Notes about the curation task. \"\"\" definition = \"\"\" # Manual curation procedure -> Clustering curation_id: int --- curation_time: datetime # time of generation of this set of curated clustering results curation_output_dir: varchar(255) # output directory of the curated results, relative to root data directory quality_control: bool # has this clustering result undergone quality control? manual_curation: bool # has manual curation been performed on this clustering result? curation_note='': varchar(2000) \"\"\" def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "Curation"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation.create1_from_clustering_task", "text": "A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" Source code in element_array_ephys/ephys_precluster.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 def create1_from_clustering_task ( self , key , curation_note : str = \"\" ): \"\"\" A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\" \"\"\" if key not in Clustering (): raise ValueError ( f \"No corresponding entry in Clustering available\" f \" for: { key } ; do `Clustering.populate(key)`\" ) task_mode , output_dir = ( ClusteringTask & key ) . fetch1 ( \"task_mode\" , \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) creation_time , is_curated , is_qc = kilosort . extract_clustering_info ( kilosort_dir ) # Synthesize curation_id curation_id = ( dj . U () . aggr ( self & key , n = \"ifnull(max(curation_id)+1,1)\" ) . fetch1 ( \"n\" ) ) self . insert1 ( { ** key , \"curation_id\" : curation_id , \"curation_time\" : creation_time , \"curation_output_dir\" : output_dir , \"quality_control\" : is_qc , \"manual_curation\" : is_curated , \"curation_note\" : curation_note , } )", "title": "create1_from_clustering_task()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording", "text": "Bases: dj . Imported Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: Name Type Description ProbeInsertion foreign key ProbeInsertion primary key. probe.ElectrodeConfig dict probe.ElectrodeConfig primary key. AcquisitionSoftware dict AcquisitionSoftware primary key. sampling_rate float sampling rate of the recording in Hertz (Hz). recording_datetime datetime datetime of the recording from this probe. recording_duration float duration of the entire recording from this probe in seconds. Source code in element_array_ephys/ephys_precluster.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 @schema class EphysRecording ( dj . Imported ): \"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key. AcquisitionSoftware (dict): AcquisitionSoftware primary key. sampling_rate (float): sampling rate of the recording in Hertz (Hz). recording_datetime (datetime): datetime of the recording from this probe. recording_duration (float): duration of the entire recording from this probe in seconds. \"\"\" definition = \"\"\" # Ephys recording from a probe insertion for a given session. -> ProbeInsertion --- -> probe.ElectrodeConfig -> AcquisitionSoftware sampling_rate: float # (Hz) recording_datetime: datetime # datetime of the recording from this probe recording_duration: float # (seconds) duration of the recording from this probe \"\"\" class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\" def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = [ fp for fp in session_dir . rglob ( ephys_pattern )] if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , spikeglx_meta . probe_model ): probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , probe_data . probe_model ): probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_ids\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "EphysRecording"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording.EphysFile", "text": "Bases: dj . Part Paths of electrophysiology recording files for each insertion. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. file_path varchar (255) relative file path for electrophysiology recording. Source code in element_array_ephys/ephys_precluster.py 182 183 184 185 186 187 188 189 190 191 192 193 194 class EphysFile ( dj . Part ): \"\"\"Paths of electrophysiology recording files for each insertion. Attributes: EphysRecording (foreign key): EphysRecording primary key. file_path (varchar(255) ): relative file path for electrophysiology recording. \"\"\" definition = \"\"\" # Paths of files of a given EphysRecording round. -> master file_path: varchar(255) # filepath relative to root data directory \"\"\"", "title": "EphysFile"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording.make", "text": "Populates table with electrophysiology recording information. Source code in element_array_ephys/ephys_precluster.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def make ( self , key ): \"\"\"Populates table with electrophysiology recording information. \"\"\" session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in ( ( \"*.ap.meta\" , \"SpikeGLX\" ), ( \"*.oebin\" , \"Open Ephys\" ), ): ephys_meta_filepaths = [ fp for fp in session_dir . rglob ( ephys_pattern )] if ephys_meta_filepaths : acq_software = ephys_acq_type break else : raise FileNotFoundError ( f \"Ephys recording data not found!\" f \" Neither SpikeGLX nor Open Ephys recording files found\" f \" in { session_dir } \" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , spikeglx_meta . probe_model ): probe_type = spikeglx_meta . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } electrode_group_members = [ probe_electrodes [( shank , shank_col , shank_row )] for shank , shank_col , shank_row , _ in spikeglx_meta . shankmap [ \"data\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels probe model\" \" {} not yet implemented\" . format ( spikeglx_meta . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : spikeglx_meta . meta [ \"imSampRate\" ], \"recording_datetime\" : spikeglx_meta . recording_time , \"recording_duration\" : ( spikeglx_meta . recording_duration or spikeglx . retrieve_recording_duration ( meta_filepath ) ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), meta_filepath ) self . EphysFile . insert1 ( { ** key , \"file_path\" : meta_filepath . relative_to ( root_dir ) . as_posix ()} ) elif acq_software == \"Open Ephys\" : dataset = openephys . OpenEphys ( session_dir ) for serial_number , probe_data in dataset . probes . items (): if str ( serial_number ) == inserted_probe_serial_number : break else : raise FileNotFoundError ( \"No Open Ephys data found for probe insertion: {} \" . format ( key ) ) if re . search ( \"(1.0|2.0)\" , probe_data . probe_model ): probe_type = probe_data . probe_model electrode_query = probe . ProbeType . Electrode & { \"probe_type\" : probe_type } probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } electrode_group_members = [ probe_electrodes [ channel_idx ] for channel_idx in probe_data . ap_meta [ \"channels_ids\" ] ] else : raise NotImplementedError ( \"Processing for neuropixels\" \" probe model {} not yet implemented\" . format ( probe_data . probe_model ) ) self . insert1 ( { ** key , ** generate_electrode_config ( probe_type , electrode_group_members ), \"acq_software\" : acq_software , \"sampling_rate\" : probe_data . ap_meta [ \"sample_rate\" ], \"recording_datetime\" : probe_data . recording_info [ \"recording_datetimes\" ][ 0 ], \"recording_duration\" : np . sum ( probe_data . recording_info [ \"recording_durations\" ] ), } ) root_dir = find_root_directory ( get_ephys_root_data_dir (), probe_data . recording_info [ \"recording_files\" ][ 0 ], ) self . EphysFile . insert ( [ { ** key , \"file_path\" : fp . relative_to ( root_dir ) . as_posix ()} for fp in probe_data . recording_info [ \"recording_files\" ] ] ) else : raise NotImplementedError ( f \"Processing ephys files from\" f \" acquisition software of type { acq_software } is\" f \" not yet implemented\" )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.InsertionLocation", "text": "Bases: dj . Manual Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis Source code in element_array_ephys/ephys_precluster.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 @schema class InsertionLocation ( dj . Manual ): \"\"\"Stereotaxic location information for each probe insertion. Attributes: ProbeInsertion (foreign key): ProbeInsertion primary key. SkullReference (dict): SkullReference primary key. ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive. ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive. depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative. Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis. phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis \"\"\" definition = \"\"\" # Brain Location of a given probe insertion. -> ProbeInsertion --- -> SkullReference ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive depth: decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative theta=null: decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis phi=null: decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis beta=null: decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior \"\"\"", "title": "InsertionLocation"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP", "text": "Bases: dj . Imported Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. lfp_sampling_rate float Sampling rate for LFPs in Hz. lfp_time_stamps longblob Time stamps with respect to the start of the recording. lfp_mean longblob Overall mean LFP across electrodes. Source code in element_array_ephys/ephys_precluster.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 @schema class LFP ( dj . Imported ): \"\"\"Extracts local field potentials (LFP) from an electrophysiology recording. Attributes: EphysRecording (foreign key): EphysRecording primary key. lfp_sampling_rate (float): Sampling rate for LFPs in Hz. lfp_time_stamps (longblob): Time stamps with respect to the start of the recording. lfp_mean (longblob): Overall mean LFP across electrodes. \"\"\" definition = \"\"\" # Acquired local field potential (LFP) from a given Ephys recording. -> PreCluster --- lfp_sampling_rate: float # (Hz) lfp_time_stamps: longblob # (s) timestamps with respect to the start of the recording (recording_timestamp) lfp_mean: longblob # (uV) mean of LFP across electrodes - shape (time,) \"\"\" class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\" # Only store LFP for every 9th channel, due to high channel density, # close-by channels exhibit highly similar LFP _skip_channel_counts = 9 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software , probe_sn = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) oe_probe = loaded_oe . probes [ probe_sn ] lfp_channel_ind = np . arange ( len ( oe_probe . lfp_meta [ \"channels_ids\" ]))[ - 1 :: - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } for channel_idx in np . array ( oe_probe . lfp_meta [ \"channels_ids\" ])[ lfp_channel_ind ]: electrode_keys . append ( probe_electrodes [ channel_idx ]) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "LFP"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP.Electrode", "text": "Bases: dj . Part Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. Source code in element_array_ephys/ephys_precluster.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 class Electrode ( dj . Part ): \"\"\"Saves local field potential data for each electrode. Attributes: LFP (foreign key): LFP primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. lfp (longblob): LFP recording at this electrode in microvolts. \"\"\" definition = \"\"\" -> master -> probe.ElectrodeConfig.Electrode --- lfp: longblob # (uV) recorded lfp at this electrode \"\"\"", "title": "Electrode"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP.make", "text": "Populates the LFP tables. Source code in element_array_ephys/ephys_precluster.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 def make ( self , key ): \"\"\"Populates the LFP tables. \"\"\" acq_software , probe_sn = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) electrode_keys , lfp = [], [] if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) spikeglx_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) lfp_channel_ind = spikeglx_recording . lfmeta . recording_channels [ - 1 :: - self . _skip_channel_counts ] # Extract LFP data at specified channels and convert to uV lfp = spikeglx_recording . lf_timeseries [ :, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * spikeglx_recording . get_channel_bit_volts ( \"lf\" )[ lfp_channel_ind ] ) . T # (channel x sample) self . insert1 ( dict ( key , lfp_sampling_rate = spikeglx_recording . lfmeta . meta [ \"imSampRate\" ], lfp_time_stamps = ( np . arange ( lfp . shape [ 1 ]) / spikeglx_recording . lfmeta . meta [ \"imSampRate\" ] ), lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } for recorded_site in lfp_channel_ind : shank , shank_col , shank_row , _ = spikeglx_recording . apmeta . shankmap [ \"data\" ][ recorded_site ] electrode_keys . append ( probe_electrodes [( shank , shank_col , shank_row )]) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) loaded_oe = openephys . OpenEphys ( session_dir ) oe_probe = loaded_oe . probes [ probe_sn ] lfp_channel_ind = np . arange ( len ( oe_probe . lfp_meta [ \"channels_ids\" ]))[ - 1 :: - self . _skip_channel_counts ] lfp = oe_probe . lfp_timeseries [:, lfp_channel_ind ] # (sample x channel) lfp = ( lfp * np . array ( oe_probe . lfp_meta [ \"channels_gains\" ])[ lfp_channel_ind ] ) . T # (channel x sample) lfp_timestamps = oe_probe . lfp_timestamps self . insert1 ( dict ( key , lfp_sampling_rate = oe_probe . lfp_meta [ \"sample_rate\" ], lfp_time_stamps = lfp_timestamps , lfp_mean = lfp . mean ( axis = 0 ), ) ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } for channel_idx in np . array ( oe_probe . lfp_meta [ \"channels_ids\" ])[ lfp_channel_ind ]: electrode_keys . append ( probe_electrodes [ channel_idx ]) else : raise NotImplementedError ( f \"LFP extraction from acquisition software\" f \" of type { acq_software } is not yet implemented\" ) # single insert in loop to mitigate potential memory issue for electrode_key , lfp_trace in zip ( electrode_keys , lfp ): self . Electrode . insert1 ({ ** key , ** electrode_key , \"lfp\" : lfp_trace })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster", "text": "Bases: dj . Imported A processing table to handle each PreClusterTask: Attributes: Name Type Description PreClusterTask foreign key PreClusterTask primary key. precluster_time datetime Time of generation of this set of pre-clustering results. package_version varchar (16) Package version used for performing pre-clustering. Source code in element_array_ephys/ephys_precluster.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 @schema class PreCluster ( dj . Imported ): \"\"\" A processing table to handle each PreClusterTask: Attributes: PreClusterTask (foreign key): PreClusterTask primary key. precluster_time (datetime): Time of generation of this set of pre-clustering results. package_version (varchar(16) ): Package version used for performing pre-clustering. \"\"\" definition = \"\"\" -> PreClusterTask --- precluster_time: datetime # time of generation of this set of pre-clustering results package_version='': varchar(16) \"\"\" def make ( self , key ): \"\"\"Populate pre-clustering tables.\"\"\" task_mode , output_dir = ( PreClusterTask & key ) . fetch1 ( \"task_mode\" , \"precluster_output_dir\" ) precluster_output_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"none\" : if len (( PreClusterParamSteps . Step & key ) . fetch ()) > 0 : raise ValueError ( \"There are entries in the PreClusterParamSteps.Step \" \"table and task_mode=none\" ) creation_time = ( EphysRecording & key ) . fetch1 ( \"recording_datetime\" ) elif task_mode == \"load\" : acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in precluster_output_dir . rglob ( \"*.ap.meta\" ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : creation_time = spikeglx_meta . recording_time break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) else : raise NotImplementedError ( f \"Pre-clustering analysis of { acq_software } \" \"is not yet supported.\" ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" pre-clustering analysis is not yet supported.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"precluster_time\" : creation_time })", "title": "PreCluster"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster.make", "text": "Populate pre-clustering tables. Source code in element_array_ephys/ephys_precluster.py 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def make ( self , key ): \"\"\"Populate pre-clustering tables.\"\"\" task_mode , output_dir = ( PreClusterTask & key ) . fetch1 ( \"task_mode\" , \"precluster_output_dir\" ) precluster_output_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) if task_mode == \"none\" : if len (( PreClusterParamSteps . Step & key ) . fetch ()) > 0 : raise ValueError ( \"There are entries in the PreClusterParamSteps.Step \" \"table and task_mode=none\" ) creation_time = ( EphysRecording & key ) . fetch1 ( \"recording_datetime\" ) elif task_mode == \"load\" : acq_software = ( EphysRecording & key ) . fetch1 ( \"acq_software\" ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & key ) . fetch1 ( \"probe\" ) if acq_software == \"SpikeGLX\" : for meta_filepath in precluster_output_dir . rglob ( \"*.ap.meta\" ): spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : creation_time = spikeglx_meta . recording_time break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( key ) ) else : raise NotImplementedError ( f \"Pre-clustering analysis of { acq_software } \" \"is not yet supported.\" ) elif task_mode == \"trigger\" : raise NotImplementedError ( \"Automatic triggering of\" \" pre-clustering analysis is not yet supported.\" ) else : raise ValueError ( f \"Unknown task mode: { task_mode } \" ) self . insert1 ({ ** key , \"precluster_time\" : creation_time })", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterMethod", "text": "Bases: dj . Lookup Pre-clustering method Attributes: precluster_method (foreign key, varchar(16) ): Pre-clustering method for the dataset. precluster_method_desc(varchar(1000) ): Pre-clustering method description. Source code in element_array_ephys/ephys_precluster.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 @schema class PreClusterMethod ( dj . Lookup ): \"\"\"Pre-clustering method Attributes: precluster_method (foreign key, varchar(16) ): Pre-clustering method for the dataset. precluster_method_desc(varchar(1000) ): Pre-clustering method description. \"\"\" definition = \"\"\" # Method for pre-clustering precluster_method: varchar(16) --- precluster_method_desc: varchar(1000) \"\"\" contents = [( \"catgt\" , \"Time shift, Common average referencing, Zeroing\" )]", "title": "PreClusterMethod"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSet", "text": "Bases: dj . Lookup Parameters for the pre-clustering method. Attributes: Name Type Description paramset_idx foreign key Unique parameter set ID. PreClusterMethod dict PreClusterMethod query for this dataset. paramset_desc varchar (128) Description for the pre-clustering parameter set. param_set_hash uuid Unique hash for parameter set. params longblob All parameters for the pre-clustering method. Source code in element_array_ephys/ephys_precluster.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 @schema class PreClusterParamSet ( dj . Lookup ): \"\"\"Parameters for the pre-clustering method. Attributes: paramset_idx (foreign key): Unique parameter set ID. PreClusterMethod (dict): PreClusterMethod query for this dataset. paramset_desc (varchar(128) ): Description for the pre-clustering parameter set. param_set_hash (uuid): Unique hash for parameter set. params (longblob): All parameters for the pre-clustering method. \"\"\" definition = \"\"\" # Parameter set to be used in a clustering procedure paramset_idx: smallint --- -> PreClusterMethod paramset_desc: varchar(128) param_set_hash: uuid unique index (param_set_hash) params: longblob # dictionary of all applicable parameters \"\"\" @classmethod def insert_new_params ( cls , precluster_method : str , paramset_idx : int , paramset_desc : str , params : dict ): param_dict = { \"precluster_method\" : precluster_method , \"paramset_idx\" : paramset_idx , \"paramset_desc\" : paramset_desc , \"params\" : params , \"param_set_hash\" : dict_to_uuid ( params ), } param_query = cls & { \"param_set_hash\" : param_dict [ \"param_set_hash\" ]} if param_query : # If the specified param-set already exists existing_paramset_idx = param_query . fetch1 ( \"paramset_idx\" ) if ( existing_paramset_idx == paramset_idx ): # If the existing set has the same paramset_idx: job done return else : # If not same name: human error, trying to add the same paramset with different name raise dj . DataJointError ( \"The specified param-set\" \" already exists - paramset_idx: {} \" . format ( existing_paramset_idx ) ) else : cls . insert1 ( param_dict )", "title": "PreClusterParamSet"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSteps", "text": "Bases: dj . Manual Ordered list of parameter sets that will be run. Attributes: Name Type Description precluster_param_steps_id foreign key Unique ID for the pre-clustering parameter sets to be run. precluster_param_steps_name varchar (32) User-friendly name for the parameter steps. precluster_param_steps_desc varchar (128) Description of the parameter steps. Source code in element_array_ephys/ephys_precluster.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 @schema class PreClusterParamSteps ( dj . Manual ): \"\"\"Ordered list of parameter sets that will be run. Attributes: precluster_param_steps_id (foreign key): Unique ID for the pre-clustering parameter sets to be run. precluster_param_steps_name (varchar(32) ): User-friendly name for the parameter steps. precluster_param_steps_desc (varchar(128) ): Description of the parameter steps. \"\"\" definition = \"\"\" # Ordered list of paramset_idx that are to be run # When pre-clustering is not performed, do not create an entry in `Step` Part table precluster_param_steps_id: smallint --- precluster_param_steps_name: varchar(32) precluster_param_steps_desc: varchar(128) \"\"\" class Step ( dj . Part ): \"\"\"Define the order of operations for parameter sets. Attributes: PreClusterParamSteps (foreign key): PreClusterParamSteps primary key. step_number (foreign key, smallint): Order of operations. PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreClusterParamSet \"\"\"", "title": "PreClusterParamSteps"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSteps.Step", "text": "Bases: dj . Part Define the order of operations for parameter sets. Attributes: Name Type Description PreClusterParamSteps foreign key PreClusterParamSteps primary key. step_number foreign key, smallint Order of operations. PreClusterParamSet dict PreClusterParamSet to be used in pre-clustering. Source code in element_array_ephys/ephys_precluster.py 422 423 424 425 426 427 428 429 430 431 432 433 434 435 class Step ( dj . Part ): \"\"\"Define the order of operations for parameter sets. Attributes: PreClusterParamSteps (foreign key): PreClusterParamSteps primary key. step_number (foreign key, smallint): Order of operations. PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering. \"\"\" definition = \"\"\" -> master step_number: smallint # Order of operations --- -> PreClusterParamSet \"\"\"", "title": "Step"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterTask", "text": "Bases: dj . Manual Defines a pre-clusting task ready to be run. Attributes: Name Type Description EphysRecording foreign key EphysRecording primary key. PreclusterParamSteps foreign key PreClusterParam Steps primary key. precluster_output_dir varchar (255) relative path to directory for storing results of pre-clustering. task_mode enum none (no pre-clustering), load results from file, or trigger automated pre-clustering. Source code in element_array_ephys/ephys_precluster.py 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 @schema class PreClusterTask ( dj . Manual ): \"\"\"Defines a pre-clusting task ready to be run. Attributes: EphysRecording (foreign key): EphysRecording primary key. PreclusterParamSteps (foreign key): PreClusterParam Steps primary key. precluster_output_dir (varchar(255) ): relative path to directory for storing results of pre-clustering. task_mode (enum ): `none` (no pre-clustering), `load` results from file, or `trigger` automated pre-clustering. \"\"\" definition = \"\"\" # Manual table for defining a clustering task ready to be run -> EphysRecording -> PreClusterParamSteps --- precluster_output_dir='': varchar(255) # pre-clustering output directory relative to the root data directory task_mode='none': enum('none','load', 'trigger') # 'none': no pre-clustering analysis # 'load': load analysis results # 'trigger': trigger computation \"\"\"", "title": "PreClusterTask"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ProbeInsertion", "text": "Bases: dj . Manual Information about probe insertion across subjects and sessions. Attributes: Name Type Description Session foreign key Session primary key. insertion_number foreign key, str Unique insertion number for each probe insertion for a given session. probe.Probe str probe.Probe primary key. Source code in element_array_ephys/ephys_precluster.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 @schema class ProbeInsertion ( dj . Manual ): \"\"\"Information about probe insertion across subjects and sessions. Attributes: Session (foreign key): Session primary key. insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session. probe.Probe (str): probe.Probe primary key. \"\"\" definition = \"\"\" # Probe insertion implanted into an animal for a given session. -> Session insertion_number: tinyint unsigned --- -> probe.Probe \"\"\"", "title": "ProbeInsertion"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics", "text": "Bases: dj . Imported Clustering and waveform quality metrics. Attributes: Name Type Description CuratedClustering foreign key CuratedClustering primary key. Source code in element_array_ephys/ephys_precluster.py 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 @schema class QualityMetrics ( dj . Imported ): \"\"\"Clustering and waveform quality metrics. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # Clusters and waveforms metrics -> CuratedClustering \"\"\" class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\" class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\" def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "QualityMetrics"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.Cluster", "text": "Bases: dj . Part Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. Source code in element_array_ephys/ephys_precluster.py 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 class Cluster ( dj . Part ): \"\"\"Cluster metrics for a unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. firing_rate (float): Firing rate of the unit. snr (float): Signal-to-noise ratio for a unit. presence_ratio (float): Fraction of time where spikes are present. isi_violation (float): rate of ISI violation as a fraction of overall rate. number_violation (int): Total ISI violations. amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram. isolation_distance (float): Distance to nearest cluster. l_ratio (float): Amount of empty space between a cluster and other spikes in dataset. d_prime (float): Classification accuracy based on LDA. nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster. nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster. silhouette_core (float): Maximum change in spike depth throughout recording. cumulative_drift (float): Cumulative change in spike depth throughout recording. contamination_rate (float): Frequency of spikes in the refractory period. \"\"\" definition = \"\"\" # Cluster metrics for a particular unit -> master -> CuratedClustering.Unit --- firing_rate=null: float # (Hz) firing rate for a unit snr=null: float # signal-to-noise ratio for a unit presence_ratio=null: float # fraction of time in which spikes are present isi_violation=null: float # rate of ISI violation as a fraction of overall rate number_violation=null: int # total number of ISI violations amplitude_cutoff=null: float # estimate of miss rate based on amplitude histogram isolation_distance=null: float # distance to nearest cluster in Mahalanobis space l_ratio=null: float # d_prime=null: float # Classification accuracy based on LDA nn_hit_rate=null: float # Fraction of neighbors for target cluster that are also in target cluster nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster silhouette_score=null: float # Standard metric for cluster overlap max_drift=null: float # Maximum change in spike depth throughout recording cumulative_drift=null: float # Cumulative change in spike depth throughout recording contamination_rate=null: float # \"\"\"", "title": "Cluster"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.Waveform", "text": "Bases: dj . Part Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. Source code in element_array_ephys/ephys_precluster.py 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 class Waveform ( dj . Part ): \"\"\"Waveform metrics for a particular unit. Attributes: QualityMetrics (foreign key): QualityMetrics primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. amplitude (float): Absolute difference between waveform peak and trough in microvolts. duration (float): Time between waveform peak and trough in milliseconds. halfwidth (float): Spike width at half max amplitude. pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0. repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak. recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail. spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe. velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe. velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe. \"\"\" definition = \"\"\" # Waveform metrics for a particular unit -> master -> CuratedClustering.Unit --- amplitude: float # (uV) absolute difference between waveform peak and trough duration: float # (ms) time between waveform peak and trough halfwidth=null: float # (ms) spike width at half max amplitude pt_ratio=null: float # absolute amplitude of peak divided by absolute amplitude of trough relative to 0 repolarization_slope=null: float # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak recovery_slope=null: float # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail spread=null: float # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe velocity_above=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe velocity_below=null: float # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.make", "text": "Populates tables with quality metrics data. Source code in element_array_ephys/ephys_precluster.py 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 def make ( self , key ): \"\"\"Populates tables with quality metrics data. \"\"\" output_dir = ( ClusteringTask & key ) . fetch1 ( \"clustering_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) metric_fp = kilosort_dir / \"metrics.csv\" if not metric_fp . exists (): raise FileNotFoundError ( f \"QC metrics file not found: { metric_fp } \" ) metrics_df = pd . read_csv ( metric_fp ) metrics_df . set_index ( \"cluster_id\" , inplace = True ) metrics_list = [ dict ( metrics_df . loc [ unit_key [ \"unit\" ]], ** unit_key ) for unit_key in ( CuratedClustering . Unit & key ) . fetch ( \"KEY\" ) ] self . insert1 ( key ) self . Cluster . insert ( metrics_list , ignore_extra_fields = True ) self . Waveform . insert ( metrics_list , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet", "text": "Bases: dj . Imported A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. Source code in element_array_ephys/ephys_precluster.py 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 @schema class WaveformSet ( dj . Imported ): \"\"\"A set of spike waveforms for units out of a given CuratedClustering. Attributes: CuratedClustering (foreign key): CuratedClustering primary key. \"\"\" definition = \"\"\" # A set of spike waveforms for units out of a given CuratedClustering -> CuratedClustering \"\"\" class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\" class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\" def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "WaveformSet"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.PeakWaveform", "text": "Bases: dj . Part Mean waveform across spikes for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. peak_electrode_waveform longblob Mean waveform for a given unit at its representative electrode. Source code in element_array_ephys/ephys_precluster.py 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 class PeakWaveform ( dj . Part ): \"\"\"Mean waveform across spikes for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode. \"\"\" definition = \"\"\" # Mean waveform across spikes for a given unit at its representative electrode -> master -> CuratedClustering.Unit --- peak_electrode_waveform: longblob # (uV) mean waveform for a given unit at its representative electrode \"\"\"", "title": "PeakWaveform"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.Waveform", "text": "Bases: dj . Part Spike waveforms for a given unit. Attributes: Name Type Description WaveformSet foreign key WaveformSet primary key. CuratedClustering.Unit foreign key CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode foreign key probe.ElectrodeConfig.Electrode primary key. waveform_mean longblob mean waveform across spikes of the unit in microvolts. waveforms longblob waveforms of a sampling of spikes at the given electrode and unit. Source code in element_array_ephys/ephys_precluster.py 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 class Waveform ( dj . Part ): \"\"\"Spike waveforms for a given unit. Attributes: WaveformSet (foreign key): WaveformSet primary key. CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key. probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key. waveform_mean (longblob): mean waveform across spikes of the unit in microvolts. waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit. \"\"\" definition = \"\"\" # Spike waveforms and their mean across spikes for the given unit -> master -> CuratedClustering.Unit -> probe.ElectrodeConfig.Electrode --- waveform_mean: longblob # (uV) mean waveform across spikes of the given unit waveforms=null: longblob # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit \"\"\"", "title": "Waveform"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.make", "text": "Populates waveform tables. Source code in element_array_ephys/ephys_precluster.py 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 def make ( self , key ): \"\"\"Populates waveform tables. \"\"\" output_dir = ( Curation & key ) . fetch1 ( \"curation_output_dir\" ) kilosort_dir = find_full_path ( get_ephys_root_data_dir (), output_dir ) kilosort_dataset = kilosort . Kilosort ( kilosort_dir ) acq_software , probe_serial_number = ( EphysRecording * ProbeInsertion & key ) . fetch1 ( \"acq_software\" , \"probe\" ) # -- Get channel and electrode-site mapping recording_key = ( EphysRecording & key ) . fetch1 ( \"KEY\" ) channel2electrodes = get_neuropixels_channel2electrode_map ( recording_key , acq_software ) is_qc = ( Curation & key ) . fetch1 ( \"quality_control\" ) # Get all units units = { u [ \"unit\" ]: u for u in ( CuratedClustering . Unit & key ) . fetch ( as_dict = True , order_by = \"unit\" ) } if is_qc : unit_waveforms = np . load ( kilosort_dir / \"mean_waveforms.npy\" ) # unit x channel x sample def yield_unit_waveforms (): for unit_no , unit_waveform in zip ( kilosort_dataset . data [ \"cluster_ids\" ], unit_waveforms ): unit_peak_waveform = {} unit_electrode_waveforms = [] if unit_no in units : for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], unit_waveform ): unit_electrode_waveforms . append ( { ** units [ unit_no ], ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == units [ unit_no ][ \"electrode\" ] ): unit_peak_waveform = { ** units [ unit_no ], \"peak_electrode_waveform\" : channel_waveform , } yield unit_peak_waveform , unit_electrode_waveforms else : if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( key ) neuropixels_recording = spikeglx . SpikeGLX ( spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) neuropixels_recording = openephys_dataset . probes [ probe_serial_number ] def yield_unit_waveforms (): for unit_dict in units . values (): unit_peak_waveform = {} unit_electrode_waveforms = [] spikes = unit_dict [ \"spike_times\" ] waveforms = neuropixels_recording . extract_spike_waveforms ( spikes , kilosort_dataset . data [ \"channel_map\" ] ) # (sample x channel x spike) waveforms = waveforms . transpose ( ( 1 , 2 , 0 ) ) # (channel x spike x sample) for channel , channel_waveform in zip ( kilosort_dataset . data [ \"channel_map\" ], waveforms ): unit_electrode_waveforms . append ( { ** unit_dict , ** channel2electrodes [ channel ], \"waveform_mean\" : channel_waveform . mean ( axis = 0 ), \"waveforms\" : channel_waveform , } ) if ( channel2electrodes [ channel ][ \"electrode\" ] == unit_dict [ \"electrode\" ] ): unit_peak_waveform = { ** unit_dict , \"peak_electrode_waveform\" : channel_waveform . mean ( axis = 0 ), } yield unit_peak_waveform , unit_electrode_waveforms # insert waveform on a per-unit basis to mitigate potential memory issue self . insert1 ( key ) for unit_peak_waveform , unit_electrode_waveforms in yield_unit_waveforms (): self . PeakWaveform . insert1 ( unit_peak_waveform , ignore_extra_fields = True ) self . Waveform . insert ( unit_electrode_waveforms , ignore_extra_fields = True )", "title": "make()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.activate", "text": "Activates the ephys and probe schemas. Parameters: Name Type Description Default ephys_schema_name str A string containing the name of the ephys schema. required probe_schema_name str A string containing the name of the probe scehma. None create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True linking_module str A string containing the module name or module containing the required dependencies to activate the schema. None Dependencies: Upstream tables Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. Source code in element_array_ephys/ephys_precluster.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def activate ( ephys_schema_name : str , probe_schema_name : str = None , * , create_schema : bool = True , create_tables : bool = True , linking_module : str = None ): \"\"\"Activates the `ephys` and `probe` schemas. Args: ephys_schema_name (str): A string containing the name of the ephys schema. probe_schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported. Functions: get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), \"The argument 'dependency' must be a module's name or a module\" global _linking_module _linking_module = linking_module probe . activate ( probe_schema_name , create_schema = create_schema , create_tables = create_tables ) schema . activate ( ephys_schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = _linking_module . __dict__ , ) ephys_report . activate ( f \" { ephys_schema_name } _report\" , ephys_schema_name )", "title": "activate()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.generate_electrode_config", "text": "Generate and insert new ElectrodeConfig Parameters: Name Type Description Default probe_type str probe type (e.g. neuropixels 2.0 - SS) required electrodes list Electrode dict (keys of the probe.ProbeType.Electrode table) required Returns: Name Type Description dict dict representing a key of the probe.ElectrodeConfig table Source code in element_array_ephys/ephys_precluster.py 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 def generate_electrode_config ( probe_type : str , electrodes : list ) -> dict : \"\"\"Generate and insert new ElectrodeConfig Args: probe_type (str): probe type (e.g. neuropixels 2.0 - SS) electrodes (list): Electrode dict (keys of the probe.ProbeType.Electrode table) Returns: dict: representing a key of the probe.ElectrodeConfig table \"\"\" # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode) electrode_config_hash = dict_to_uuid ({ k [ \"electrode\" ]: k for k in electrodes }) electrode_list = sorted ([ k [ \"electrode\" ] for k in electrodes ]) electrode_gaps = ( [ - 1 ] + np . where ( np . diff ( electrode_list ) > 1 )[ 0 ] . tolist () + [ len ( electrode_list ) - 1 ] ) electrode_config_name = \"; \" . join ( [ f \" { electrode_list [ start + 1 ] } - { electrode_list [ end ] } \" for start , end in zip ( electrode_gaps [: - 1 ], electrode_gaps [ 1 :]) ] ) electrode_config_key = { \"electrode_config_hash\" : electrode_config_hash } # ---- make new ElectrodeConfig if needed ---- if not probe . ElectrodeConfig & electrode_config_key : probe . ElectrodeConfig . insert1 ( { ** electrode_config_key , \"probe_type\" : probe_type , \"electrode_config_name\" : electrode_config_name , } ) probe . ElectrodeConfig . Electrode . insert ( { ** electrode_config_key , ** electrode } for electrode in electrodes ) return electrode_config_key", "title": "generate_electrode_config()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_ephys_root_data_dir", "text": "Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: Type Description list A list of the absolute path(s) to ephys data directories. Source code in element_array_ephys/ephys_precluster.py 70 71 72 73 74 75 76 77 78 def get_ephys_root_data_dir () -> list : \"\"\"Fetches absolute data path to ephys data directories. The absolute path here is used as a reference for all downstream relative paths used in DataJoint. Returns: A list of the absolute path(s) to ephys data directories. \"\"\" return _linking_module . get_ephys_root_data_dir ()", "title": "get_ephys_root_data_dir()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_neuropixels_channel2electrode_map", "text": "Get the channel map for neuropixels probe. Source code in element_array_ephys/ephys_precluster.py 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 def get_neuropixels_channel2electrode_map ( ephys_recording_key : dict , acq_software : str ) -> dict : \"\"\"Get the channel map for neuropixels probe. \"\"\" if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = get_spikeglx_meta_filepath ( ephys_recording_key ) spikeglx_meta = spikeglx . SpikeGLXMeta ( spikeglx_meta_filepath ) electrode_config_key = ( EphysRecording * probe . ElectrodeConfig & ephys_recording_key ) . fetch1 ( \"KEY\" ) electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode & electrode_config_key ) probe_electrodes = { ( shank , shank_col , shank_row ): key for key , shank , shank_col , shank_row in zip ( * electrode_query . fetch ( \"KEY\" , \"shank\" , \"shank_col\" , \"shank_row\" ) ) } channel2electrode_map = { recorded_site : probe_electrodes [( shank , shank_col , shank_row )] for recorded_site , ( shank , shank_col , shank_row , _ ) in enumerate ( spikeglx_meta . shankmap [ \"data\" ] ) } elif acq_software == \"Open Ephys\" : session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) openephys_dataset = openephys . OpenEphys ( session_dir ) probe_serial_number = ( ProbeInsertion & ephys_recording_key ) . fetch1 ( \"probe\" ) probe_dataset = openephys_dataset . probes [ probe_serial_number ] electrode_query = ( probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode * EphysRecording & ephys_recording_key ) probe_electrodes = { key [ \"electrode\" ]: key for key in electrode_query . fetch ( \"KEY\" ) } channel2electrode_map = { channel_idx : probe_electrodes [ channel_idx ] for channel_idx in probe_dataset . ap_meta [ \"channels_ids\" ] } return channel2electrode_map", "title": "get_neuropixels_channel2electrode_map()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_session_directory", "text": "Retrieve the session directory with Neuropixels for the given session. Parameters: Name Type Description Default session_key dict A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. required Returns: A string for the path to the session directory. Source code in element_array_ephys/ephys_precluster.py 81 82 83 84 85 86 87 88 89 90 def get_session_directory ( session_key : dict ) -> str : \"\"\"Retrieve the session directory with Neuropixels for the given session. Args: session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database. Returns: A string for the path to the session directory. \"\"\" return _linking_module . get_session_directory ( session_key )", "title": "get_session_directory()"}, {"location": "api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_spikeglx_meta_filepath", "text": "Get spikeGLX data filepath. Source code in element_array_ephys/ephys_precluster.py 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 def get_spikeglx_meta_filepath ( ephys_recording_key : dict ) -> str : \"\"\"Get spikeGLX data filepath. \"\"\" # attempt to retrieve from EphysRecording.EphysFile spikeglx_meta_filepath = ( EphysRecording . EphysFile & ephys_recording_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) try : spikeglx_meta_filepath = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath ) except FileNotFoundError : # if not found, search in session_dir again if not spikeglx_meta_filepath . exists (): session_dir = find_full_path ( get_ephys_root_data_dir (), get_session_directory ( ephys_recording_key ) ) inserted_probe_serial_number = ( ProbeInsertion * probe . Probe & ephys_recording_key ) . fetch1 ( \"probe\" ) spikeglx_meta_filepaths = [ fp for fp in session_dir . rglob ( \"*.ap.meta\" )] for meta_filepath in spikeglx_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) if str ( spikeglx_meta . probe_SN ) == inserted_probe_serial_number : spikeglx_meta_filepath = meta_filepath break else : raise FileNotFoundError ( \"No SpikeGLX data found for probe insertion: {} \" . format ( ephys_recording_key ) ) return spikeglx_meta_filepath", "title": "get_spikeglx_meta_filepath()"}, {"location": "api/element_array_ephys/ephys_report/", "text": "activate ( schema_name , ephys_schema_name , * , create_schema = True , create_tables = True ) \u00b6 activate(schema_name, *, create_schema=True, create_tables=True, activated_ephys=None) :param schema_name: schema name on the database server to activate the ephys_report schema :param ephys_schema_name: schema name of the activated ephys element for which this ephys_report schema will be downstream from :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. (The \"activation\" of this ephys_report module should be evoked by one of the ephys modules only) Source code in element_array_ephys/ephys_report.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def activate ( schema_name , ephys_schema_name , * , create_schema = True , create_tables = True ): \"\"\" activate(schema_name, *, create_schema=True, create_tables=True, activated_ephys=None) :param schema_name: schema name on the database server to activate the `ephys_report` schema :param ephys_schema_name: schema name of the activated ephys element for which this ephys_report schema will be downstream from :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. (The \"activation\" of this ephys_report module should be evoked by one of the ephys modules only) \"\"\" global ephys ephys = dj . create_virtual_module ( \"ephys\" , ephys_schema_name ) schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = ephys . __dict__ , )", "title": "ephys_report.py"}, {"location": "api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.activate", "text": "activate(schema_name, *, create_schema=True, create_tables=True, activated_ephys=None) :param schema_name: schema name on the database server to activate the ephys_report schema :param ephys_schema_name: schema name of the activated ephys element for which this ephys_report schema will be downstream from :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. (The \"activation\" of this ephys_report module should be evoked by one of the ephys modules only) Source code in element_array_ephys/ephys_report.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def activate ( schema_name , ephys_schema_name , * , create_schema = True , create_tables = True ): \"\"\" activate(schema_name, *, create_schema=True, create_tables=True, activated_ephys=None) :param schema_name: schema name on the database server to activate the `ephys_report` schema :param ephys_schema_name: schema name of the activated ephys element for which this ephys_report schema will be downstream from :param create_schema: when True (default), create schema in the database if it does not yet exist. :param create_tables: when True (default), create tables in the database if they do not yet exist. (The \"activation\" of this ephys_report module should be evoked by one of the ephys modules only) \"\"\" global ephys ephys = dj . create_virtual_module ( \"ephys\" , ephys_schema_name ) schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = ephys . __dict__ , )", "title": "activate()"}, {"location": "api/element_array_ephys/probe/", "text": "Neuropixels Probes ElectrodeConfig \u00b6 Bases: dj . Lookup Electrode configuration setting on a probe. Attributes: Name Type Description electrode_config_hash foreign key, uuid unique index for electrode configuration. ProbeType dict ProbeType entry. electrode_config_name varchar (4000) User-friendly name for this electrode configuration. Source code in element_array_ephys/probe.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 @schema class ElectrodeConfig ( dj . Lookup ): \"\"\"Electrode configuration setting on a probe. Attributes: electrode_config_hash (foreign key, uuid): unique index for electrode configuration. ProbeType (dict): ProbeType entry. electrode_config_name (varchar(4000) ): User-friendly name for this electrode configuration. \"\"\" definition = \"\"\" # The electrode configuration setting on a given probe electrode_config_hash: uuid --- -> ProbeType electrode_config_name: varchar(4000) # user friendly name \"\"\" class Electrode ( dj . Part ): \"\"\"Electrode included in the recording. Attributes: ElectrodeConfig (foreign key): ElectrodeConfig primary key. ProbeType.Electrode (foreign key): ProbeType.Electrode primary key. \"\"\" definition = \"\"\" # Electrodes selected for recording -> master -> ProbeType.Electrode \"\"\" Electrode \u00b6 Bases: dj . Part Electrode included in the recording. Attributes: Name Type Description ElectrodeConfig foreign key ElectrodeConfig primary key. ProbeType.Electrode foreign key ProbeType.Electrode primary key. Source code in element_array_ephys/probe.py 243 244 245 246 247 248 249 250 251 252 253 254 class Electrode ( dj . Part ): \"\"\"Electrode included in the recording. Attributes: ElectrodeConfig (foreign key): ElectrodeConfig primary key. ProbeType.Electrode (foreign key): ProbeType.Electrode primary key. \"\"\" definition = \"\"\" # Electrodes selected for recording -> master -> ProbeType.Electrode \"\"\" Probe \u00b6 Bases: dj . Lookup Represent a physical probe with unique ID Attributes: Name Type Description probe foreign key, varchar(32) Unique ID for this model of the probe. ProbeType dict ProbeType entry. probe_comment varchar (1000) Comment about this model of probe. Source code in element_array_ephys/probe.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 @schema class Probe ( dj . Lookup ): \"\"\"Represent a physical probe with unique ID Attributes: probe (foreign key, varchar(32) ): Unique ID for this model of the probe. ProbeType (dict): ProbeType entry. probe_comment (varchar(1000) ): Comment about this model of probe. \"\"\" definition = \"\"\" # Represent a physical probe with unique identification probe: varchar(32) # unique identifier for this model of probe (e.g. serial number) --- -> ProbeType probe_comment='' : varchar(1000) \"\"\" ProbeType \u00b6 Bases: dj . Lookup Type of probe. Attributes: Name Type Description probe_type foreign key, varchar (32) Name of the probe type. Source code in element_array_ephys/probe.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @schema class ProbeType ( dj . Lookup ): \"\"\"Type of probe. Attributes: probe_type (foreign key, varchar (32) ): Name of the probe type. \"\"\" definition = \"\"\" # Type of probe, with specific electrodes geometry defined probe_type: varchar(32) # e.g. neuropixels_1.0 \"\"\" class Electrode ( dj . Part ): \"\"\"Electrode information for a given probe. Attributes: ProbeType (foreign key): ProbeType primary key. electrode (foreign key, int): Electrode index, starting at 0. shank (int): shank index, starting at 0. shank_col (int): column index, starting at 0. shank_row (int): row index, starting at 0. x_coord (float): x-coordinate of the electrode within the probe in micrometers. y_coord (float): y-coordinate of the electrode within the probe in micrometers. \"\"\" definition = \"\"\" -> master electrode: int # electrode index, starts at 0 --- shank: int # shank index, starts at 0, advance left to right shank_col: int # column index, starts at 0, advance left to right shank_row: int # row index, starts at 0, advance tip to tail x_coord=NULL: float # (um) x coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe y_coord=NULL: float # (um) y coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe \"\"\" @staticmethod def create_neuropixels_probe ( probe_type : str = \"neuropixels 1.0 - 3A\" ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" neuropixels_probes_config = { \"neuropixels 1.0 - 3A\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 1.0 - 3B\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels UHD\" : dict ( site_count = 384 , col_spacing = 6 , row_spacing = 6 , white_spacing = 0 , col_count = 8 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 2.0 - SS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 , ), \"neuropixels 2.0 - MS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 , ), } def build_electrodes ( site_count : int , col_spacing : float , row_spacing : float , white_spacing : float , col_count : int , shank_count : int , shank_spacing : float , ) -> dict : \"\"\"Builds electrode layouts. Args: site_count (int): site count per shank col_spacing (float): (um) horrizontal spacing between sites row_spacing (float): (um) vertical spacing between columns white_spacing (float): (um) offset spacing col_count (int): number of column per shank shank_count (int): number of shank shank_spacing (float): spacing between shanks \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ( np . arange ( 0 , col_spacing * col_count , col_spacing ), row_count ) y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , col_count ) if white_spacing : x_white_spaces = np . tile ( [ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 ) ) x_coords = x_coords + x_white_spaces shank_cols = np . tile ( range ( col_count ), row_count ) shank_rows = np . repeat ( range ( row_count ), col_count ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ( [ { \"electrode\" : ( site_count * shank_no ) + e_id , \"shank\" : shank_no , \"shank_col\" : c_id , \"shank_row\" : r_id , \"x_coord\" : x + ( shank_no * shank_spacing ), \"y_coord\" : y , } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ) ) ] ) return npx_electrodes electrodes = build_electrodes ( ** neuropixels_probes_config [ probe_type ]) probe_type = { \"probe_type\" : probe_type } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ( [{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) Electrode \u00b6 Bases: dj . Part Electrode information for a given probe. Attributes: Name Type Description ProbeType foreign key ProbeType primary key. electrode foreign key, int Electrode index, starting at 0. shank int shank index, starting at 0. shank_col int column index, starting at 0. shank_row int row index, starting at 0. x_coord float x-coordinate of the electrode within the probe in micrometers. y_coord float y-coordinate of the electrode within the probe in micrometers. Source code in element_array_ephys/probe.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class Electrode ( dj . Part ): \"\"\"Electrode information for a given probe. Attributes: ProbeType (foreign key): ProbeType primary key. electrode (foreign key, int): Electrode index, starting at 0. shank (int): shank index, starting at 0. shank_col (int): column index, starting at 0. shank_row (int): row index, starting at 0. x_coord (float): x-coordinate of the electrode within the probe in micrometers. y_coord (float): y-coordinate of the electrode within the probe in micrometers. \"\"\" definition = \"\"\" -> master electrode: int # electrode index, starts at 0 --- shank: int # shank index, starts at 0, advance left to right shank_col: int # column index, starts at 0, advance left to right shank_row: int # row index, starts at 0, advance tip to tail x_coord=NULL: float # (um) x coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe y_coord=NULL: float # (um) y coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe \"\"\" create_neuropixels_probe ( probe_type = 'neuropixels 1.0 - 3A' ) staticmethod \u00b6 Create ProbeType and Electrode for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing Source code in element_array_ephys/probe.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @staticmethod def create_neuropixels_probe ( probe_type : str = \"neuropixels 1.0 - 3A\" ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" neuropixels_probes_config = { \"neuropixels 1.0 - 3A\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 1.0 - 3B\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels UHD\" : dict ( site_count = 384 , col_spacing = 6 , row_spacing = 6 , white_spacing = 0 , col_count = 8 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 2.0 - SS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 , ), \"neuropixels 2.0 - MS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 , ), } def build_electrodes ( site_count : int , col_spacing : float , row_spacing : float , white_spacing : float , col_count : int , shank_count : int , shank_spacing : float , ) -> dict : \"\"\"Builds electrode layouts. Args: site_count (int): site count per shank col_spacing (float): (um) horrizontal spacing between sites row_spacing (float): (um) vertical spacing between columns white_spacing (float): (um) offset spacing col_count (int): number of column per shank shank_count (int): number of shank shank_spacing (float): spacing between shanks \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ( np . arange ( 0 , col_spacing * col_count , col_spacing ), row_count ) y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , col_count ) if white_spacing : x_white_spaces = np . tile ( [ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 ) ) x_coords = x_coords + x_white_spaces shank_cols = np . tile ( range ( col_count ), row_count ) shank_rows = np . repeat ( range ( row_count ), col_count ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ( [ { \"electrode\" : ( site_count * shank_no ) + e_id , \"shank\" : shank_no , \"shank_col\" : c_id , \"shank_row\" : r_id , \"x_coord\" : x + ( shank_no * shank_spacing ), \"y_coord\" : y , } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ) ) ] ) return npx_electrodes electrodes = build_electrodes ( ** neuropixels_probes_config [ probe_type ]) probe_type = { \"probe_type\" : probe_type } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ( [{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True ) activate ( schema_name , * , create_schema = True , create_tables = True ) \u00b6 Activates the probe schemas. Parameters: Name Type Description Default schema_name str A string containing the name of the probe scehma. required create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True Dependencies: Upstream tables Session: A parent table to ProbeInsertion. Functions: Source code in element_array_ephys/probe.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def activate ( schema_name : str , * , create_schema : bool = True , create_tables : bool = True , ): \"\"\"Activates the `probe` schemas. Args: schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion. Functions: \"\"\" schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables ) # Add neuropixels probes for probe_type in ( \"neuropixels 1.0 - 3A\" , \"neuropixels 1.0 - 3B\" , \"neuropixels UHD\" , \"neuropixels 2.0 - SS\" , \"neuropixels 2.0 - MS\" , ): ProbeType . create_neuropixels_probe ( probe_type )", "title": "probe.py"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ElectrodeConfig", "text": "Bases: dj . Lookup Electrode configuration setting on a probe. Attributes: Name Type Description electrode_config_hash foreign key, uuid unique index for electrode configuration. ProbeType dict ProbeType entry. electrode_config_name varchar (4000) User-friendly name for this electrode configuration. Source code in element_array_ephys/probe.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 @schema class ElectrodeConfig ( dj . Lookup ): \"\"\"Electrode configuration setting on a probe. Attributes: electrode_config_hash (foreign key, uuid): unique index for electrode configuration. ProbeType (dict): ProbeType entry. electrode_config_name (varchar(4000) ): User-friendly name for this electrode configuration. \"\"\" definition = \"\"\" # The electrode configuration setting on a given probe electrode_config_hash: uuid --- -> ProbeType electrode_config_name: varchar(4000) # user friendly name \"\"\" class Electrode ( dj . Part ): \"\"\"Electrode included in the recording. Attributes: ElectrodeConfig (foreign key): ElectrodeConfig primary key. ProbeType.Electrode (foreign key): ProbeType.Electrode primary key. \"\"\" definition = \"\"\" # Electrodes selected for recording -> master -> ProbeType.Electrode \"\"\"", "title": "ElectrodeConfig"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ElectrodeConfig.Electrode", "text": "Bases: dj . Part Electrode included in the recording. Attributes: Name Type Description ElectrodeConfig foreign key ElectrodeConfig primary key. ProbeType.Electrode foreign key ProbeType.Electrode primary key. Source code in element_array_ephys/probe.py 243 244 245 246 247 248 249 250 251 252 253 254 class Electrode ( dj . Part ): \"\"\"Electrode included in the recording. Attributes: ElectrodeConfig (foreign key): ElectrodeConfig primary key. ProbeType.Electrode (foreign key): ProbeType.Electrode primary key. \"\"\" definition = \"\"\" # Electrodes selected for recording -> master -> ProbeType.Electrode \"\"\"", "title": "Electrode"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.Probe", "text": "Bases: dj . Lookup Represent a physical probe with unique ID Attributes: Name Type Description probe foreign key, varchar(32) Unique ID for this model of the probe. ProbeType dict ProbeType entry. probe_comment varchar (1000) Comment about this model of probe. Source code in element_array_ephys/probe.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 @schema class Probe ( dj . Lookup ): \"\"\"Represent a physical probe with unique ID Attributes: probe (foreign key, varchar(32) ): Unique ID for this model of the probe. ProbeType (dict): ProbeType entry. probe_comment (varchar(1000) ): Comment about this model of probe. \"\"\" definition = \"\"\" # Represent a physical probe with unique identification probe: varchar(32) # unique identifier for this model of probe (e.g. serial number) --- -> ProbeType probe_comment='' : varchar(1000) \"\"\"", "title": "Probe"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType", "text": "Bases: dj . Lookup Type of probe. Attributes: Name Type Description probe_type foreign key, varchar (32) Name of the probe type. Source code in element_array_ephys/probe.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @schema class ProbeType ( dj . Lookup ): \"\"\"Type of probe. Attributes: probe_type (foreign key, varchar (32) ): Name of the probe type. \"\"\" definition = \"\"\" # Type of probe, with specific electrodes geometry defined probe_type: varchar(32) # e.g. neuropixels_1.0 \"\"\" class Electrode ( dj . Part ): \"\"\"Electrode information for a given probe. Attributes: ProbeType (foreign key): ProbeType primary key. electrode (foreign key, int): Electrode index, starting at 0. shank (int): shank index, starting at 0. shank_col (int): column index, starting at 0. shank_row (int): row index, starting at 0. x_coord (float): x-coordinate of the electrode within the probe in micrometers. y_coord (float): y-coordinate of the electrode within the probe in micrometers. \"\"\" definition = \"\"\" -> master electrode: int # electrode index, starts at 0 --- shank: int # shank index, starts at 0, advance left to right shank_col: int # column index, starts at 0, advance left to right shank_row: int # row index, starts at 0, advance tip to tail x_coord=NULL: float # (um) x coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe y_coord=NULL: float # (um) y coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe \"\"\" @staticmethod def create_neuropixels_probe ( probe_type : str = \"neuropixels 1.0 - 3A\" ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" neuropixels_probes_config = { \"neuropixels 1.0 - 3A\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 1.0 - 3B\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels UHD\" : dict ( site_count = 384 , col_spacing = 6 , row_spacing = 6 , white_spacing = 0 , col_count = 8 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 2.0 - SS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 , ), \"neuropixels 2.0 - MS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 , ), } def build_electrodes ( site_count : int , col_spacing : float , row_spacing : float , white_spacing : float , col_count : int , shank_count : int , shank_spacing : float , ) -> dict : \"\"\"Builds electrode layouts. Args: site_count (int): site count per shank col_spacing (float): (um) horrizontal spacing between sites row_spacing (float): (um) vertical spacing between columns white_spacing (float): (um) offset spacing col_count (int): number of column per shank shank_count (int): number of shank shank_spacing (float): spacing between shanks \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ( np . arange ( 0 , col_spacing * col_count , col_spacing ), row_count ) y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , col_count ) if white_spacing : x_white_spaces = np . tile ( [ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 ) ) x_coords = x_coords + x_white_spaces shank_cols = np . tile ( range ( col_count ), row_count ) shank_rows = np . repeat ( range ( row_count ), col_count ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ( [ { \"electrode\" : ( site_count * shank_no ) + e_id , \"shank\" : shank_no , \"shank_col\" : c_id , \"shank_row\" : r_id , \"x_coord\" : x + ( shank_no * shank_spacing ), \"y_coord\" : y , } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ) ) ] ) return npx_electrodes electrodes = build_electrodes ( ** neuropixels_probes_config [ probe_type ]) probe_type = { \"probe_type\" : probe_type } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ( [{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True )", "title": "ProbeType"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.Electrode", "text": "Bases: dj . Part Electrode information for a given probe. Attributes: Name Type Description ProbeType foreign key ProbeType primary key. electrode foreign key, int Electrode index, starting at 0. shank int shank index, starting at 0. shank_col int column index, starting at 0. shank_row int row index, starting at 0. x_coord float x-coordinate of the electrode within the probe in micrometers. y_coord float y-coordinate of the electrode within the probe in micrometers. Source code in element_array_ephys/probe.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class Electrode ( dj . Part ): \"\"\"Electrode information for a given probe. Attributes: ProbeType (foreign key): ProbeType primary key. electrode (foreign key, int): Electrode index, starting at 0. shank (int): shank index, starting at 0. shank_col (int): column index, starting at 0. shank_row (int): row index, starting at 0. x_coord (float): x-coordinate of the electrode within the probe in micrometers. y_coord (float): y-coordinate of the electrode within the probe in micrometers. \"\"\" definition = \"\"\" -> master electrode: int # electrode index, starts at 0 --- shank: int # shank index, starts at 0, advance left to right shank_col: int # column index, starts at 0, advance left to right shank_row: int # row index, starts at 0, advance tip to tail x_coord=NULL: float # (um) x coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe y_coord=NULL: float # (um) y coordinate of the electrode within the probe, (0, 0) is the bottom left corner of the probe \"\"\"", "title": "Electrode"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.create_neuropixels_probe", "text": "Create ProbeType and Electrode for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing Source code in element_array_ephys/probe.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 @staticmethod def create_neuropixels_probe ( probe_type : str = \"neuropixels 1.0 - 3A\" ): \"\"\" Create `ProbeType` and `Electrode` for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS For electrode location, the (0, 0) is the bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing \"\"\" neuropixels_probes_config = { \"neuropixels 1.0 - 3A\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 1.0 - 3B\" : dict ( site_count = 960 , col_spacing = 32 , row_spacing = 20 , white_spacing = 16 , col_count = 2 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels UHD\" : dict ( site_count = 384 , col_spacing = 6 , row_spacing = 6 , white_spacing = 0 , col_count = 8 , shank_count = 1 , shank_spacing = 0 , ), \"neuropixels 2.0 - SS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 1 , shank_spacing = 250 , ), \"neuropixels 2.0 - MS\" : dict ( site_count = 1280 , col_spacing = 32 , row_spacing = 15 , white_spacing = 0 , col_count = 2 , shank_count = 4 , shank_spacing = 250 , ), } def build_electrodes ( site_count : int , col_spacing : float , row_spacing : float , white_spacing : float , col_count : int , shank_count : int , shank_spacing : float , ) -> dict : \"\"\"Builds electrode layouts. Args: site_count (int): site count per shank col_spacing (float): (um) horrizontal spacing between sites row_spacing (float): (um) vertical spacing between columns white_spacing (float): (um) offset spacing col_count (int): number of column per shank shank_count (int): number of shank shank_spacing (float): spacing between shanks \"\"\" row_count = int ( site_count / col_count ) x_coords = np . tile ( np . arange ( 0 , col_spacing * col_count , col_spacing ), row_count ) y_coords = np . repeat ( np . arange ( row_count ) * row_spacing , col_count ) if white_spacing : x_white_spaces = np . tile ( [ white_spacing , white_spacing , 0 , 0 ], int ( row_count / 2 ) ) x_coords = x_coords + x_white_spaces shank_cols = np . tile ( range ( col_count ), row_count ) shank_rows = np . repeat ( range ( row_count ), col_count ) npx_electrodes = [] for shank_no in range ( shank_count ): npx_electrodes . extend ( [ { \"electrode\" : ( site_count * shank_no ) + e_id , \"shank\" : shank_no , \"shank_col\" : c_id , \"shank_row\" : r_id , \"x_coord\" : x + ( shank_no * shank_spacing ), \"y_coord\" : y , } for e_id , ( c_id , r_id , x , y ) in enumerate ( zip ( shank_cols , shank_rows , x_coords , y_coords ) ) ] ) return npx_electrodes electrodes = build_electrodes ( ** neuropixels_probes_config [ probe_type ]) probe_type = { \"probe_type\" : probe_type } with ProbeType . connection . transaction : ProbeType . insert1 ( probe_type , skip_duplicates = True ) ProbeType . Electrode . insert ( [{ ** probe_type , ** e } for e in electrodes ], skip_duplicates = True )", "title": "create_neuropixels_probe()"}, {"location": "api/element_array_ephys/probe/#element_array_ephys.probe.activate", "text": "Activates the probe schemas. Parameters: Name Type Description Default schema_name str A string containing the name of the probe scehma. required create_schema bool If True, schema will be created in the database. True create_tables bool If True, tables related to the schema will be created in the database. True Dependencies: Upstream tables Session: A parent table to ProbeInsertion. Functions: Source code in element_array_ephys/probe.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def activate ( schema_name : str , * , create_schema : bool = True , create_tables : bool = True , ): \"\"\"Activates the `probe` schemas. Args: schema_name (str): A string containing the name of the probe scehma. create_schema (bool): If True, schema will be created in the database. create_tables (bool): If True, tables related to the schema will be created in the database. Dependencies: Upstream tables: Session: A parent table to ProbeInsertion. Functions: \"\"\" schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables ) # Add neuropixels probes for probe_type in ( \"neuropixels 1.0 - 3A\" , \"neuropixels 1.0 - 3B\" , \"neuropixels UHD\" , \"neuropixels 2.0 - SS\" , \"neuropixels 2.0 - MS\" , ): ProbeType . create_neuropixels_probe ( probe_type )", "title": "activate()"}, {"location": "api/element_array_ephys/version/", "text": "Package metadata.", "title": "version.py"}, {"location": "api/element_array_ephys/export/nwb/nwb/", "text": "LFPDataChunkIterator \u00b6 Bases: GenericDataChunkIterator DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) Source code in element_array_ephys/export/nwb/nwb.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class LFPDataChunkIterator ( GenericDataChunkIterator ): \"\"\" DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) \"\"\" def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 () self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 )) def _get_data ( self , selection ): electrode = self . electrodes [ selection [ 1 ]][ 0 ] return ( self . lfp_electrodes_query & dict ( electrode = electrode )) . fetch1 ( \"lfp\" )[ selection [ 0 ], np . newaxis ] def _get_dtype ( self ): return self . _dtype def _get_maxshape ( self ): return self . n_tt , self . n_channels __init__ ( lfp_electrodes_query , chunk_length = 10000 ) \u00b6 Parameters \u00b6 lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode int, optional Chunks are blocks of disk space where data are stored contiguously and compressed Source code in element_array_ephys/export/nwb/nwb.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 () self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 )) add_electrodes_to_nwb ( session_key , nwbfile ) \u00b6 Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"] Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile Source code in element_array_ephys/export/nwb/nwb.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def add_electrodes_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile \"\"\" electrodes_query = probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode for additional_attribute in [ \"shank_col\" , \"shank_row\" , \"shank\" ]: nwbfile . add_electrode_column ( name = electrodes_query . heading . attributes [ additional_attribute ] . name , description = electrodes_query . heading . attributes [ additional_attribute ] . comment , ) nwbfile . add_electrode_column ( name = \"id_in_probe\" , description = \"electrode id within the probe\" , ) for this_probe in ( ephys . ProbeInsertion * probe . Probe & session_key ) . fetch ( as_dict = True ): insertion_record = ( ephys . InsertionLocation & this_probe ) . fetch ( as_dict = True ) if len ( insertion_record ) == 1 : insert_location = json . dumps ( { k : v for k , v in insertion_record [ 0 ] . items () if k not in ephys . InsertionLocation . primary_key }, cls = DecimalEncoder , ) elif len ( insertion_record ) == 0 : insert_location = \"unknown\" else : raise DataJointError ( f 'Found multiple insertion locations for { this_probe } ' ) device = nwbfile . create_device ( name = this_probe [ \"probe\" ], description = this_probe . get ( \"probe_comment\" , None ), manufacturer = this_probe . get ( \"probe_type\" , None ), ) shank_ids = set (( probe . ProbeType . Electrode & this_probe ) . fetch ( \"shank\" )) for shank_id in shank_ids : electrode_group = nwbfile . create_electrode_group ( name = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , description = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , location = insert_location , device = device , ) electrodes_query = ( probe . ProbeType . Electrode & this_probe & dict ( shank = shank_id ) ) . fetch ( as_dict = True ) for electrode in electrodes_query : nwbfile . add_electrode ( group = electrode_group , filtering = \"unknown\" , imp =- 1.0 , x = np . nan , y = np . nan , z = np . nan , rel_x = electrode [ \"x_coord\" ], rel_y = electrode [ \"y_coord\" ], rel_z = np . nan , shank_col = electrode [ \"shank_col\" ], shank_row = electrode [ \"shank_row\" ], location = \"unknown\" , id_in_probe = electrode [ \"electrode\" ], shank = electrode [ \"shank\" ], ) add_ephys_lfp_from_dj_to_nwb ( session_key , nwbfile ) \u00b6 Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].timestamps Parameters \u00b6 session_key: dict nwbfile: NWBFile Source code in element_array_ephys/export/nwb/nwb.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def add_ephys_lfp_from_dj_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].timestamps Parameters ---------- session_key: dict nwbfile: NWBFile \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) nwb_lfp = pynwb . ecephys . LFP ( name = \"LFP\" ) ecephys_module . add ( nwb_lfp ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for lfp_record in ( ephys . LFP & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion & lfp_record ) . fetch1 ( \"probe\" ) lfp_electrodes_query = ephys . LFP . Electrode & lfp_record lfp_data = LFPDataChunkIterator ( lfp_electrodes_query ) nwb_lfp . create_electrical_series ( name = f \"ElectricalSeries { lfp_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = H5DataIO ( lfp_data , compression = True ), timestamps = lfp_record [ \"lfp_time_stamps\" ], electrodes = nwbfile . create_electrode_table_region ( name = \"electrodes\" , description = \"electrodes used for LFP\" , region = [ mapping [( probe_id , x )] for x in lfp_electrodes_query . fetch ( \"electrode\" ) ], ), ) add_ephys_lfp_from_source_to_nwb ( session_key , ephys_root_data_dir , nwbfile , end_frame = None ) \u00b6 Read the LFP data from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile int, optional use for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 def add_ephys_lfp_from_source_to_nwb ( session_key : dict , ephys_root_data_dir , nwbfile : pynwb . NWBFile , end_frame = None ): \"\"\" Read the LFP data from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile end_frame: int, optional use for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) lfp = pynwb . ecephys . LFP () ecephys_module . add ( lfp ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.lf\" ) else : raise ValueError ( \"unsupported acq_software type:\" + f \" { ephys_recording_record [ 'acq_software' ] } \" ) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) lfp . add_electrical_series ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = extractor . get_sampling_frequency (), starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) ) add_ephys_recording_to_nwb ( session_key , ephys_root_data_dir , nwbfile , end_frame = None ) \u00b6 Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"] Parameters \u00b6 dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional Used for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 def add_ephys_recording_to_nwb ( session_key : dict , ephys_root_data_dir : str , nwbfile : pynwb . NWBFile , end_frame : int = None , ): \"\"\" Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"] Parameters ---------- session_key: dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional Used for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.ap\" ) elif ephys_recording_record [ \"acq_software\" ] == \"OpenEphys\" : extractor = extractors . read_openephys ( file_path , stream_id = \"0\" ) else : raise ValueError ( f \"unsupported acq_software type: { ephys_recording_record [ 'acq_software' ] } \" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) nwbfile . add_acquisition ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = str ( ephys_recording_record ), data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = ephys_recording_record [ \"sampling_rate\" ], starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) ) add_ephys_units_to_nwb ( session_key , nwbfile , primary_clustering_paramset_idx = 0 ) \u00b6 Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use primary_clustering_paramset_idx to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional Source code in element_array_ephys/export/nwb/nwb.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def add_ephys_units_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile , primary_clustering_paramset_idx : int = 0 ): \"\"\" Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use `primary_clustering_paramset_idx` to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional \"\"\" if not ephys . ClusteringTask & session_key : warnings . warn ( f 'No unit data exists for session: { session_key } ' ) return if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) for paramset_record in ( ephys . ClusteringParamSet & ephys . CuratedClustering & session_key ) . fetch ( \"paramset_idx\" , \"clustering_method\" , \"paramset_desc\" , as_dict = True ): if paramset_record [ \"paramset_idx\" ] == primary_clustering_paramset_idx : units_table = create_units_table ( session_key , nwbfile , paramset_record , desc = paramset_record [ \"paramset_desc\" ], ) nwbfile . units = units_table else : name = f \"units_ { paramset_record [ 'clustering_method' ] } \" units_table = create_units_table ( session_key , nwbfile , paramset_record , name = name , desc = paramset_record [ \"paramset_desc\" ], ) ecephys_module = get_module ( nwbfile , \"ecephys\" ) ecephys_module . add ( units_table ) create_units_table ( session_key , nwbfile , paramset_record , name = 'units' , desc = 'data on spiking units' ) \u00b6 ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters \u00b6 session_key: dict nwbfile: pynwb.NWBFile paramset_record: int str, optional default=\"units\" str, optional default=\"data on spiking units\" Source code in element_array_ephys/export/nwb/nwb.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def create_units_table ( session_key : dict , nwbfile : pynwb . NWBFile , paramset_record , name = \"units\" , desc = \"data on spiking units\" ): \"\"\" ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile paramset_record: int name: str, optional default=\"units\" desc: str, optional default=\"data on spiking units\" \"\"\" # electrode id mapping mapping = get_electrodes_mapping ( nwbfile . electrodes ) units_table = pynwb . misc . Units ( name = name , description = desc ) # add additional columns to the units table for additional_attribute in [ \"cluster_quality_label\" , \"spike_depths\" ]: # The `index` parameter indicates whether the column is a \"ragged array,\" i.e. # whether each row of this column is a vector with potentially different lengths # for each row. units_table . add_column ( name = additional_attribute , description = ephys . CuratedClustering . Unit . heading . attributes [ additional_attribute ] . comment , index = additional_attribute == \"spike_depths\" , ) clustering_query = ( ephys . EphysRecording * ephys . ClusteringTask & session_key & paramset_record ) for unit in tqdm ( ( ephys . CuratedClustering . Unit & clustering_query . proj ()) . fetch ( as_dict = True ), desc = f \"creating units table for paramset { paramset_record [ 'paramset_idx' ] } \" , ): probe_id , shank_num = ( ephys . ProbeInsertion * ephys . CuratedClustering . Unit * probe . ProbeType . Electrode & dict (( k , unit [ k ]) for k in unit . keys () # excess keys caused errs if k not in [ 'spike_times' , 'spike_sites' , 'spike_depths' ]) ) . fetch1 ( \"probe\" , \"shank\" ) waveform_mean = ( ephys . WaveformSet . PeakWaveform () & clustering_query & unit ) . fetch1 ( \"peak_electrode_waveform\" ) units_table . add_row ( id = unit [ \"unit\" ], electrodes = [ mapping [( probe_id , unit [ \"electrode\" ])]], electrode_group = nwbfile . electrode_groups [ f \"probe { probe_id } _shank { shank_num } \" ], cluster_quality_label = unit [ \"cluster_quality_label\" ], spike_times = unit [ \"spike_times\" ], spike_depths = unit [ \"spike_depths\" ], waveform_mean = waveform_mean , ) return units_table ecephys_session_to_nwb ( session_key , raw = True , spikes = True , lfp = 'source' , end_frame = None , lab_key = None , project_key = None , protocol_key = None , nwbfile_kwargs = None ) \u00b6 Main function for converting ephys data to NWB Parameters \u00b6 session_key: dict bool Whether to include the raw data from source. SpikeGLX & OpenEphys are supported bool Whether to include CuratedClustering lfp \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries to look up optional metadata dict, optional If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required minimal data for instantiating an NWBFile object. If element-session is being used, this argument can optionally be used to overwrite NWBFile fields. Source code in element_array_ephys/export/nwb/nwb.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 def ecephys_session_to_nwb ( session_key , raw = True , spikes = True , lfp = \"source\" , end_frame = None , lab_key = None , project_key = None , protocol_key = None , nwbfile_kwargs = None , ): \"\"\" Main function for converting ephys data to NWB Parameters ---------- session_key: dict raw: bool Whether to include the raw data from source. SpikeGLX & OpenEphys are supported spikes: bool Whether to include CuratedClustering lfp: \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP end_frame: int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries to look up optional metadata nwbfile_kwargs: dict, optional - If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required minimal data for instantiating an NWBFile object. - If element-session is being used, this argument can optionally be used to overwrite NWBFile fields. \"\"\" session_to_nwb = getattr ( ephys . _linking_module , 'session_to_nwb' , False ) if session_to_nwb : nwbfile = session_to_nwb ( session_key , lab_key = lab_key , project_key = project_key , protocol_key = protocol_key , additional_nwbfile_kwargs = nwbfile_kwargs , ) else : nwbfile = pynwb . NWBFile ( ** nwbfile_kwargs ) ephys_root_data_dir = ephys . get_ephys_root_data_dir () if raw : add_ephys_recording_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) if spikes : add_ephys_units_to_nwb ( session_key , nwbfile ) if lfp == \"dj\" : add_ephys_lfp_from_dj_to_nwb ( session_key , nwbfile ) if lfp == \"source\" : add_ephys_lfp_from_source_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) return nwbfile gains_helper ( gains ) \u00b6 This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the channel_conversion field in addition to the conversion field so that each channel can be converted to volts using its own individual gain. Parameters \u00b6 gains: np.ndarray Returns \u00b6 dict conversion : float channel_conversion : np.ndarray Source code in element_array_ephys/export/nwb/nwb.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def gains_helper ( gains ): \"\"\" This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the `channel_conversion` field in addition to the `conversion` field so that each channel can be converted to volts using its own individual gain. Parameters ---------- gains: np.ndarray Returns ------- dict conversion : float channel_conversion : np.ndarray \"\"\" if all ( x == 1 for x in gains ): return dict ( conversion = 1e-6 , channel_conversion = None ) if all ( x == gains [ 0 ] for x in gains ): return dict ( conversion = 1e-6 * gains [ 0 ], channel_conversion = None ) return dict ( conversion = 1e-6 , channel_conversion = gains ) get_electrodes_mapping ( electrodes ) \u00b6 Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries. Parameters \u00b6 electrodes: hdmf.common.table.DynamicTable Returns \u00b6 dict Source code in element_array_ephys/export/nwb/nwb.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def get_electrodes_mapping ( electrodes ): \"\"\" Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries. Parameters ---------- electrodes: hdmf.common.table.DynamicTable Returns ------- dict \"\"\" return { ( electrodes [ \"group\" ][ idx ] . device . name , electrodes [ \"id_in_probe\" ][ idx ],): idx for idx in range ( len ( electrodes )) } write_nwb ( nwbfile , fname , check_read = True ) \u00b6 Export NWBFile Parameters \u00b6 nwbfile: pynwb.NWBFile fname: str Absolute path including *.nwb extension. bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. Source code in element_array_ephys/export/nwb/nwb.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def write_nwb ( nwbfile , fname , check_read = True ): \"\"\" Export NWBFile Parameters ---------- nwbfile: pynwb.NWBFile fname: str Absolute path including `*.nwb` extension. check_read: bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. \"\"\" with pynwb . NWBHDF5IO ( fname , \"w\" ) as io : io . write ( nwbfile ) if check_read : with pynwb . NWBHDF5IO ( fname , \"r\" ) as io : io . read ()", "title": "nwb.py"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator", "text": "Bases: GenericDataChunkIterator DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) Source code in element_array_ephys/export/nwb/nwb.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class LFPDataChunkIterator ( GenericDataChunkIterator ): \"\"\" DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files) \"\"\" def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 () self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 )) def _get_data ( self , selection ): electrode = self . electrodes [ selection [ 1 ]][ 0 ] return ( self . lfp_electrodes_query & dict ( electrode = electrode )) . fetch1 ( \"lfp\" )[ selection [ 0 ], np . newaxis ] def _get_dtype ( self ): return self . _dtype def _get_maxshape ( self ): return self . n_tt , self . n_channels", "title": "LFPDataChunkIterator"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__", "text": "", "title": "__init__()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__--parameters", "text": "lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode int, optional Chunks are blocks of disk space where data are stored contiguously and compressed Source code in element_array_ephys/export/nwb/nwb.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , lfp_electrodes_query , chunk_length : int = 10000 ): \"\"\" Parameters ---------- lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode chunk_length: int, optional Chunks are blocks of disk space where data are stored contiguously and compressed \"\"\" self . lfp_electrodes_query = lfp_electrodes_query self . electrodes = self . lfp_electrodes_query . fetch ( \"electrode\" ) first_record = ( self . lfp_electrodes_query & dict ( electrode = self . electrodes [ 0 ]) ) . fetch1 () self . n_channels = len ( self . electrodes ) self . n_tt = len ( first_record [ \"lfp\" ]) self . _dtype = first_record [ \"lfp\" ] . dtype super () . __init__ ( buffer_shape = ( self . n_tt , 1 ), chunk_shape = ( chunk_length , 1 ))", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb", "text": "Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"]", "title": "add_electrodes_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile Source code in element_array_ephys/export/nwb/nwb.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def add_electrodes_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP. ephys.InsertionLocation -> ElectrodeGroup.location probe.Probe::probe -> device.name probe.Probe::probe_comment -> device.description probe.Probe::probe_type -> device.manufacturer probe.ProbeType.Electrode::electrode -> electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -> electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -> electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -> electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -> electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -> electrodes[\"shank_row\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile \"\"\" electrodes_query = probe . ProbeType . Electrode * probe . ElectrodeConfig . Electrode for additional_attribute in [ \"shank_col\" , \"shank_row\" , \"shank\" ]: nwbfile . add_electrode_column ( name = electrodes_query . heading . attributes [ additional_attribute ] . name , description = electrodes_query . heading . attributes [ additional_attribute ] . comment , ) nwbfile . add_electrode_column ( name = \"id_in_probe\" , description = \"electrode id within the probe\" , ) for this_probe in ( ephys . ProbeInsertion * probe . Probe & session_key ) . fetch ( as_dict = True ): insertion_record = ( ephys . InsertionLocation & this_probe ) . fetch ( as_dict = True ) if len ( insertion_record ) == 1 : insert_location = json . dumps ( { k : v for k , v in insertion_record [ 0 ] . items () if k not in ephys . InsertionLocation . primary_key }, cls = DecimalEncoder , ) elif len ( insertion_record ) == 0 : insert_location = \"unknown\" else : raise DataJointError ( f 'Found multiple insertion locations for { this_probe } ' ) device = nwbfile . create_device ( name = this_probe [ \"probe\" ], description = this_probe . get ( \"probe_comment\" , None ), manufacturer = this_probe . get ( \"probe_type\" , None ), ) shank_ids = set (( probe . ProbeType . Electrode & this_probe ) . fetch ( \"shank\" )) for shank_id in shank_ids : electrode_group = nwbfile . create_electrode_group ( name = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , description = f \"probe { this_probe [ 'probe' ] } _shank { shank_id } \" , location = insert_location , device = device , ) electrodes_query = ( probe . ProbeType . Electrode & this_probe & dict ( shank = shank_id ) ) . fetch ( as_dict = True ) for electrode in electrodes_query : nwbfile . add_electrode ( group = electrode_group , filtering = \"unknown\" , imp =- 1.0 , x = np . nan , y = np . nan , z = np . nan , rel_x = electrode [ \"x_coord\" ], rel_y = electrode [ \"y_coord\" ], rel_z = np . nan , shank_col = electrode [ \"shank_col\" ], shank_row = electrode [ \"shank_row\" ], location = \"unknown\" , id_in_probe = electrode [ \"electrode\" ], shank = electrode [ \"shank\" ], )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb", "text": "Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].timestamps", "title": "add_ephys_lfp_from_dj_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb--parameters", "text": "session_key: dict nwbfile: NWBFile Source code in element_array_ephys/export/nwb/nwb.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def add_ephys_lfp_from_dj_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile ): \"\"\" Read LFP data from the data in element-aray-ephys ephys.LFP.Electrode::lfp -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].data ephys.LFP::lfp_time_stamps -> processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\" ].timestamps Parameters ---------- session_key: dict nwbfile: NWBFile \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) nwb_lfp = pynwb . ecephys . LFP ( name = \"LFP\" ) ecephys_module . add ( nwb_lfp ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for lfp_record in ( ephys . LFP & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion & lfp_record ) . fetch1 ( \"probe\" ) lfp_electrodes_query = ephys . LFP . Electrode & lfp_record lfp_data = LFPDataChunkIterator ( lfp_electrodes_query ) nwb_lfp . create_electrical_series ( name = f \"ElectricalSeries { lfp_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = H5DataIO ( lfp_data , compression = True ), timestamps = lfp_record [ \"lfp_time_stamps\" ], electrodes = nwbfile . create_electrode_table_region ( name = \"electrodes\" , description = \"electrodes used for LFP\" , region = [ mapping [( probe_id , x )] for x in lfp_electrodes_query . fetch ( \"electrode\" ) ], ), )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb", "text": "Read the LFP data from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition", "title": "add_ephys_lfp_from_source_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile int, optional use for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 def add_ephys_lfp_from_source_to_nwb ( session_key : dict , ephys_root_data_dir , nwbfile : pynwb . NWBFile , end_frame = None ): \"\"\" Read the LFP data from the source file. Currently, only works for SpikeGLX data. ephys.EphysRecording::recording_datetime -> acquisition Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile end_frame: int, optional use for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) ecephys_module = get_module ( nwbfile , name = \"ecephys\" , description = \"preprocessed ephys data\" ) lfp = pynwb . ecephys . LFP () ecephys_module . add ( lfp ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.lf\" ) else : raise ValueError ( \"unsupported acq_software type:\" + f \" { ephys_recording_record [ 'acq_software' ] } \" ) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) lfp . add_electrical_series ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = f \"LFP from probe { probe_id } \" , data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = extractor . get_sampling_frequency (), starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb", "text": "Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"]", "title": "add_ephys_recording_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb--parameters", "text": "dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional Used for small test conversions Source code in element_array_ephys/export/nwb/nwb.py 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 def add_ephys_recording_to_nwb ( session_key : dict , ephys_root_data_dir : str , nwbfile : pynwb . NWBFile , end_frame : int = None , ): \"\"\" Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data. source data -> acquisition[\"ElectricalSeries\"] Parameters ---------- session_key: dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional Used for small test conversions \"\"\" if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) mapping = get_electrodes_mapping ( nwbfile . electrodes ) for ephys_recording_record in ( ephys . EphysRecording & session_key ) . fetch ( as_dict = True ): probe_id = ( ephys . ProbeInsertion () & ephys_recording_record ) . fetch1 ( \"probe\" ) relative_path = ( ephys . EphysRecording . EphysFile & ephys_recording_record ) . fetch1 ( \"file_path\" ) relative_path = relative_path . replace ( \" \\\\ \" , \"/\" ) file_path = find_full_path ( ephys_root_data_dir , relative_path ) if ephys_recording_record [ \"acq_software\" ] == \"SpikeGLX\" : extractor = extractors . read_spikeglx ( os . path . split ( file_path )[ 0 ], \"imec.ap\" ) elif ephys_recording_record [ \"acq_software\" ] == \"OpenEphys\" : extractor = extractors . read_openephys ( file_path , stream_id = \"0\" ) else : raise ValueError ( f \"unsupported acq_software type: { ephys_recording_record [ 'acq_software' ] } \" ) conversion_kwargs = gains_helper ( extractor . get_channel_gains ()) if end_frame is not None : extractor = extractor . frame_slice ( 0 , end_frame ) recording_channels_by_id = ( probe . ElectrodeConfig . Electrode () & ephys_recording_record ) . fetch ( \"electrode\" ) nwbfile . add_acquisition ( pynwb . ecephys . ElectricalSeries ( name = f \"ElectricalSeries { ephys_recording_record [ 'insertion_number' ] } \" , description = str ( ephys_recording_record ), data = SpikeInterfaceRecordingDataChunkIterator ( extractor ), rate = ephys_recording_record [ \"sampling_rate\" ], starting_time = ( ephys_recording_record [ \"recording_datetime\" ] - ephys_recording_record [ \"session_datetime\" ] ) . total_seconds (), electrodes = nwbfile . create_electrode_table_region ( region = [ mapping [( probe_id , x )] for x in recording_channels_by_id ], name = \"electrodes\" , description = \"recorded electrodes\" , ), ** conversion_kwargs ) )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb", "text": "Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use primary_clustering_paramset_idx to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"]", "title": "add_ephys_units_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional Source code in element_array_ephys/export/nwb/nwb.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def add_ephys_units_to_nwb ( session_key : dict , nwbfile : pynwb . NWBFile , primary_clustering_paramset_idx : int = 0 ): \"\"\" Add spiking data to NWBFile. In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations. Use `primary_clustering_paramset_idx` to indicate which clustering is primary. All others will be stored in /processing/ecephys/. ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional \"\"\" if not ephys . ClusteringTask & session_key : warnings . warn ( f 'No unit data exists for session: { session_key } ' ) return if nwbfile . electrodes is None : add_electrodes_to_nwb ( session_key , nwbfile ) for paramset_record in ( ephys . ClusteringParamSet & ephys . CuratedClustering & session_key ) . fetch ( \"paramset_idx\" , \"clustering_method\" , \"paramset_desc\" , as_dict = True ): if paramset_record [ \"paramset_idx\" ] == primary_clustering_paramset_idx : units_table = create_units_table ( session_key , nwbfile , paramset_record , desc = paramset_record [ \"paramset_desc\" ], ) nwbfile . units = units_table else : name = f \"units_ { paramset_record [ 'clustering_method' ] } \" units_table = create_units_table ( session_key , nwbfile , paramset_record , name = name , desc = paramset_record [ \"paramset_desc\" ], ) ecephys_module = get_module ( nwbfile , \"ecephys\" ) ecephys_module . add ( units_table )", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table", "text": "ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"]", "title": "create_units_table()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table--parameters", "text": "session_key: dict nwbfile: pynwb.NWBFile paramset_record: int str, optional default=\"units\" str, optional default=\"data on spiking units\" Source code in element_array_ephys/export/nwb/nwb.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def create_units_table ( session_key : dict , nwbfile : pynwb . NWBFile , paramset_record , name = \"units\" , desc = \"data on spiking units\" ): \"\"\" ephys.CuratedClustering.Unit::unit -> units.id ephys.CuratedClustering.Unit::spike_times -> units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -> units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -> units[\"cluster_quality_label\"] ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -> units[\"waveform_mean\"] Parameters ---------- session_key: dict nwbfile: pynwb.NWBFile paramset_record: int name: str, optional default=\"units\" desc: str, optional default=\"data on spiking units\" \"\"\" # electrode id mapping mapping = get_electrodes_mapping ( nwbfile . electrodes ) units_table = pynwb . misc . Units ( name = name , description = desc ) # add additional columns to the units table for additional_attribute in [ \"cluster_quality_label\" , \"spike_depths\" ]: # The `index` parameter indicates whether the column is a \"ragged array,\" i.e. # whether each row of this column is a vector with potentially different lengths # for each row. units_table . add_column ( name = additional_attribute , description = ephys . CuratedClustering . Unit . heading . attributes [ additional_attribute ] . comment , index = additional_attribute == \"spike_depths\" , ) clustering_query = ( ephys . EphysRecording * ephys . ClusteringTask & session_key & paramset_record ) for unit in tqdm ( ( ephys . CuratedClustering . Unit & clustering_query . proj ()) . fetch ( as_dict = True ), desc = f \"creating units table for paramset { paramset_record [ 'paramset_idx' ] } \" , ): probe_id , shank_num = ( ephys . ProbeInsertion * ephys . CuratedClustering . Unit * probe . ProbeType . Electrode & dict (( k , unit [ k ]) for k in unit . keys () # excess keys caused errs if k not in [ 'spike_times' , 'spike_sites' , 'spike_depths' ]) ) . fetch1 ( \"probe\" , \"shank\" ) waveform_mean = ( ephys . WaveformSet . PeakWaveform () & clustering_query & unit ) . fetch1 ( \"peak_electrode_waveform\" ) units_table . add_row ( id = unit [ \"unit\" ], electrodes = [ mapping [( probe_id , unit [ \"electrode\" ])]], electrode_group = nwbfile . electrode_groups [ f \"probe { probe_id } _shank { shank_num } \" ], cluster_quality_label = unit [ \"cluster_quality_label\" ], spike_times = unit [ \"spike_times\" ], spike_depths = unit [ \"spike_depths\" ], waveform_mean = waveform_mean , ) return units_table", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb", "text": "Main function for converting ephys data to NWB", "title": "ecephys_session_to_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb--parameters", "text": "session_key: dict bool Whether to include the raw data from source. SpikeGLX & OpenEphys are supported bool Whether to include CuratedClustering lfp \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries to look up optional metadata dict, optional If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required minimal data for instantiating an NWBFile object. If element-session is being used, this argument can optionally be used to overwrite NWBFile fields. Source code in element_array_ephys/export/nwb/nwb.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 def ecephys_session_to_nwb ( session_key , raw = True , spikes = True , lfp = \"source\" , end_frame = None , lab_key = None , project_key = None , protocol_key = None , nwbfile_kwargs = None , ): \"\"\" Main function for converting ephys data to NWB Parameters ---------- session_key: dict raw: bool Whether to include the raw data from source. SpikeGLX & OpenEphys are supported spikes: bool Whether to include CuratedClustering lfp: \"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP end_frame: int, optional Used to create small test conversions where large datasets are truncated. lab_key, project_key, and protocol_key: dictionaries to look up optional metadata nwbfile_kwargs: dict, optional - If element-session is not being used, this argument is required and must be a dictionary containing 'session_description' (str), 'identifier' (str), and 'session_start_time' (datetime), the required minimal data for instantiating an NWBFile object. - If element-session is being used, this argument can optionally be used to overwrite NWBFile fields. \"\"\" session_to_nwb = getattr ( ephys . _linking_module , 'session_to_nwb' , False ) if session_to_nwb : nwbfile = session_to_nwb ( session_key , lab_key = lab_key , project_key = project_key , protocol_key = protocol_key , additional_nwbfile_kwargs = nwbfile_kwargs , ) else : nwbfile = pynwb . NWBFile ( ** nwbfile_kwargs ) ephys_root_data_dir = ephys . get_ephys_root_data_dir () if raw : add_ephys_recording_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) if spikes : add_ephys_units_to_nwb ( session_key , nwbfile ) if lfp == \"dj\" : add_ephys_lfp_from_dj_to_nwb ( session_key , nwbfile ) if lfp == \"source\" : add_ephys_lfp_from_source_to_nwb ( session_key , ephys_root_data_dir = ephys_root_data_dir , nwbfile = nwbfile , end_frame = end_frame ) return nwbfile", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper", "text": "This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the channel_conversion field in addition to the conversion field so that each channel can be converted to volts using its own individual gain.", "title": "gains_helper()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper--parameters", "text": "gains: np.ndarray", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper--returns", "text": "dict conversion : float channel_conversion : np.ndarray Source code in element_array_ephys/export/nwb/nwb.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def gains_helper ( gains ): \"\"\" This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the `channel_conversion` field in addition to the `conversion` field so that each channel can be converted to volts using its own individual gain. Parameters ---------- gains: np.ndarray Returns ------- dict conversion : float channel_conversion : np.ndarray \"\"\" if all ( x == 1 for x in gains ): return dict ( conversion = 1e-6 , channel_conversion = None ) if all ( x == gains [ 0 ] for x in gains ): return dict ( conversion = 1e-6 * gains [ 0 ], channel_conversion = None ) return dict ( conversion = 1e-6 , channel_conversion = gains )", "title": "Returns"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping", "text": "Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries.", "title": "get_electrodes_mapping()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping--parameters", "text": "electrodes: hdmf.common.table.DynamicTable", "title": "Parameters"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping--returns", "text": "dict Source code in element_array_ephys/export/nwb/nwb.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def get_electrodes_mapping ( electrodes ): \"\"\" Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries. Parameters ---------- electrodes: hdmf.common.table.DynamicTable Returns ------- dict \"\"\" return { ( electrodes [ \"group\" ][ idx ] . device . name , electrodes [ \"id_in_probe\" ][ idx ],): idx for idx in range ( len ( electrodes )) }", "title": "Returns"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb", "text": "Export NWBFile", "title": "write_nwb()"}, {"location": "api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb--parameters", "text": "nwbfile: pynwb.NWBFile fname: str Absolute path including *.nwb extension. bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. Source code in element_array_ephys/export/nwb/nwb.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def write_nwb ( nwbfile , fname , check_read = True ): \"\"\" Export NWBFile Parameters ---------- nwbfile: pynwb.NWBFile fname: str Absolute path including `*.nwb` extension. check_read: bool If True, PyNWB will try to read the produced NWB file and ensure that it can be read. \"\"\" with pynwb . NWBHDF5IO ( fname , \"w\" ) as io : io . write ( nwbfile ) if check_read : with pynwb . NWBHDF5IO ( fname , \"r\" ) as io : io . read ()", "title": "Parameters"}, {"location": "api/element_array_ephys/plotting/probe_level/", "text": "", "title": "probe_level.py"}, {"location": "api/element_array_ephys/plotting/unit_level/", "text": "", "title": "unit_level.py"}, {"location": "api/element_array_ephys/plotting/widget/", "text": "", "title": "widget.py"}, {"location": "api/element_array_ephys/readers/kilosort/", "text": "Kilosort \u00b6 Source code in element_array_ephys/readers/kilosort.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class Kilosort : _kilosort_core_files = [ 'params.py' , 'amplitudes.npy' , 'channel_map.npy' , 'channel_positions.npy' , 'pc_features.npy' , 'pc_feature_ind.npy' , 'similar_templates.npy' , 'spike_templates.npy' , 'spike_times.npy' , 'template_features.npy' , 'template_feature_ind.npy' , 'templates.npy' , 'templates_ind.npy' , 'whitening_mat.npy' , 'whitening_mat_inv.npy' , 'spike_clusters.npy' ] _kilosort_additional_files = [ 'spike_times_sec.npy' , 'spike_times_sec_adj.npy' , 'cluster_groups.csv' , 'cluster_KSLabel.tsv' ] kilosort_files = _kilosort_core_files + _kilosort_additional_files def __init__ ( self , kilosort_dir ): self . _kilosort_dir = pathlib . Path ( kilosort_dir ) self . _files = {} self . _data = None self . _clusters = None self . validate () params_filepath = kilosort_dir / 'params.py' self . _info = { 'time_created' : datetime . fromtimestamp ( params_filepath . stat () . st_ctime ), 'time_modified' : datetime . fromtimestamp ( params_filepath . stat () . st_mtime )} @property def data ( self ): if self . _data is None : self . _load () return self . _data @property def info ( self ): return self . _info def validate ( self ): \"\"\" Check if this is a valid set of kilosort outputs - i.e. all crucial files exist \"\"\" missing_files = [] for f in Kilosort . _kilosort_core_files : full_path = self . _kilosort_dir / f if not full_path . exists (): missing_files . append ( f ) if missing_files : raise FileNotFoundError ( f 'Kilosort files missing in ( { self . _kilosort_dir } ):' f ' { missing_files } ' ) def _load ( self ): self . _data = {} for kilosort_filename in Kilosort . kilosort_files : kilosort_filepath = self . _kilosort_dir / kilosort_filename if not kilosort_filepath . exists (): log . debug ( 'skipping {} - does not exist' . format ( kilosort_filepath )) continue base , ext = path . splitext ( kilosort_filename ) self . _files [ base ] = kilosort_filepath if kilosort_filename == 'params.py' : log . debug ( 'loading params.py {} ' . format ( kilosort_filepath )) # params.py is a 'key = val' file params = {} for line in open ( kilosort_filepath , 'r' ) . readlines (): k , v = line . strip ( ' \\n ' ) . split ( '=' ) params [ k . strip ()] = convert_to_number ( v . strip ()) log . debug ( 'params: {} ' . format ( params )) self . _data [ base ] = params if ext == '.npy' : log . debug ( 'loading npy {} ' . format ( kilosort_filepath )) d = np . load ( kilosort_filepath , mmap_mode = 'r' , allow_pickle = False , fix_imports = False ) self . _data [ base ] = ( np . reshape ( d , d . shape [ 0 ]) if d . ndim == 2 and d . shape [ 1 ] == 1 else d ) self . _data [ 'channel_map' ] = self . _data [ 'channel_map' ] . flatten () # Read the Cluster Groups for cluster_pattern , cluster_col_name in zip ([ 'cluster_group.*' , 'cluster_KSLabel.*' ], [ 'group' , 'KSLabel' ]): try : cluster_file = next ( self . _kilosort_dir . glob ( cluster_pattern )) except StopIteration : pass else : cluster_file_suffix = cluster_file . suffix assert cluster_file_suffix in ( '.tsv' , '.xlsx' ) break else : raise FileNotFoundError ( 'Neither \"cluster_groups\" nor \"cluster_KSLabel\" file found!' ) if cluster_file_suffix == '.tsv' : df = pd . read_csv ( cluster_file , sep = ' \\t ' , header = 0 ) elif cluster_file_suffix == '.xlsx' : df = pd . read_excel ( cluster_file , engine = 'openpyxl' ) else : df = pd . read_csv ( cluster_file , delimiter = ' \\t ' ) self . _data [ 'cluster_groups' ] = np . array ( df [ cluster_col_name ] . values ) self . _data [ 'cluster_ids' ] = np . array ( df [ 'cluster_id' ] . values ) def get_best_channel ( self , unit ): template_idx = self . data [ 'spike_templates' ][ np . where ( self . data [ 'spike_clusters' ] == unit )[ 0 ][ 0 ]] channel_templates = self . data [ 'templates' ][ template_idx , :, :] max_channel_idx = np . abs ( channel_templates ) . max ( axis = 0 ) . argmax () max_channel = self . data [ 'channel_map' ][ max_channel_idx ] return max_channel , max_channel_idx def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ] extract_spike_depths () \u00b6 Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m Source code in element_array_ephys/readers/kilosort.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ] validate () \u00b6 Check if this is a valid set of kilosort outputs - i.e. all crucial files exist Source code in element_array_ephys/readers/kilosort.py 65 66 67 68 69 70 71 72 73 74 75 76 def validate ( self ): \"\"\" Check if this is a valid set of kilosort outputs - i.e. all crucial files exist \"\"\" missing_files = [] for f in Kilosort . _kilosort_core_files : full_path = self . _kilosort_dir / f if not full_path . exists (): missing_files . append ( f ) if missing_files : raise FileNotFoundError ( f 'Kilosort files missing in ( { self . _kilosort_dir } ):' f ' { missing_files } ' )", "title": "kilosort.py"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort", "text": "Source code in element_array_ephys/readers/kilosort.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class Kilosort : _kilosort_core_files = [ 'params.py' , 'amplitudes.npy' , 'channel_map.npy' , 'channel_positions.npy' , 'pc_features.npy' , 'pc_feature_ind.npy' , 'similar_templates.npy' , 'spike_templates.npy' , 'spike_times.npy' , 'template_features.npy' , 'template_feature_ind.npy' , 'templates.npy' , 'templates_ind.npy' , 'whitening_mat.npy' , 'whitening_mat_inv.npy' , 'spike_clusters.npy' ] _kilosort_additional_files = [ 'spike_times_sec.npy' , 'spike_times_sec_adj.npy' , 'cluster_groups.csv' , 'cluster_KSLabel.tsv' ] kilosort_files = _kilosort_core_files + _kilosort_additional_files def __init__ ( self , kilosort_dir ): self . _kilosort_dir = pathlib . Path ( kilosort_dir ) self . _files = {} self . _data = None self . _clusters = None self . validate () params_filepath = kilosort_dir / 'params.py' self . _info = { 'time_created' : datetime . fromtimestamp ( params_filepath . stat () . st_ctime ), 'time_modified' : datetime . fromtimestamp ( params_filepath . stat () . st_mtime )} @property def data ( self ): if self . _data is None : self . _load () return self . _data @property def info ( self ): return self . _info def validate ( self ): \"\"\" Check if this is a valid set of kilosort outputs - i.e. all crucial files exist \"\"\" missing_files = [] for f in Kilosort . _kilosort_core_files : full_path = self . _kilosort_dir / f if not full_path . exists (): missing_files . append ( f ) if missing_files : raise FileNotFoundError ( f 'Kilosort files missing in ( { self . _kilosort_dir } ):' f ' { missing_files } ' ) def _load ( self ): self . _data = {} for kilosort_filename in Kilosort . kilosort_files : kilosort_filepath = self . _kilosort_dir / kilosort_filename if not kilosort_filepath . exists (): log . debug ( 'skipping {} - does not exist' . format ( kilosort_filepath )) continue base , ext = path . splitext ( kilosort_filename ) self . _files [ base ] = kilosort_filepath if kilosort_filename == 'params.py' : log . debug ( 'loading params.py {} ' . format ( kilosort_filepath )) # params.py is a 'key = val' file params = {} for line in open ( kilosort_filepath , 'r' ) . readlines (): k , v = line . strip ( ' \\n ' ) . split ( '=' ) params [ k . strip ()] = convert_to_number ( v . strip ()) log . debug ( 'params: {} ' . format ( params )) self . _data [ base ] = params if ext == '.npy' : log . debug ( 'loading npy {} ' . format ( kilosort_filepath )) d = np . load ( kilosort_filepath , mmap_mode = 'r' , allow_pickle = False , fix_imports = False ) self . _data [ base ] = ( np . reshape ( d , d . shape [ 0 ]) if d . ndim == 2 and d . shape [ 1 ] == 1 else d ) self . _data [ 'channel_map' ] = self . _data [ 'channel_map' ] . flatten () # Read the Cluster Groups for cluster_pattern , cluster_col_name in zip ([ 'cluster_group.*' , 'cluster_KSLabel.*' ], [ 'group' , 'KSLabel' ]): try : cluster_file = next ( self . _kilosort_dir . glob ( cluster_pattern )) except StopIteration : pass else : cluster_file_suffix = cluster_file . suffix assert cluster_file_suffix in ( '.tsv' , '.xlsx' ) break else : raise FileNotFoundError ( 'Neither \"cluster_groups\" nor \"cluster_KSLabel\" file found!' ) if cluster_file_suffix == '.tsv' : df = pd . read_csv ( cluster_file , sep = ' \\t ' , header = 0 ) elif cluster_file_suffix == '.xlsx' : df = pd . read_excel ( cluster_file , engine = 'openpyxl' ) else : df = pd . read_csv ( cluster_file , delimiter = ' \\t ' ) self . _data [ 'cluster_groups' ] = np . array ( df [ cluster_col_name ] . values ) self . _data [ 'cluster_ids' ] = np . array ( df [ 'cluster_id' ] . values ) def get_best_channel ( self , unit ): template_idx = self . data [ 'spike_templates' ][ np . where ( self . data [ 'spike_clusters' ] == unit )[ 0 ][ 0 ]] channel_templates = self . data [ 'templates' ][ template_idx , :, :] max_channel_idx = np . abs ( channel_templates ) . max ( axis = 0 ) . argmax () max_channel = self . data [ 'channel_map' ][ max_channel_idx ] return max_channel , max_channel_idx def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ]", "title": "Kilosort"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.extract_spike_depths", "text": "Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m Source code in element_array_ephys/readers/kilosort.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def extract_spike_depths ( self ): \"\"\" Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m \"\"\" if 'pc_features' in self . data : ycoords = self . data [ 'channel_positions' ][:, 1 ] pc_features = self . data [ 'pc_features' ][:, 0 , :] # 1st PC only pc_features = np . where ( pc_features < 0 , 0 , pc_features ) # ---- compute center of mass of these features (spike depths) ---- # which channels for each spike? spk_feature_ind = self . data [ 'pc_feature_ind' ][ self . data [ 'spike_templates' ], :] # ycoords of those channels? spk_feature_ycoord = ycoords [ spk_feature_ind ] # center of mass is sum(coords.*features)/sum(features) self . _data [ 'spike_depths' ] = ( np . sum ( spk_feature_ycoord * pc_features ** 2 , axis = 1 ) / np . sum ( pc_features ** 2 , axis = 1 )) else : self . _data [ 'spike_depths' ] = None # ---- extract spike sites ---- max_site_ind = np . argmax ( np . abs ( self . data [ 'templates' ]) . max ( axis = 1 ), axis = 1 ) spike_site_ind = max_site_ind [ self . data [ 'spike_templates' ]] self . _data [ 'spike_sites' ] = self . data [ 'channel_map' ][ spike_site_ind ]", "title": "extract_spike_depths()"}, {"location": "api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.validate", "text": "Check if this is a valid set of kilosort outputs - i.e. all crucial files exist Source code in element_array_ephys/readers/kilosort.py 65 66 67 68 69 70 71 72 73 74 75 76 def validate ( self ): \"\"\" Check if this is a valid set of kilosort outputs - i.e. all crucial files exist \"\"\" missing_files = [] for f in Kilosort . _kilosort_core_files : full_path = self . _kilosort_dir / f if not full_path . exists (): missing_files . append ( f ) if missing_files : raise FileNotFoundError ( f 'Kilosort files missing in ( { self . _kilosort_dir } ):' f ' { missing_files } ' )", "title": "validate()"}, {"location": "api/element_array_ephys/readers/kilosort_triggering/", "text": "OpenEphysKilosortPipeline \u00b6 An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Open Ephys acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Which is based on ecephys_spike_sorting routines from Allen Institute https://github.com/AllenInstitute/ecephys_spike_sorting Source code in element_array_ephys/readers/kilosort_triggering.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 class OpenEphysKilosortPipeline : \"\"\" An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Open Ephys acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Which is based on `ecephys_spike_sorting` routines from Allen Institute https://github.com/AllenInstitute/ecephys_spike_sorting \"\"\" _modules = [ 'depth_estimation' , 'median_subtraction' , 'kilosort_helper' , 'kilosort_postprocessing' , 'noise_templates' , 'mean_waveforms' , 'quality_metrics' ] _input_json_args = list ( inspect . signature ( createInputJson ) . parameters ) def __init__ ( self , npx_input_dir : str , ks_output_dir : str , params : dict , KS2ver : str ): self . _npx_input_dir = pathlib . Path ( npx_input_dir ) self . _ks_output_dir = pathlib . Path ( ks_output_dir ) self . _ks_output_dir . mkdir ( parents = True , exist_ok = True ) self . _params = params self . _KS2ver = KS2ver self . _json_directory = self . _ks_output_dir / 'json_configs' self . _json_directory . mkdir ( parents = True , exist_ok = True ) self . _median_subtraction_finished = False self . ks_input_params = None self . _modules_input_hash = None self . _modules_input_hash_fp = None def make_chanmap_file ( self ): continuous_file = self . _npx_input_dir / 'continuous.dat' self . _chanmap_filepath = self . _ks_output_dir / 'chanMap.mat' _write_channel_map_file ( channel_ind = self . _params [ 'channel_ind' ], x_coords = self . _params [ 'x_coords' ], y_coords = self . _params [ 'y_coords' ], shank_ind = self . _params [ 'shank_ind' ], connected = self . _params [ 'connected' ], probe_name = self . _params [ 'probe_type' ], ap_band_file = continuous_file . as_posix (), bit_volts = self . _params [ 'uVPerBit' ], sample_rate = self . _params [ 'sample_rate' ], save_path = self . _chanmap_filepath . as_posix (), is_0_based = True ) def generate_modules_input_json ( self ): self . make_chanmap_file () self . _module_input_json = self . _json_directory / f ' { self . _npx_input_dir . name } -input.json' continuous_file = self . _get_raw_data_filepaths () lf_dir = self . _npx_input_dir . as_posix () try : # old probe folder convention with 100.0, 100.1, 100.2, 100.3, etc. name , num = re . search ( r \"(.+\\.)(\\d)+$\" , lf_dir ) . groups () except AttributeError : # new probe folder convention with -AP or -LFP assert lf_dir . endswith ( \"AP\" ) lf_dir = re . sub ( \"-AP$\" , \"-LFP\" , lf_dir ) else : lf_dir = f \" { name }{ int ( num ) + 1 } \" lf_file = pathlib . Path ( lf_dir ) / 'continuous.dat' params = {} for k , v in self . _params . items (): value = str ( v ) if isinstance ( v , list ) else v if f 'ks_ { k } ' in self . _input_json_args : params [ f 'ks_ { k } ' ] = value if k in self . _input_json_args : params [ k ] = value self . ks_input_params = createInputJson ( self . _module_input_json . as_posix (), KS2ver = self . _KS2ver , npx_directory = self . _npx_input_dir . as_posix (), spikeGLX_data = False , continuous_file = continuous_file . as_posix (), lf_file = lf_file . as_posix (), extracted_data_directory = self . _ks_output_dir . as_posix (), kilosort_output_directory = self . _ks_output_dir . as_posix (), kilosort_output_tmp = self . _ks_output_dir . as_posix (), ks_make_copy = True , noise_template_use_rf = self . _params . get ( 'noise_template_use_rf' , False ), use_C_Waves = False , c_Waves_snr_um = self . _params . get ( 'c_Waves_snr_um' , 160 ), qm_isi_thresh = self . _params . get ( 'refPerMS' , 2.0 ) / 1000 , kilosort_repository = _get_kilosort_repository ( self . _KS2ver ), chanMap_path = self . _chanmap_filepath . as_posix (), ** params ) self . _modules_input_hash = dict_to_uuid ( self . ks_input_params ) def run_modules ( self ): print ( '---- Running Modules ----' ) self . generate_modules_input_json () module_input_json = self . _module_input_json . as_posix () module_logfile = module_input_json . replace ( '-input.json' , '-run_modules-log.txt' ) for module in self . _modules : module_status = self . _get_module_status ( module ) if module_status [ 'completion_time' ] is not None : continue if module == 'median_subtraction' and self . _median_subtraction_finished : self . _update_module_status ( { module : { 'start_time' : datetime . utcnow (), 'completion_time' : datetime . utcnow (), 'duration' : 0 }}) continue module_output_json = self . _get_module_output_json_filename ( module ) command = [ sys . executable , '-W' , 'ignore' , '-m' , 'ecephys_spike_sorting.modules.' + module , '--input_json' , module_input_json , '--output_json' , module_output_json ] start_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : None , 'duration' : None }}) with open ( module_logfile , \"a\" ) as f : subprocess . check_call ( command , stdout = f ) completion_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : completion_time , 'duration' : ( completion_time - start_time ) . total_seconds ()}}) self . _update_total_duration () def _get_raw_data_filepaths ( self ): raw_ap_fp = self . _npx_input_dir / 'continuous.dat' if 'median_subtraction' not in self . _modules : return raw_ap_fp # median subtraction step will overwrite original continuous.dat file with the corrected version # to preserve the original raw data - make a copy here and work on the copied version assert 'depth_estimation' in self . _modules continuous_file = self . _ks_output_dir / 'continuous.dat' if continuous_file . exists (): if raw_ap_fp . stat () . st_mtime < continuous_file . stat () . st_mtime : # if the copied continuous.dat was actually modified, # median_subtraction may have been completed - let's check module_input_json = self . _module_input_json . as_posix () module_logfile = module_input_json . replace ( '-input.json' , '-run_modules-log.txt' ) with open ( module_logfile , 'r' ) as f : previous_line = '' for line in f . readlines (): if ( line . startswith ( 'ecephys spike sorting: median subtraction module' ) and previous_line . startswith ( 'Total processing time:' )): self . _median_subtraction_finished = True return continuous_file previous_line = line shutil . copy2 ( raw_ap_fp , continuous_file ) return continuous_file def _update_module_status ( self , updated_module_status = {}): if self . _modules_input_hash is None : raise RuntimeError ( '\"generate_modules_input_json()\" not yet performed!' ) self . _modules_input_hash_fp = self . _json_directory / f '. { self . _modules_input_hash } .json' if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) modules_status = { ** modules_status , ** updated_module_status } else : modules_status = { module : { 'start_time' : None , 'completion_time' : None , 'duration' : None } for module in self . _modules } with open ( self . _modules_input_hash_fp , 'w' ) as f : json . dump ( modules_status , f , default = str ) def _get_module_status ( self , module ): if self . _modules_input_hash_fp is None : self . _update_module_status () if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) if modules_status [ module ][ 'completion_time' ] is None : # additional logic to read from the \"-output.json\" file for this module as well # handle cases where the module has finished successfully, # but the \"_modules_input_hash_fp\" is not updated (for whatever reason), # resulting in this module not registered as completed in the \"_modules_input_hash_fp\" module_output_json_fp = pathlib . Path ( self . _get_module_output_json_filename ( module )) if module_output_json_fp . exists (): with open ( module_output_json_fp ) as f : module_run_output = json . load ( f ) modules_status [ module ][ 'duration' ] = module_run_output [ 'execution_time' ] modules_status [ module ][ 'completion_time' ] = ( datetime . strptime ( modules_status [ module ][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) + timedelta ( seconds = module_run_output [ 'execution_time' ])) return modules_status [ module ] return { 'start_time' : None , 'completion_time' : None , 'duration' : None } def _get_module_output_json_filename ( self , module ): module_input_json = self . _module_input_json . as_posix () module_output_json = module_input_json . replace ( '-input.json' , '-' + module + '-' + str ( self . _modules_input_hash ) + '-output.json' ) return module_output_json def _update_total_duration ( self ): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) cumulative_execution_duration = sum ( v [ 'duration' ] or 0 for k , v in modules_status . items () if k not in ( 'cumulative_execution_duration' , 'total_duration' )) total_duration = ( datetime . strptime ( modules_status [ self . _modules [ - 1 ]][ 'completion_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) - datetime . strptime ( modules_status [ self . _modules [ 0 ]][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) ) . total_seconds () self . _update_module_status ( { 'cumulative_execution_duration' : cumulative_execution_duration , 'total_duration' : total_duration }) SGLXKilosortPipeline \u00b6 An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Spike GLX acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Source code in element_array_ephys/readers/kilosort_triggering.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 class SGLXKilosortPipeline : \"\"\" An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Spike GLX acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting \"\"\" _modules = [ 'kilosort_helper' , 'kilosort_postprocessing' , 'noise_templates' , 'mean_waveforms' , 'quality_metrics' ] _default_catgt_params = { 'catGT_car_mode' : 'gblcar' , 'catGT_loccar_min_um' : 40 , 'catGT_loccar_max_um' : 160 , 'catGT_cmd_string' : '-prb_fld -out_prb_fld -gfix=0.4,0.10,0.02' , 'ni_present' : False , 'ni_extract_string' : '-XA=0,1,3,500 -iXA=1,3,3,0 -XD=-1,1,50 -XD=-1,2,1.7 -XD=-1,3,5 -iXD=-1,3,5' } _input_json_args = list ( inspect . signature ( createInputJson ) . parameters ) def __init__ ( self , npx_input_dir : str , ks_output_dir : str , params : dict , KS2ver : str , run_CatGT = False , ni_present = False , ni_extract_string = None ): self . _npx_input_dir = pathlib . Path ( npx_input_dir ) self . _ks_output_dir = pathlib . Path ( ks_output_dir ) self . _ks_output_dir . mkdir ( parents = True , exist_ok = True ) self . _params = params self . _KS2ver = KS2ver self . _run_CatGT = run_CatGT self . _run_CatGT = run_CatGT self . _default_catgt_params [ 'ni_present' ] = ni_present self . _default_catgt_params [ 'ni_extract_string' ] = ni_extract_string or self . _default_catgt_params [ 'ni_extract_string' ] self . _json_directory = self . _ks_output_dir / 'json_configs' self . _json_directory . mkdir ( parents = True , exist_ok = True ) self . _CatGT_finished = False self . ks_input_params = None self . _modules_input_hash = None self . _modules_input_hash_fp = None def parse_input_filename ( self ): meta_filename = next ( self . _npx_input_dir . glob ( '*.ap.meta' )) . name match = re . search ( '(.*)_g(\\d)_t(\\d+|cat)\\.imec(\\d?)\\.ap\\.meta' , meta_filename ) session_str , gate_str , trigger_str , probe_str = match . groups () return session_str , gate_str , trigger_str , probe_str or '0' def generate_CatGT_input_json ( self ): if not self . _run_CatGT : print ( 'run_CatGT is set to False, skipping...' ) return session_str , gate_str , trig_str , probe_str = self . parse_input_filename () first_trig , last_trig = SpikeGLX_utils . ParseTrigStr ( 'start,end' , probe_str , gate_str , self . _npx_input_dir . as_posix ()) trigger_str = repr ( first_trig ) + ',' + repr ( last_trig ) self . _catGT_input_json = self . _json_directory / f ' { session_str }{ probe_str } _CatGT-input.json' catgt_params = { k : self . _params . get ( k , v ) for k , v in self . _default_catgt_params . items ()} ni_present = catgt_params . pop ( 'ni_present' ) ni_extract_string = catgt_params . pop ( 'ni_extract_string' ) catgt_params [ 'catGT_stream_string' ] = '-ap -ni' if ni_present else '-ap' sync_extract = '-SY=' + probe_str + ',-1,6,500' extract_string = sync_extract + ( f ' { ni_extract_string } ' if ni_present else '' ) catgt_params [ 'catGT_cmd_string' ] += f ' { extract_string } ' input_meta_fullpath , continuous_file = self . _get_raw_data_filepaths () # create symbolic link to the actual data files - as CatGT expects files to follow a certain naming convention continuous_file_symlink = ( continuous_file . parent / f ' { session_str } _g { gate_str } ' / f ' { session_str } _g { gate_str } _imec { probe_str } ' / f ' { session_str } _g { gate_str } _t { trig_str } .imec { probe_str } .ap.bin' ) continuous_file_symlink . parent . mkdir ( parents = True , exist_ok = True ) if continuous_file_symlink . exists (): continuous_file_symlink . unlink () continuous_file_symlink . symlink_to ( continuous_file ) input_meta_fullpath_symlink = ( input_meta_fullpath . parent / f ' { session_str } _g { gate_str } ' / f ' { session_str } _g { gate_str } _imec { probe_str } ' / f ' { session_str } _g { gate_str } _t { trig_str } .imec { probe_str } .ap.meta' ) input_meta_fullpath_symlink . parent . mkdir ( parents = True , exist_ok = True ) if input_meta_fullpath_symlink . exists (): input_meta_fullpath_symlink . unlink () input_meta_fullpath_symlink . symlink_to ( input_meta_fullpath ) createInputJson ( self . _catGT_input_json . as_posix (), KS2ver = self . _KS2ver , npx_directory = self . _npx_input_dir . as_posix (), spikeGLX_data = True , catGT_run_name = session_str , gate_string = gate_str , trigger_string = trigger_str , probe_string = probe_str , continuous_file = continuous_file . as_posix (), input_meta_path = input_meta_fullpath . as_posix (), extracted_data_directory = self . _ks_output_dir . as_posix (), kilosort_output_directory = self . _ks_output_dir . as_posix (), kilosort_output_tmp = self . _ks_output_dir . as_posix (), kilosort_repository = _get_kilosort_repository ( self . _KS2ver ), ** { k : v for k , v in catgt_params . items () if k in self . _input_json_args } ) def run_CatGT ( self , force_rerun = False ): if self . _run_CatGT and ( not self . _CatGT_finished or force_rerun ): self . generate_CatGT_input_json () print ( '---- Running CatGT ----' ) catGT_input_json = self . _catGT_input_json . as_posix () catGT_output_json = catGT_input_json . replace ( 'CatGT-input.json' , 'CatGT-output.json' ) command = ( sys . executable + \" -W ignore -m ecephys_spike_sorting.modules.\" + 'catGT_helper' + \" --input_json \" + catGT_input_json + \" --output_json \" + catGT_output_json ) subprocess . check_call ( command . split ( ' ' )) self . _CatGT_finished = True def generate_modules_input_json ( self ): session_str , _ , _ , probe_str = self . parse_input_filename () self . _module_input_json = self . _json_directory / f ' { session_str } _imec { probe_str } -input.json' input_meta_fullpath , continuous_file = self . _get_raw_data_filepaths () params = {} for k , v in self . _params . items (): value = str ( v ) if isinstance ( v , list ) else v if f 'ks_ { k } ' in self . _input_json_args : params [ f 'ks_ { k } ' ] = value if k in self . _input_json_args : params [ k ] = value self . ks_input_params = createInputJson ( self . _module_input_json . as_posix (), KS2ver = self . _KS2ver , npx_directory = self . _npx_input_dir . as_posix (), spikeGLX_data = True , continuous_file = continuous_file . as_posix (), input_meta_path = input_meta_fullpath . as_posix (), extracted_data_directory = self . _ks_output_dir . as_posix (), kilosort_output_directory = self . _ks_output_dir . as_posix (), kilosort_output_tmp = self . _ks_output_dir . as_posix (), ks_make_copy = True , noise_template_use_rf = self . _params . get ( 'noise_template_use_rf' , False ), c_Waves_snr_um = self . _params . get ( 'c_Waves_snr_um' , 160 ), qm_isi_thresh = self . _params . get ( 'refPerMS' , 2.0 ) / 1000 , kilosort_repository = _get_kilosort_repository ( self . _KS2ver ), ** params ) self . _modules_input_hash = dict_to_uuid ( self . ks_input_params ) def run_modules ( self ): if self . _run_CatGT and not self . _CatGT_finished : self . run_CatGT () print ( '---- Running Modules ----' ) self . generate_modules_input_json () module_input_json = self . _module_input_json . as_posix () module_logfile = module_input_json . replace ( '-input.json' , '-run_modules-log.txt' ) for module in self . _modules : module_status = self . _get_module_status ( module ) if module_status [ 'completion_time' ] is not None : continue module_output_json = self . _get_module_output_json_filename ( module ) command = ( sys . executable + \" -W ignore -m ecephys_spike_sorting.modules.\" + module + \" --input_json \" + module_input_json + \" --output_json \" + module_output_json ) start_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : None , 'duration' : None }}) with open ( module_logfile , \"a\" ) as f : subprocess . check_call ( command . split ( ' ' ), stdout = f ) completion_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : completion_time , 'duration' : ( completion_time - start_time ) . total_seconds ()}}) self . _update_total_duration () def _get_raw_data_filepaths ( self ): session_str , gate_str , _ , probe_str = self . parse_input_filename () if self . _CatGT_finished : catGT_dest = self . _ks_output_dir run_str = session_str + '_g' + gate_str run_folder = 'catgt_' + run_str prb_folder = run_str + '_imec' + probe_str data_directory = catGT_dest / run_folder / prb_folder else : data_directory = self . _npx_input_dir try : meta_fp = next ( data_directory . glob ( f ' { session_str } *.ap.meta' )) bin_fp = next ( data_directory . glob ( f ' { session_str } *.ap.bin' )) except StopIteration : raise RuntimeError ( f 'No ap meta/bin files found in { data_directory } - CatGT error?' ) return meta_fp , bin_fp def _update_module_status ( self , updated_module_status = {}): if self . _modules_input_hash is None : raise RuntimeError ( '\"generate_modules_input_json()\" not yet performed!' ) self . _modules_input_hash_fp = self . _json_directory / f '. { self . _modules_input_hash } .json' if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) modules_status = { ** modules_status , ** updated_module_status } else : modules_status = { module : { 'start_time' : None , 'completion_time' : None , 'duration' : None } for module in self . _modules } with open ( self . _modules_input_hash_fp , 'w' ) as f : json . dump ( modules_status , f , default = str ) def _get_module_status ( self , module ): if self . _modules_input_hash_fp is None : self . _update_module_status () if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) if modules_status [ module ][ 'completion_time' ] is None : # additional logic to read from the \"-output.json\" file for this module as well # handle cases where the module has finished successfully, # but the \"_modules_input_hash_fp\" is not updated (for whatever reason), # resulting in this module not registered as completed in the \"_modules_input_hash_fp\" module_output_json_fp = pathlib . Path ( self . _get_module_output_json_filename ( module )) if module_output_json_fp . exists (): with open ( module_output_json_fp ) as f : module_run_output = json . load ( f ) modules_status [ module ][ 'duration' ] = module_run_output [ 'execution_time' ] modules_status [ module ][ 'completion_time' ] = ( datetime . strptime ( modules_status [ module ][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) + timedelta ( seconds = module_run_output [ 'execution_time' ])) return modules_status [ module ] return { 'start_time' : None , 'completion_time' : None , 'duration' : None } def _get_module_output_json_filename ( self , module ): module_input_json = self . _module_input_json . as_posix () module_output_json = module_input_json . replace ( '-input.json' , '-' + module + '-' + str ( self . _modules_input_hash ) + '-output.json' ) return module_output_json def _update_total_duration ( self ): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) cumulative_execution_duration = sum ( v [ 'duration' ] or 0 for k , v in modules_status . items () if k not in ( 'cumulative_execution_duration' , 'total_duration' )) total_duration = ( datetime . strptime ( modules_status [ self . _modules [ - 1 ]][ 'completion_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) - datetime . strptime ( modules_status [ self . _modules [ 0 ]][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) ) . total_seconds () self . _update_module_status ( { 'cumulative_execution_duration' : cumulative_execution_duration , 'total_duration' : total_duration })", "title": "kilosort_triggering.py"}, {"location": "api/element_array_ephys/readers/kilosort_triggering/#element_array_ephys.readers.kilosort_triggering.OpenEphysKilosortPipeline", "text": "An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Open Ephys acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Which is based on ecephys_spike_sorting routines from Allen Institute https://github.com/AllenInstitute/ecephys_spike_sorting Source code in element_array_ephys/readers/kilosort_triggering.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 class OpenEphysKilosortPipeline : \"\"\" An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Open Ephys acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Which is based on `ecephys_spike_sorting` routines from Allen Institute https://github.com/AllenInstitute/ecephys_spike_sorting \"\"\" _modules = [ 'depth_estimation' , 'median_subtraction' , 'kilosort_helper' , 'kilosort_postprocessing' , 'noise_templates' , 'mean_waveforms' , 'quality_metrics' ] _input_json_args = list ( inspect . signature ( createInputJson ) . parameters ) def __init__ ( self , npx_input_dir : str , ks_output_dir : str , params : dict , KS2ver : str ): self . _npx_input_dir = pathlib . Path ( npx_input_dir ) self . _ks_output_dir = pathlib . Path ( ks_output_dir ) self . _ks_output_dir . mkdir ( parents = True , exist_ok = True ) self . _params = params self . _KS2ver = KS2ver self . _json_directory = self . _ks_output_dir / 'json_configs' self . _json_directory . mkdir ( parents = True , exist_ok = True ) self . _median_subtraction_finished = False self . ks_input_params = None self . _modules_input_hash = None self . _modules_input_hash_fp = None def make_chanmap_file ( self ): continuous_file = self . _npx_input_dir / 'continuous.dat' self . _chanmap_filepath = self . _ks_output_dir / 'chanMap.mat' _write_channel_map_file ( channel_ind = self . _params [ 'channel_ind' ], x_coords = self . _params [ 'x_coords' ], y_coords = self . _params [ 'y_coords' ], shank_ind = self . _params [ 'shank_ind' ], connected = self . _params [ 'connected' ], probe_name = self . _params [ 'probe_type' ], ap_band_file = continuous_file . as_posix (), bit_volts = self . _params [ 'uVPerBit' ], sample_rate = self . _params [ 'sample_rate' ], save_path = self . _chanmap_filepath . as_posix (), is_0_based = True ) def generate_modules_input_json ( self ): self . make_chanmap_file () self . _module_input_json = self . _json_directory / f ' { self . _npx_input_dir . name } -input.json' continuous_file = self . _get_raw_data_filepaths () lf_dir = self . _npx_input_dir . as_posix () try : # old probe folder convention with 100.0, 100.1, 100.2, 100.3, etc. name , num = re . search ( r \"(.+\\.)(\\d)+$\" , lf_dir ) . groups () except AttributeError : # new probe folder convention with -AP or -LFP assert lf_dir . endswith ( \"AP\" ) lf_dir = re . sub ( \"-AP$\" , \"-LFP\" , lf_dir ) else : lf_dir = f \" { name }{ int ( num ) + 1 } \" lf_file = pathlib . Path ( lf_dir ) / 'continuous.dat' params = {} for k , v in self . _params . items (): value = str ( v ) if isinstance ( v , list ) else v if f 'ks_ { k } ' in self . _input_json_args : params [ f 'ks_ { k } ' ] = value if k in self . _input_json_args : params [ k ] = value self . ks_input_params = createInputJson ( self . _module_input_json . as_posix (), KS2ver = self . _KS2ver , npx_directory = self . _npx_input_dir . as_posix (), spikeGLX_data = False , continuous_file = continuous_file . as_posix (), lf_file = lf_file . as_posix (), extracted_data_directory = self . _ks_output_dir . as_posix (), kilosort_output_directory = self . _ks_output_dir . as_posix (), kilosort_output_tmp = self . _ks_output_dir . as_posix (), ks_make_copy = True , noise_template_use_rf = self . _params . get ( 'noise_template_use_rf' , False ), use_C_Waves = False , c_Waves_snr_um = self . _params . get ( 'c_Waves_snr_um' , 160 ), qm_isi_thresh = self . _params . get ( 'refPerMS' , 2.0 ) / 1000 , kilosort_repository = _get_kilosort_repository ( self . _KS2ver ), chanMap_path = self . _chanmap_filepath . as_posix (), ** params ) self . _modules_input_hash = dict_to_uuid ( self . ks_input_params ) def run_modules ( self ): print ( '---- Running Modules ----' ) self . generate_modules_input_json () module_input_json = self . _module_input_json . as_posix () module_logfile = module_input_json . replace ( '-input.json' , '-run_modules-log.txt' ) for module in self . _modules : module_status = self . _get_module_status ( module ) if module_status [ 'completion_time' ] is not None : continue if module == 'median_subtraction' and self . _median_subtraction_finished : self . _update_module_status ( { module : { 'start_time' : datetime . utcnow (), 'completion_time' : datetime . utcnow (), 'duration' : 0 }}) continue module_output_json = self . _get_module_output_json_filename ( module ) command = [ sys . executable , '-W' , 'ignore' , '-m' , 'ecephys_spike_sorting.modules.' + module , '--input_json' , module_input_json , '--output_json' , module_output_json ] start_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : None , 'duration' : None }}) with open ( module_logfile , \"a\" ) as f : subprocess . check_call ( command , stdout = f ) completion_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : completion_time , 'duration' : ( completion_time - start_time ) . total_seconds ()}}) self . _update_total_duration () def _get_raw_data_filepaths ( self ): raw_ap_fp = self . _npx_input_dir / 'continuous.dat' if 'median_subtraction' not in self . _modules : return raw_ap_fp # median subtraction step will overwrite original continuous.dat file with the corrected version # to preserve the original raw data - make a copy here and work on the copied version assert 'depth_estimation' in self . _modules continuous_file = self . _ks_output_dir / 'continuous.dat' if continuous_file . exists (): if raw_ap_fp . stat () . st_mtime < continuous_file . stat () . st_mtime : # if the copied continuous.dat was actually modified, # median_subtraction may have been completed - let's check module_input_json = self . _module_input_json . as_posix () module_logfile = module_input_json . replace ( '-input.json' , '-run_modules-log.txt' ) with open ( module_logfile , 'r' ) as f : previous_line = '' for line in f . readlines (): if ( line . startswith ( 'ecephys spike sorting: median subtraction module' ) and previous_line . startswith ( 'Total processing time:' )): self . _median_subtraction_finished = True return continuous_file previous_line = line shutil . copy2 ( raw_ap_fp , continuous_file ) return continuous_file def _update_module_status ( self , updated_module_status = {}): if self . _modules_input_hash is None : raise RuntimeError ( '\"generate_modules_input_json()\" not yet performed!' ) self . _modules_input_hash_fp = self . _json_directory / f '. { self . _modules_input_hash } .json' if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) modules_status = { ** modules_status , ** updated_module_status } else : modules_status = { module : { 'start_time' : None , 'completion_time' : None , 'duration' : None } for module in self . _modules } with open ( self . _modules_input_hash_fp , 'w' ) as f : json . dump ( modules_status , f , default = str ) def _get_module_status ( self , module ): if self . _modules_input_hash_fp is None : self . _update_module_status () if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) if modules_status [ module ][ 'completion_time' ] is None : # additional logic to read from the \"-output.json\" file for this module as well # handle cases where the module has finished successfully, # but the \"_modules_input_hash_fp\" is not updated (for whatever reason), # resulting in this module not registered as completed in the \"_modules_input_hash_fp\" module_output_json_fp = pathlib . Path ( self . _get_module_output_json_filename ( module )) if module_output_json_fp . exists (): with open ( module_output_json_fp ) as f : module_run_output = json . load ( f ) modules_status [ module ][ 'duration' ] = module_run_output [ 'execution_time' ] modules_status [ module ][ 'completion_time' ] = ( datetime . strptime ( modules_status [ module ][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) + timedelta ( seconds = module_run_output [ 'execution_time' ])) return modules_status [ module ] return { 'start_time' : None , 'completion_time' : None , 'duration' : None } def _get_module_output_json_filename ( self , module ): module_input_json = self . _module_input_json . as_posix () module_output_json = module_input_json . replace ( '-input.json' , '-' + module + '-' + str ( self . _modules_input_hash ) + '-output.json' ) return module_output_json def _update_total_duration ( self ): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) cumulative_execution_duration = sum ( v [ 'duration' ] or 0 for k , v in modules_status . items () if k not in ( 'cumulative_execution_duration' , 'total_duration' )) total_duration = ( datetime . strptime ( modules_status [ self . _modules [ - 1 ]][ 'completion_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) - datetime . strptime ( modules_status [ self . _modules [ 0 ]][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) ) . total_seconds () self . _update_module_status ( { 'cumulative_execution_duration' : cumulative_execution_duration , 'total_duration' : total_duration })", "title": "OpenEphysKilosortPipeline"}, {"location": "api/element_array_ephys/readers/kilosort_triggering/#element_array_ephys.readers.kilosort_triggering.SGLXKilosortPipeline", "text": "An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Spike GLX acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Source code in element_array_ephys/readers/kilosort_triggering.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 class SGLXKilosortPipeline : \"\"\" An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline for one Neuropixels probe in one recording session using the Spike GLX acquisition software. Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting \"\"\" _modules = [ 'kilosort_helper' , 'kilosort_postprocessing' , 'noise_templates' , 'mean_waveforms' , 'quality_metrics' ] _default_catgt_params = { 'catGT_car_mode' : 'gblcar' , 'catGT_loccar_min_um' : 40 , 'catGT_loccar_max_um' : 160 , 'catGT_cmd_string' : '-prb_fld -out_prb_fld -gfix=0.4,0.10,0.02' , 'ni_present' : False , 'ni_extract_string' : '-XA=0,1,3,500 -iXA=1,3,3,0 -XD=-1,1,50 -XD=-1,2,1.7 -XD=-1,3,5 -iXD=-1,3,5' } _input_json_args = list ( inspect . signature ( createInputJson ) . parameters ) def __init__ ( self , npx_input_dir : str , ks_output_dir : str , params : dict , KS2ver : str , run_CatGT = False , ni_present = False , ni_extract_string = None ): self . _npx_input_dir = pathlib . Path ( npx_input_dir ) self . _ks_output_dir = pathlib . Path ( ks_output_dir ) self . _ks_output_dir . mkdir ( parents = True , exist_ok = True ) self . _params = params self . _KS2ver = KS2ver self . _run_CatGT = run_CatGT self . _run_CatGT = run_CatGT self . _default_catgt_params [ 'ni_present' ] = ni_present self . _default_catgt_params [ 'ni_extract_string' ] = ni_extract_string or self . _default_catgt_params [ 'ni_extract_string' ] self . _json_directory = self . _ks_output_dir / 'json_configs' self . _json_directory . mkdir ( parents = True , exist_ok = True ) self . _CatGT_finished = False self . ks_input_params = None self . _modules_input_hash = None self . _modules_input_hash_fp = None def parse_input_filename ( self ): meta_filename = next ( self . _npx_input_dir . glob ( '*.ap.meta' )) . name match = re . search ( '(.*)_g(\\d)_t(\\d+|cat)\\.imec(\\d?)\\.ap\\.meta' , meta_filename ) session_str , gate_str , trigger_str , probe_str = match . groups () return session_str , gate_str , trigger_str , probe_str or '0' def generate_CatGT_input_json ( self ): if not self . _run_CatGT : print ( 'run_CatGT is set to False, skipping...' ) return session_str , gate_str , trig_str , probe_str = self . parse_input_filename () first_trig , last_trig = SpikeGLX_utils . ParseTrigStr ( 'start,end' , probe_str , gate_str , self . _npx_input_dir . as_posix ()) trigger_str = repr ( first_trig ) + ',' + repr ( last_trig ) self . _catGT_input_json = self . _json_directory / f ' { session_str }{ probe_str } _CatGT-input.json' catgt_params = { k : self . _params . get ( k , v ) for k , v in self . _default_catgt_params . items ()} ni_present = catgt_params . pop ( 'ni_present' ) ni_extract_string = catgt_params . pop ( 'ni_extract_string' ) catgt_params [ 'catGT_stream_string' ] = '-ap -ni' if ni_present else '-ap' sync_extract = '-SY=' + probe_str + ',-1,6,500' extract_string = sync_extract + ( f ' { ni_extract_string } ' if ni_present else '' ) catgt_params [ 'catGT_cmd_string' ] += f ' { extract_string } ' input_meta_fullpath , continuous_file = self . _get_raw_data_filepaths () # create symbolic link to the actual data files - as CatGT expects files to follow a certain naming convention continuous_file_symlink = ( continuous_file . parent / f ' { session_str } _g { gate_str } ' / f ' { session_str } _g { gate_str } _imec { probe_str } ' / f ' { session_str } _g { gate_str } _t { trig_str } .imec { probe_str } .ap.bin' ) continuous_file_symlink . parent . mkdir ( parents = True , exist_ok = True ) if continuous_file_symlink . exists (): continuous_file_symlink . unlink () continuous_file_symlink . symlink_to ( continuous_file ) input_meta_fullpath_symlink = ( input_meta_fullpath . parent / f ' { session_str } _g { gate_str } ' / f ' { session_str } _g { gate_str } _imec { probe_str } ' / f ' { session_str } _g { gate_str } _t { trig_str } .imec { probe_str } .ap.meta' ) input_meta_fullpath_symlink . parent . mkdir ( parents = True , exist_ok = True ) if input_meta_fullpath_symlink . exists (): input_meta_fullpath_symlink . unlink () input_meta_fullpath_symlink . symlink_to ( input_meta_fullpath ) createInputJson ( self . _catGT_input_json . as_posix (), KS2ver = self . _KS2ver , npx_directory = self . _npx_input_dir . as_posix (), spikeGLX_data = True , catGT_run_name = session_str , gate_string = gate_str , trigger_string = trigger_str , probe_string = probe_str , continuous_file = continuous_file . as_posix (), input_meta_path = input_meta_fullpath . as_posix (), extracted_data_directory = self . _ks_output_dir . as_posix (), kilosort_output_directory = self . _ks_output_dir . as_posix (), kilosort_output_tmp = self . _ks_output_dir . as_posix (), kilosort_repository = _get_kilosort_repository ( self . _KS2ver ), ** { k : v for k , v in catgt_params . items () if k in self . _input_json_args } ) def run_CatGT ( self , force_rerun = False ): if self . _run_CatGT and ( not self . _CatGT_finished or force_rerun ): self . generate_CatGT_input_json () print ( '---- Running CatGT ----' ) catGT_input_json = self . _catGT_input_json . as_posix () catGT_output_json = catGT_input_json . replace ( 'CatGT-input.json' , 'CatGT-output.json' ) command = ( sys . executable + \" -W ignore -m ecephys_spike_sorting.modules.\" + 'catGT_helper' + \" --input_json \" + catGT_input_json + \" --output_json \" + catGT_output_json ) subprocess . check_call ( command . split ( ' ' )) self . _CatGT_finished = True def generate_modules_input_json ( self ): session_str , _ , _ , probe_str = self . parse_input_filename () self . _module_input_json = self . _json_directory / f ' { session_str } _imec { probe_str } -input.json' input_meta_fullpath , continuous_file = self . _get_raw_data_filepaths () params = {} for k , v in self . _params . items (): value = str ( v ) if isinstance ( v , list ) else v if f 'ks_ { k } ' in self . _input_json_args : params [ f 'ks_ { k } ' ] = value if k in self . _input_json_args : params [ k ] = value self . ks_input_params = createInputJson ( self . _module_input_json . as_posix (), KS2ver = self . _KS2ver , npx_directory = self . _npx_input_dir . as_posix (), spikeGLX_data = True , continuous_file = continuous_file . as_posix (), input_meta_path = input_meta_fullpath . as_posix (), extracted_data_directory = self . _ks_output_dir . as_posix (), kilosort_output_directory = self . _ks_output_dir . as_posix (), kilosort_output_tmp = self . _ks_output_dir . as_posix (), ks_make_copy = True , noise_template_use_rf = self . _params . get ( 'noise_template_use_rf' , False ), c_Waves_snr_um = self . _params . get ( 'c_Waves_snr_um' , 160 ), qm_isi_thresh = self . _params . get ( 'refPerMS' , 2.0 ) / 1000 , kilosort_repository = _get_kilosort_repository ( self . _KS2ver ), ** params ) self . _modules_input_hash = dict_to_uuid ( self . ks_input_params ) def run_modules ( self ): if self . _run_CatGT and not self . _CatGT_finished : self . run_CatGT () print ( '---- Running Modules ----' ) self . generate_modules_input_json () module_input_json = self . _module_input_json . as_posix () module_logfile = module_input_json . replace ( '-input.json' , '-run_modules-log.txt' ) for module in self . _modules : module_status = self . _get_module_status ( module ) if module_status [ 'completion_time' ] is not None : continue module_output_json = self . _get_module_output_json_filename ( module ) command = ( sys . executable + \" -W ignore -m ecephys_spike_sorting.modules.\" + module + \" --input_json \" + module_input_json + \" --output_json \" + module_output_json ) start_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : None , 'duration' : None }}) with open ( module_logfile , \"a\" ) as f : subprocess . check_call ( command . split ( ' ' ), stdout = f ) completion_time = datetime . utcnow () self . _update_module_status ( { module : { 'start_time' : start_time , 'completion_time' : completion_time , 'duration' : ( completion_time - start_time ) . total_seconds ()}}) self . _update_total_duration () def _get_raw_data_filepaths ( self ): session_str , gate_str , _ , probe_str = self . parse_input_filename () if self . _CatGT_finished : catGT_dest = self . _ks_output_dir run_str = session_str + '_g' + gate_str run_folder = 'catgt_' + run_str prb_folder = run_str + '_imec' + probe_str data_directory = catGT_dest / run_folder / prb_folder else : data_directory = self . _npx_input_dir try : meta_fp = next ( data_directory . glob ( f ' { session_str } *.ap.meta' )) bin_fp = next ( data_directory . glob ( f ' { session_str } *.ap.bin' )) except StopIteration : raise RuntimeError ( f 'No ap meta/bin files found in { data_directory } - CatGT error?' ) return meta_fp , bin_fp def _update_module_status ( self , updated_module_status = {}): if self . _modules_input_hash is None : raise RuntimeError ( '\"generate_modules_input_json()\" not yet performed!' ) self . _modules_input_hash_fp = self . _json_directory / f '. { self . _modules_input_hash } .json' if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) modules_status = { ** modules_status , ** updated_module_status } else : modules_status = { module : { 'start_time' : None , 'completion_time' : None , 'duration' : None } for module in self . _modules } with open ( self . _modules_input_hash_fp , 'w' ) as f : json . dump ( modules_status , f , default = str ) def _get_module_status ( self , module ): if self . _modules_input_hash_fp is None : self . _update_module_status () if self . _modules_input_hash_fp . exists (): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) if modules_status [ module ][ 'completion_time' ] is None : # additional logic to read from the \"-output.json\" file for this module as well # handle cases where the module has finished successfully, # but the \"_modules_input_hash_fp\" is not updated (for whatever reason), # resulting in this module not registered as completed in the \"_modules_input_hash_fp\" module_output_json_fp = pathlib . Path ( self . _get_module_output_json_filename ( module )) if module_output_json_fp . exists (): with open ( module_output_json_fp ) as f : module_run_output = json . load ( f ) modules_status [ module ][ 'duration' ] = module_run_output [ 'execution_time' ] modules_status [ module ][ 'completion_time' ] = ( datetime . strptime ( modules_status [ module ][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) + timedelta ( seconds = module_run_output [ 'execution_time' ])) return modules_status [ module ] return { 'start_time' : None , 'completion_time' : None , 'duration' : None } def _get_module_output_json_filename ( self , module ): module_input_json = self . _module_input_json . as_posix () module_output_json = module_input_json . replace ( '-input.json' , '-' + module + '-' + str ( self . _modules_input_hash ) + '-output.json' ) return module_output_json def _update_total_duration ( self ): with open ( self . _modules_input_hash_fp ) as f : modules_status = json . load ( f ) cumulative_execution_duration = sum ( v [ 'duration' ] or 0 for k , v in modules_status . items () if k not in ( 'cumulative_execution_duration' , 'total_duration' )) total_duration = ( datetime . strptime ( modules_status [ self . _modules [ - 1 ]][ 'completion_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) - datetime . strptime ( modules_status [ self . _modules [ 0 ]][ 'start_time' ], '%Y-%m- %d %H:%M:%S. %f ' ) ) . total_seconds () self . _update_module_status ( { 'cumulative_execution_duration' : cumulative_execution_duration , 'total_duration' : total_duration })", "title": "SGLXKilosortPipeline"}, {"location": "api/element_array_ephys/readers/openephys/", "text": "logger = logging . getLogger ( __name__ ) module-attribute \u00b6 The Open Ephys Record Node saves Neuropixels data in binary format according to the following the directory structure: (https://open-ephys.github.io/gui-docs/User-Manual/Recording-data/Binary-format.html) Record Node 102 -- experiment1 (equivalent to one experimental session - multi probes, multi recordings per probe) -- recording1 -- recording2 -- continuous -- Neuropix-PXI-100.0 (probe0 ap) -- Neuropix-PXI-100.1 (probe0 lf) -- Neuropix-PXI-100.2 (probe1 ap) -- Neuropix-PXI-100.3 (probe1 lf) ... -- events -- spikes -- structure.oebin -- experiment 2 ... -- settings.xml -- settings2.xml ... OpenEphys \u00b6 Source code in element_array_ephys/readers/openephys.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 class OpenEphys : def __init__ ( self , experiment_dir ): self . session_dir = pathlib . Path ( experiment_dir ) if self . session_dir . name . startswith ( 'recording' ): openephys_file = pyopenephys . File ( self . session_dir . parent . parent ) # this is on the Record Node level self . _is_recording_folder = True else : openephys_file = pyopenephys . File ( self . session_dir . parent ) # this is on the Record Node level self . _is_recording_folder = False # extract the \"recordings\" for this session self . experiment = next ( experiment for experiment in openephys_file . experiments if pathlib . Path ( experiment . absolute_foldername ) == ( self . session_dir . parent if self . _is_recording_folder else self . session_dir ) ) # extract probe data self . probes = self . load_probe_data () # self . _recording_time = None @property def recording_time ( self ): if self . _recording_time is None : recording_datetimes = [] for probe in self . probes . values (): recording_datetimes . extend ( probe . recording_info [ 'recording_datetimes' ]) self . _recording_time = sorted ( recording_datetimes )[ 0 ] return self . _recording_time def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"signalchains/processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} sigchain_iter = ( self . experiment . settings [ 'SIGNALCHAIN' ] if isinstance ( self . experiment . settings [ 'SIGNALCHAIN' ], list ) else [ self . experiment . settings [ 'SIGNALCHAIN' ]]) for sigchain in sigchain_iter : processor_iter = ( sigchain [ 'PROCESSOR' ] if isinstance ( sigchain [ 'PROCESSOR' ], list ) else [ sigchain [ 'PROCESSOR' ]]) for processor in processor_iter : if processor [ '@pluginName' ] in ( 'Neuropix-3a' , 'Neuropix-PXI' ): if 'STREAM' in processor : # only on version >= 0.6.0 ap_streams = [ stream for stream in processor [ 'STREAM' ] if not stream [ '@name' ] . endswith ( 'LFP' )] else : ap_streams = None if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): if isinstance ( processor [ 'EDITOR' ][ 'PROBE' ], dict ): probe_indices = ( 0 ,) else : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'PROBE' ])) elif processor [ '@pluginName' ] == 'Neuropix-PXI' : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])) else : raise NotImplementedError else : # not a processor for Neuropixels probe continue for probe_index in probe_indices : probe = Probe ( processor , probe_index ) if ap_streams : probe . probe_info [ 'ap_stream' ] = ap_streams [ probe_index ] probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : if self . _is_recording_folder and rec . absolute_foldername != self . session_dir : continue assert len ( rec . _oebin [ 'continuous' ]) == len ( rec . analog_signals ), \\ f 'Mismatch in the number of continuous data' \\ f ' - expecting { len ( rec . _oebin [ \"continuous\" ]) } (from structure.oebin file),' \\ f ' found { len ( rec . analog_signals ) } (in continuous folder)' for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue # determine if this is continuous data for AP or LFP for the current probe if 'ap_stream' in probe . probe_info : if probe . probe_info [ 'ap_stream' ][ '@name' ] . split ( '-' )[ 0 ] != continuous_info [ 'stream_name' ] . split ( '-' )[ 0 ]: continue # not continuous data for the current probe match = re . search ( '-(AP|LFP)$' , continuous_info [ 'stream_name' ]) if match : continuous_type = match . groups ()[ 0 ] . lower () else : continuous_type = 'ap' elif 'source_processor_sub_idx' in continuous_info : if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' else : continue # not continuous data for the current probe else : raise ValueError ( f 'Unable to infer type (AP or LFP) for the continuous data from: \\n\\t { continuous_info [ \"folder_name\" ] } ' ) if continuous_type == 'ap' : probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime + datetime . timedelta ( seconds = float ( rec . start_time ))) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_type == 'lfp' : probe . recording_info [ 'recording_lfp_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) meta = getattr ( probe , continuous_type + '_meta' ) if not meta : # channel indices - 0-based indexing channels_indices = [ int ( re . search ( r '\\d+$' , chn_name ) . group ()) - 1 for chn_name in analog_signal . channel_names ] meta . update ( ** continuous_info , channels_indices = channels_indices , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes load_probe_data () \u00b6 Loop through all Open Ephys \"signalchains/processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe Source code in element_array_ephys/readers/openephys.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"signalchains/processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} sigchain_iter = ( self . experiment . settings [ 'SIGNALCHAIN' ] if isinstance ( self . experiment . settings [ 'SIGNALCHAIN' ], list ) else [ self . experiment . settings [ 'SIGNALCHAIN' ]]) for sigchain in sigchain_iter : processor_iter = ( sigchain [ 'PROCESSOR' ] if isinstance ( sigchain [ 'PROCESSOR' ], list ) else [ sigchain [ 'PROCESSOR' ]]) for processor in processor_iter : if processor [ '@pluginName' ] in ( 'Neuropix-3a' , 'Neuropix-PXI' ): if 'STREAM' in processor : # only on version >= 0.6.0 ap_streams = [ stream for stream in processor [ 'STREAM' ] if not stream [ '@name' ] . endswith ( 'LFP' )] else : ap_streams = None if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): if isinstance ( processor [ 'EDITOR' ][ 'PROBE' ], dict ): probe_indices = ( 0 ,) else : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'PROBE' ])) elif processor [ '@pluginName' ] == 'Neuropix-PXI' : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])) else : raise NotImplementedError else : # not a processor for Neuropixels probe continue for probe_index in probe_indices : probe = Probe ( processor , probe_index ) if ap_streams : probe . probe_info [ 'ap_stream' ] = ap_streams [ probe_index ] probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : if self . _is_recording_folder and rec . absolute_foldername != self . session_dir : continue assert len ( rec . _oebin [ 'continuous' ]) == len ( rec . analog_signals ), \\ f 'Mismatch in the number of continuous data' \\ f ' - expecting { len ( rec . _oebin [ \"continuous\" ]) } (from structure.oebin file),' \\ f ' found { len ( rec . analog_signals ) } (in continuous folder)' for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue # determine if this is continuous data for AP or LFP for the current probe if 'ap_stream' in probe . probe_info : if probe . probe_info [ 'ap_stream' ][ '@name' ] . split ( '-' )[ 0 ] != continuous_info [ 'stream_name' ] . split ( '-' )[ 0 ]: continue # not continuous data for the current probe match = re . search ( '-(AP|LFP)$' , continuous_info [ 'stream_name' ]) if match : continuous_type = match . groups ()[ 0 ] . lower () else : continuous_type = 'ap' elif 'source_processor_sub_idx' in continuous_info : if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' else : continue # not continuous data for the current probe else : raise ValueError ( f 'Unable to infer type (AP or LFP) for the continuous data from: \\n\\t { continuous_info [ \"folder_name\" ] } ' ) if continuous_type == 'ap' : probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime + datetime . timedelta ( seconds = float ( rec . start_time ))) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_type == 'lfp' : probe . recording_info [ 'recording_lfp_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) meta = getattr ( probe , continuous_type + '_meta' ) if not meta : # channel indices - 0-based indexing channels_indices = [ int ( re . search ( r '\\d+$' , chn_name ) . group ()) - 1 for chn_name in analog_signal . channel_names ] meta . update ( ** continuous_info , channels_indices = channels_indices , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes Probe \u00b6 Source code in element_array_ephys/readers/openephys.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 class Probe : def __init__ ( self , processor , probe_index = 0 ): processor_node_id = processor . get ( \"@nodeId\" , processor . get ( \"@NodeId\" )) if processor_node_id is None : raise KeyError ( 'Neither \"@nodeId\" nor \"@NodeId\" key found' ) self . processor_id = int ( processor_node_id ) if processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]: self . probe_info = processor [ 'EDITOR' ][ 'PROBE' ] if isinstance ( processor [ 'EDITOR' ][ 'PROBE' ], dict ) else processor [ 'EDITOR' ][ 'PROBE' ][ probe_index ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = _probe_model_name_mapping [ processor [ '@pluginName' ]] self . _channels_connected = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info . pop ( 'CHANNELSTATUS' ) . items ()} else : # Neuropix-PXI self . probe_info = processor [ 'EDITOR' ][ 'NP_PROBE' ][ probe_index ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = _probe_model_name_mapping [ self . probe_info [ '@probe_name' ]] if 'ELECTRODE_XPOS' in self . probe_info : self . probe_info [ 'ELECTRODE_XPOS' ] = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info . pop ( 'ELECTRODE_XPOS' ) . items ()} self . probe_info [ 'ELECTRODE_YPOS' ] = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info . pop ( 'ELECTRODE_YPOS' ) . items ()} self . probe_info [ 'ELECTRODE_SHANK' ] = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info [ 'CHANNELS' ] . items ()} self . _channels_connected = { int ( re . search ( r '\\d+$' , k ) . group ()): 1 for k in self . probe_info . pop ( 'CHANNELS' )} self . ap_meta = {} self . lfp_meta = {} self . ap_analog_signals = [] self . lfp_analog_signals = [] self . recording_info = { 'recording_count' : 0 , 'recording_datetimes' : [], 'recording_durations' : [], 'recording_files' : [], 'recording_lfp_files' : []} self . _ap_timeseries = None self . _ap_timestamps = None self . _lfp_timeseries = None self . _lfp_timestamps = None @property def channels_connected ( self ): return { chn_idx : self . _channels_connected . get ( chn_idx , 0 ) for chn_idx in self . ap_meta [ 'channels_indices' ]} @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries @property def ap_timestamps ( self ): if self . _ap_timestamps is None : self . _ap_timestamps = np . hstack ([ s . times for s in self . ap_analog_signals ]) return self . _ap_timestamps @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries @property def lfp_timestamps ( self ): if self . _lfp_timestamps is None : self . _lfp_timestamps = np . hstack ([ s . times for s in self . lfp_analog_signals ]) return self . _lfp_timestamps def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) def compress ( self ): from mtscomp import compress as mts_compress ap_dirs = self . recording_info [ 'recording_files' ] lfp_dirs = self . recording_info [ 'recording_lfp_files' ] meta_mapping = { 'ap' : self . ap_meta , 'lfp' : self . lfp_meta } compressed_files = [] for continuous_dir , continuous_type in zip ( ap_dirs + lfp_dirs , [ 'ap' ] * len ( ap_dirs ) + [ 'lfp' ] * len ( lfp_dirs )): dat_fp = continuous_dir / 'continuous.dat' if not dat_fp . exists (): raise FileNotFoundError ( f 'Compression error - \" { dat_fp } \" does not exist' ) cdat_fp = continuous_dir / 'continuous.cdat' ch_fp = continuous_dir / 'continuous.ch' if cdat_fp . exists (): assert ch_fp . exists () logger . info ( f 'Compressed file exists ( { cdat_fp } ), skipping...' ) continue try : mts_compress ( dat_fp , cdat_fp , ch_fp , sample_rate = meta_mapping [ continuous_type ][ 'sample_rate' ], n_channels = meta_mapping [ continuous_type ][ 'num_channels' ], dtype = np . memmap ( dat_fp ) . dtype ) except Exception as e : cdat_fp . unlink ( missing_ok = True ) ch_fp . unlink ( missing_ok = True ) raise e else : compressed_files . append (( cdat_fp , ch_fp )) return compressed_files def decompress ( self ): from mtscomp import decompress as mts_decompress ap_dirs = self . recording_info [ 'recording_files' ] lfp_dirs = self . recording_info [ 'recording_lfp_files' ] decompressed_files = [] for continuous_dir , continuous_type in zip ( ap_dirs + lfp_dirs , [ 'ap' ] * len ( ap_dirs ) + [ 'lfp' ] * len ( lfp_dirs )): dat_fp = continuous_dir / 'continuous.dat' if dat_fp . exists (): logger . info ( f 'Decompressed file exists ( { dat_fp } ), skipping...' ) continue cdat_fp = continuous_dir / 'continuous.cdat' ch_fp = continuous_dir / 'continuous.ch' if not cdat_fp . exists (): raise FileNotFoundError ( f 'Decompression error - \" { cdat_fp } \" does not exist' ) try : decomp_arr = mts_decompress ( cdat_fp , ch_fp ) decomp_arr . tofile ( dat_fp ) except Exception as e : dat_fp . unlink ( missing_ok = True ) raise e else : decompressed_files . append ( dat_fp ) return decompressed_files ap_timeseries () property \u00b6 AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 251 252 253 254 255 256 257 258 259 260 @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries extract_spike_waveforms ( spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )) \u00b6 :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) Source code in element_array_ephys/readers/openephys.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) lfp_timeseries () property \u00b6 LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 268 269 270 271 272 273 274 275 276 277 @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries", "title": "openephys.py"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.logger", "text": "The Open Ephys Record Node saves Neuropixels data in binary format according to the following the directory structure: (https://open-ephys.github.io/gui-docs/User-Manual/Recording-data/Binary-format.html) Record Node 102 -- experiment1 (equivalent to one experimental session - multi probes, multi recordings per probe) -- recording1 -- recording2 -- continuous -- Neuropix-PXI-100.0 (probe0 ap) -- Neuropix-PXI-100.1 (probe0 lf) -- Neuropix-PXI-100.2 (probe1 ap) -- Neuropix-PXI-100.3 (probe1 lf) ... -- events -- spikes -- structure.oebin -- experiment 2 ... -- settings.xml -- settings2.xml ...", "title": "logger"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys", "text": "Source code in element_array_ephys/readers/openephys.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 class OpenEphys : def __init__ ( self , experiment_dir ): self . session_dir = pathlib . Path ( experiment_dir ) if self . session_dir . name . startswith ( 'recording' ): openephys_file = pyopenephys . File ( self . session_dir . parent . parent ) # this is on the Record Node level self . _is_recording_folder = True else : openephys_file = pyopenephys . File ( self . session_dir . parent ) # this is on the Record Node level self . _is_recording_folder = False # extract the \"recordings\" for this session self . experiment = next ( experiment for experiment in openephys_file . experiments if pathlib . Path ( experiment . absolute_foldername ) == ( self . session_dir . parent if self . _is_recording_folder else self . session_dir ) ) # extract probe data self . probes = self . load_probe_data () # self . _recording_time = None @property def recording_time ( self ): if self . _recording_time is None : recording_datetimes = [] for probe in self . probes . values (): recording_datetimes . extend ( probe . recording_info [ 'recording_datetimes' ]) self . _recording_time = sorted ( recording_datetimes )[ 0 ] return self . _recording_time def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"signalchains/processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} sigchain_iter = ( self . experiment . settings [ 'SIGNALCHAIN' ] if isinstance ( self . experiment . settings [ 'SIGNALCHAIN' ], list ) else [ self . experiment . settings [ 'SIGNALCHAIN' ]]) for sigchain in sigchain_iter : processor_iter = ( sigchain [ 'PROCESSOR' ] if isinstance ( sigchain [ 'PROCESSOR' ], list ) else [ sigchain [ 'PROCESSOR' ]]) for processor in processor_iter : if processor [ '@pluginName' ] in ( 'Neuropix-3a' , 'Neuropix-PXI' ): if 'STREAM' in processor : # only on version >= 0.6.0 ap_streams = [ stream for stream in processor [ 'STREAM' ] if not stream [ '@name' ] . endswith ( 'LFP' )] else : ap_streams = None if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): if isinstance ( processor [ 'EDITOR' ][ 'PROBE' ], dict ): probe_indices = ( 0 ,) else : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'PROBE' ])) elif processor [ '@pluginName' ] == 'Neuropix-PXI' : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])) else : raise NotImplementedError else : # not a processor for Neuropixels probe continue for probe_index in probe_indices : probe = Probe ( processor , probe_index ) if ap_streams : probe . probe_info [ 'ap_stream' ] = ap_streams [ probe_index ] probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : if self . _is_recording_folder and rec . absolute_foldername != self . session_dir : continue assert len ( rec . _oebin [ 'continuous' ]) == len ( rec . analog_signals ), \\ f 'Mismatch in the number of continuous data' \\ f ' - expecting { len ( rec . _oebin [ \"continuous\" ]) } (from structure.oebin file),' \\ f ' found { len ( rec . analog_signals ) } (in continuous folder)' for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue # determine if this is continuous data for AP or LFP for the current probe if 'ap_stream' in probe . probe_info : if probe . probe_info [ 'ap_stream' ][ '@name' ] . split ( '-' )[ 0 ] != continuous_info [ 'stream_name' ] . split ( '-' )[ 0 ]: continue # not continuous data for the current probe match = re . search ( '-(AP|LFP)$' , continuous_info [ 'stream_name' ]) if match : continuous_type = match . groups ()[ 0 ] . lower () else : continuous_type = 'ap' elif 'source_processor_sub_idx' in continuous_info : if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' else : continue # not continuous data for the current probe else : raise ValueError ( f 'Unable to infer type (AP or LFP) for the continuous data from: \\n\\t { continuous_info [ \"folder_name\" ] } ' ) if continuous_type == 'ap' : probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime + datetime . timedelta ( seconds = float ( rec . start_time ))) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_type == 'lfp' : probe . recording_info [ 'recording_lfp_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) meta = getattr ( probe , continuous_type + '_meta' ) if not meta : # channel indices - 0-based indexing channels_indices = [ int ( re . search ( r '\\d+$' , chn_name ) . group ()) - 1 for chn_name in analog_signal . channel_names ] meta . update ( ** continuous_info , channels_indices = channels_indices , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes", "title": "OpenEphys"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys.load_probe_data", "text": "Loop through all Open Ephys \"signalchains/processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe Source code in element_array_ephys/readers/openephys.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def load_probe_data ( self ): \"\"\" Loop through all Open Ephys \"signalchains/processors\", identify the processor for the Neuropixels probe(s), extract probe info Loop through all recordings, associate recordings to the matching probes, extract recording info Yielding multiple \"Probe\" objects, each containing meta information and timeseries data associated with each probe \"\"\" probes = {} sigchain_iter = ( self . experiment . settings [ 'SIGNALCHAIN' ] if isinstance ( self . experiment . settings [ 'SIGNALCHAIN' ], list ) else [ self . experiment . settings [ 'SIGNALCHAIN' ]]) for sigchain in sigchain_iter : processor_iter = ( sigchain [ 'PROCESSOR' ] if isinstance ( sigchain [ 'PROCESSOR' ], list ) else [ sigchain [ 'PROCESSOR' ]]) for processor in processor_iter : if processor [ '@pluginName' ] in ( 'Neuropix-3a' , 'Neuropix-PXI' ): if 'STREAM' in processor : # only on version >= 0.6.0 ap_streams = [ stream for stream in processor [ 'STREAM' ] if not stream [ '@name' ] . endswith ( 'LFP' )] else : ap_streams = None if ( processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]): if isinstance ( processor [ 'EDITOR' ][ 'PROBE' ], dict ): probe_indices = ( 0 ,) else : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'PROBE' ])) elif processor [ '@pluginName' ] == 'Neuropix-PXI' : probe_indices = range ( len ( processor [ 'EDITOR' ][ 'NP_PROBE' ])) else : raise NotImplementedError else : # not a processor for Neuropixels probe continue for probe_index in probe_indices : probe = Probe ( processor , probe_index ) if ap_streams : probe . probe_info [ 'ap_stream' ] = ap_streams [ probe_index ] probes [ probe . probe_SN ] = probe for probe_index , probe_SN in enumerate ( probes ): probe = probes [ probe_SN ] for rec in self . experiment . recordings : if self . _is_recording_folder and rec . absolute_foldername != self . session_dir : continue assert len ( rec . _oebin [ 'continuous' ]) == len ( rec . analog_signals ), \\ f 'Mismatch in the number of continuous data' \\ f ' - expecting { len ( rec . _oebin [ \"continuous\" ]) } (from structure.oebin file),' \\ f ' found { len ( rec . analog_signals ) } (in continuous folder)' for continuous_info , analog_signal in zip ( rec . _oebin [ 'continuous' ], rec . analog_signals ): if continuous_info [ 'source_processor_id' ] != probe . processor_id : continue # determine if this is continuous data for AP or LFP for the current probe if 'ap_stream' in probe . probe_info : if probe . probe_info [ 'ap_stream' ][ '@name' ] . split ( '-' )[ 0 ] != continuous_info [ 'stream_name' ] . split ( '-' )[ 0 ]: continue # not continuous data for the current probe match = re . search ( '-(AP|LFP)$' , continuous_info [ 'stream_name' ]) if match : continuous_type = match . groups ()[ 0 ] . lower () else : continuous_type = 'ap' elif 'source_processor_sub_idx' in continuous_info : if continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 : # ap data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 30000 continuous_type = 'ap' elif continuous_info [ 'source_processor_sub_idx' ] == probe_index * 2 + 1 : # lfp data assert continuous_info [ 'sample_rate' ] == analog_signal . sample_rate == 2500 continuous_type = 'lfp' else : continue # not continuous data for the current probe else : raise ValueError ( f 'Unable to infer type (AP or LFP) for the continuous data from: \\n\\t { continuous_info [ \"folder_name\" ] } ' ) if continuous_type == 'ap' : probe . recording_info [ 'recording_count' ] += 1 probe . recording_info [ 'recording_datetimes' ] . append ( rec . datetime + datetime . timedelta ( seconds = float ( rec . start_time ))) probe . recording_info [ 'recording_durations' ] . append ( float ( rec . duration )) probe . recording_info [ 'recording_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) elif continuous_type == 'lfp' : probe . recording_info [ 'recording_lfp_files' ] . append ( rec . absolute_foldername / 'continuous' / continuous_info [ 'folder_name' ]) meta = getattr ( probe , continuous_type + '_meta' ) if not meta : # channel indices - 0-based indexing channels_indices = [ int ( re . search ( r '\\d+$' , chn_name ) . group ()) - 1 for chn_name in analog_signal . channel_names ] meta . update ( ** continuous_info , channels_indices = channels_indices , channels_ids = analog_signal . channel_ids , channels_names = analog_signal . channel_names , channels_gains = analog_signal . gains ) signal = getattr ( probe , continuous_type + '_analog_signals' ) signal . append ( analog_signal ) return probes", "title": "load_probe_data()"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe", "text": "Source code in element_array_ephys/readers/openephys.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 class Probe : def __init__ ( self , processor , probe_index = 0 ): processor_node_id = processor . get ( \"@nodeId\" , processor . get ( \"@NodeId\" )) if processor_node_id is None : raise KeyError ( 'Neither \"@nodeId\" nor \"@NodeId\" key found' ) self . processor_id = int ( processor_node_id ) if processor [ '@pluginName' ] == 'Neuropix-3a' or 'NP_PROBE' not in processor [ 'EDITOR' ]: self . probe_info = processor [ 'EDITOR' ][ 'PROBE' ] if isinstance ( processor [ 'EDITOR' ][ 'PROBE' ], dict ) else processor [ 'EDITOR' ][ 'PROBE' ][ probe_index ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = _probe_model_name_mapping [ processor [ '@pluginName' ]] self . _channels_connected = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info . pop ( 'CHANNELSTATUS' ) . items ()} else : # Neuropix-PXI self . probe_info = processor [ 'EDITOR' ][ 'NP_PROBE' ][ probe_index ] self . probe_SN = self . probe_info [ '@probe_serial_number' ] self . probe_model = _probe_model_name_mapping [ self . probe_info [ '@probe_name' ]] if 'ELECTRODE_XPOS' in self . probe_info : self . probe_info [ 'ELECTRODE_XPOS' ] = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info . pop ( 'ELECTRODE_XPOS' ) . items ()} self . probe_info [ 'ELECTRODE_YPOS' ] = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info . pop ( 'ELECTRODE_YPOS' ) . items ()} self . probe_info [ 'ELECTRODE_SHANK' ] = { int ( re . search ( r '\\d+$' , k ) . group ()): int ( v ) for k , v in self . probe_info [ 'CHANNELS' ] . items ()} self . _channels_connected = { int ( re . search ( r '\\d+$' , k ) . group ()): 1 for k in self . probe_info . pop ( 'CHANNELS' )} self . ap_meta = {} self . lfp_meta = {} self . ap_analog_signals = [] self . lfp_analog_signals = [] self . recording_info = { 'recording_count' : 0 , 'recording_datetimes' : [], 'recording_durations' : [], 'recording_files' : [], 'recording_lfp_files' : []} self . _ap_timeseries = None self . _ap_timestamps = None self . _lfp_timeseries = None self . _lfp_timestamps = None @property def channels_connected ( self ): return { chn_idx : self . _channels_connected . get ( chn_idx , 0 ) for chn_idx in self . ap_meta [ 'channels_indices' ]} @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries @property def ap_timestamps ( self ): if self . _ap_timestamps is None : self . _ap_timestamps = np . hstack ([ s . times for s in self . ap_analog_signals ]) return self . _ap_timestamps @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries @property def lfp_timestamps ( self ): if self . _lfp_timestamps is None : self . _lfp_timestamps = np . hstack ([ s . times for s in self . lfp_analog_signals ]) return self . _lfp_timestamps def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) def compress ( self ): from mtscomp import compress as mts_compress ap_dirs = self . recording_info [ 'recording_files' ] lfp_dirs = self . recording_info [ 'recording_lfp_files' ] meta_mapping = { 'ap' : self . ap_meta , 'lfp' : self . lfp_meta } compressed_files = [] for continuous_dir , continuous_type in zip ( ap_dirs + lfp_dirs , [ 'ap' ] * len ( ap_dirs ) + [ 'lfp' ] * len ( lfp_dirs )): dat_fp = continuous_dir / 'continuous.dat' if not dat_fp . exists (): raise FileNotFoundError ( f 'Compression error - \" { dat_fp } \" does not exist' ) cdat_fp = continuous_dir / 'continuous.cdat' ch_fp = continuous_dir / 'continuous.ch' if cdat_fp . exists (): assert ch_fp . exists () logger . info ( f 'Compressed file exists ( { cdat_fp } ), skipping...' ) continue try : mts_compress ( dat_fp , cdat_fp , ch_fp , sample_rate = meta_mapping [ continuous_type ][ 'sample_rate' ], n_channels = meta_mapping [ continuous_type ][ 'num_channels' ], dtype = np . memmap ( dat_fp ) . dtype ) except Exception as e : cdat_fp . unlink ( missing_ok = True ) ch_fp . unlink ( missing_ok = True ) raise e else : compressed_files . append (( cdat_fp , ch_fp )) return compressed_files def decompress ( self ): from mtscomp import decompress as mts_decompress ap_dirs = self . recording_info [ 'recording_files' ] lfp_dirs = self . recording_info [ 'recording_lfp_files' ] decompressed_files = [] for continuous_dir , continuous_type in zip ( ap_dirs + lfp_dirs , [ 'ap' ] * len ( ap_dirs ) + [ 'lfp' ] * len ( lfp_dirs )): dat_fp = continuous_dir / 'continuous.dat' if dat_fp . exists (): logger . info ( f 'Decompressed file exists ( { dat_fp } ), skipping...' ) continue cdat_fp = continuous_dir / 'continuous.cdat' ch_fp = continuous_dir / 'continuous.ch' if not cdat_fp . exists (): raise FileNotFoundError ( f 'Decompression error - \" { cdat_fp } \" does not exist' ) try : decomp_arr = mts_decompress ( cdat_fp , ch_fp ) decomp_arr . tofile ( dat_fp ) except Exception as e : dat_fp . unlink ( missing_ok = True ) raise e else : decompressed_files . append ( dat_fp ) return decompressed_files", "title": "Probe"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.ap_timeseries", "text": "AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 251 252 253 254 255 256 257 258 259 260 @property def ap_timeseries ( self ): \"\"\" AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.ap_meta['channels_gains'] \"\"\" if self . _ap_timeseries is None : self . _ap_timeseries = np . hstack ([ s . signal for s in self . ap_analog_signals ]) . T return self . _ap_timeseries", "title": "ap_timeseries()"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.extract_spike_waveforms", "text": ":param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) Source code in element_array_ephys/readers/openephys.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike) \"\"\" channel_bit_volts = np . array ( self . ap_meta [ 'channels_gains' ])[ channel_ind ] # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > ( - wf_win [ 0 ] / self . ap_meta [ 'sample_rate' ]), spikes < ( self . ap_timestamps . max () - wf_win [ - 1 ] / self . ap_meta [ 'sample_rate' ]))] # select a randomized set of \"n_wf\" spikes np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] # extract waveforms if len ( spikes ) > 0 : spike_indices = np . searchsorted ( self . ap_timestamps , spikes , side = \"left\" ) # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ self . ap_timeseries [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spike_indices ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan )", "title": "extract_spike_waveforms()"}, {"location": "api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.lfp_timeseries", "text": "LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] Source code in element_array_ephys/readers/openephys.py 268 269 270 271 272 273 274 275 276 277 @property def lfp_timeseries ( self ): \"\"\" LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts, multiply with self.lfp_meta['channels_gains'] \"\"\" if self . _lfp_timeseries is None : self . _lfp_timeseries = np . hstack ([ s . signal for s in self . lfp_analog_signals ]) . T return self . _lfp_timeseries", "title": "lfp_timeseries()"}, {"location": "api/element_array_ephys/readers/spikeglx/", "text": "SpikeGLX \u00b6 Source code in element_array_ephys/readers/spikeglx.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class SpikeGLX : def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' ) @property def apmeta ( self ): if self . _apmeta is None : self . _apmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.ap.meta' )) return self . _apmeta @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . validate_file ( 'ap' ) self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries @property def lfmeta ( self ): if self . _lfmeta is None : self . _lfmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.lf.meta' )) return self . _lfmeta @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . validate_file ( 'lf' ) self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well def _read_bin ( self , fname ): nchan = self . apmeta . meta [ 'nSavedChans' ] dtype = np . dtype (( np . int16 , nchan )) return np . memmap ( fname , dtype , 'r' ) def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) def validate_file ( self , file_type = 'ap' ): file_path = self . root_dir / ( self . root_name + f '. { file_type } .bin' ) file_size = file_path . stat () . st_size meta_mapping = { 'ap' : self . apmeta , 'lf' : self . lfmeta } meta = meta_mapping [ file_type ] if file_size != meta . meta [ 'fileSizeBytes' ]: raise IOError ( f 'File size error! { file_path } may be corrupted or in transfer?' ) def compress ( self ): from mtscomp import compress as mts_compress ap_file = self . root_dir / ( self . root_name + '.ap.bin' ) lfp_file = self . root_dir / ( self . root_name + '.lf.bin' ) meta_mapping = { 'ap' : self . apmeta , 'lfp' : self . lfmeta } compressed_files = [] for bin_fp , band_type in zip ([ ap_file , lfp_file ], [ 'ap' , 'lfp' ]): if not bin_fp . exists (): raise FileNotFoundError ( f 'Compression error - \" { bin_fp } \" does not exist' ) cbin_fp = bin_fp . parent / f ' { bin_fp . stem } .cbin' ch_fp = bin_fp . parent / f ' { bin_fp . stem } .ch' if cbin_fp . exists (): assert ch_fp . exists () logger . info ( f 'Compressed file exists ( { cbin_fp } ), skipping...' ) continue try : mts_compress ( bin_fp , cbin_fp , ch_fp , sample_rate = meta_mapping [ band_type ][ 'sample_rate' ], n_channels = meta_mapping [ band_type ][ 'num_channels' ], dtype = np . memmap ( bin_fp ) . dtype ) except Exception as e : cbin_fp . unlink ( missing_ok = True ) ch_fp . unlink ( missing_ok = True ) raise e else : compressed_files . append (( cbin_fp , ch_fp )) return compressed_files def decompress ( self ): from mtscomp import decompress as mts_decompress ap_file = self . root_dir / ( self . root_name + '.ap.bin' ) lfp_file = self . root_dir / ( self . root_name + '.lf.bin' ) decompressed_files = [] for bin_fp , band_type in zip ([ ap_file , lfp_file ], [ 'ap' , 'lfp' ]): if bin_fp . exists (): logger . info ( f 'Decompressed file exists ( { bin_fp } ), skipping...' ) continue cbin_fp = bin_fp . parent / f ' { bin_fp . stem } .cbin' ch_fp = bin_fp . parent / f ' { bin_fp . stem } .ch' if not cbin_fp . exists (): raise FileNotFoundError ( f 'Decompression error - \" { cbin_fp } \" does not exist' ) try : decomp_arr = mts_decompress ( cbin_fp , ch_fp ) decomp_arr . tofile ( bin_fp ) except Exception as e : bin_fp . unlink ( missing_ok = True ) raise e else : decompressed_files . append ( bin_fp ) return decompressed_files __init__ ( root_dir ) \u00b6 create neuropixels reader from 'root name' - e.g. the recording: 1 2 3 4 /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: 1 /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. Source code in element_array_ephys/readers/spikeglx.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' ) ap_timeseries () property \u00b6 AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') Source code in element_array_ephys/readers/spikeglx.py 55 56 57 58 59 60 61 62 63 64 65 @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . validate_file ( 'ap' ) self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries extract_spike_waveforms ( spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )) \u00b6 :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) Source code in element_array_ephys/readers/spikeglx.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) get_channel_bit_volts ( band = 'ap' ) \u00b6 Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain Source code in element_array_ephys/readers/spikeglx.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well lf_timeseries () property \u00b6 LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') Source code in element_array_ephys/readers/spikeglx.py 73 74 75 76 77 78 79 80 81 82 83 @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . validate_file ( 'lf' ) self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries SpikeGLXMeta \u00b6 Source code in element_array_ephys/readers/spikeglx.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 class SpikeGLXMeta : def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 1100 : self . probe_model = 'neuropixels UHD' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )] @staticmethod def _parse_chanmap ( raw ): ''' https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#channel-map Parse channel map header structure. Converts: '(x,y,z)(c0,x:y)...(cI,x:y),(sy0;x:y)' e.g: '(384,384,1)(AP0;0:0)...(AP383;383:383)(SY0;768:768)' into dict of form: {'shape': [x,y,z], 'c0': [x,y], ... } ''' res = {} for u in ( i . rstrip ( ')' ) . split ( ';' ) for i in raw . split ( '(' ) if i != '' ): if ( len ( u )) == 1 : res [ 'shape' ] = u [ 0 ] . split ( ',' ) else : res [ u [ 0 ]] = u [ 1 ] . split ( ':' ) return res @staticmethod def _parse_shankmap ( raw ): \"\"\" The shankmap contains details on the shank info for each electrode sites of the sites being recorded only https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#shank-map Parse shank map header structure. Converts: '(x,y,z)(a:b:c:d)...(a:b:c:d)' e.g: '(1,2,480)(0:0:192:1)...(0:1:191:1)' into dict of form: {'shape': [x,y,z], 'data': [[a,b,c,d],...]} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ':' )]) return res @staticmethod def _parse_imrotbl ( raw ): \"\"\" The imro table contains info for all electrode sites (no sync) for a particular electrode configuration (all 384 sites) Note: not all of these 384 sites are necessarily recorded https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#imro-per-channel-settings Parse imro tbl structure. Converts: '(X,Y,Z)(A B C D E)...(A B C D E)' e.g.: '(641251209,3,384)(0 1 0 500 250)...(383 0 0 500 250)' into dict of form: {'shape': (x,y,z), 'data': []} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ' ' )]) return res def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels __init__ ( meta_filepath ) \u00b6 Some good processing references https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m Source code in element_array_ephys/readers/spikeglx.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 1100 : self . probe_model = 'neuropixels UHD' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )] get_original_chans () \u00b6 Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function Source code in element_array_ephys/readers/spikeglx.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels get_recording_channels_indices ( exclude_sync = False ) \u00b6 The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table Source code in element_array_ephys/readers/spikeglx.py 365 366 367 368 369 370 371 372 373 374 375 376 def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind", "title": "spikeglx.py"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX", "text": "Source code in element_array_ephys/readers/spikeglx.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class SpikeGLX : def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' ) @property def apmeta ( self ): if self . _apmeta is None : self . _apmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.ap.meta' )) return self . _apmeta @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . validate_file ( 'ap' ) self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries @property def lfmeta ( self ): if self . _lfmeta is None : self . _lfmeta = SpikeGLXMeta ( self . root_dir / ( self . root_name + '.lf.meta' )) return self . _lfmeta @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . validate_file ( 'lf' ) self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well def _read_bin ( self , fname ): nchan = self . apmeta . meta [ 'nSavedChans' ] dtype = np . dtype (( np . int16 , nchan )) return np . memmap ( fname , dtype , 'r' ) def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan ) def validate_file ( self , file_type = 'ap' ): file_path = self . root_dir / ( self . root_name + f '. { file_type } .bin' ) file_size = file_path . stat () . st_size meta_mapping = { 'ap' : self . apmeta , 'lf' : self . lfmeta } meta = meta_mapping [ file_type ] if file_size != meta . meta [ 'fileSizeBytes' ]: raise IOError ( f 'File size error! { file_path } may be corrupted or in transfer?' ) def compress ( self ): from mtscomp import compress as mts_compress ap_file = self . root_dir / ( self . root_name + '.ap.bin' ) lfp_file = self . root_dir / ( self . root_name + '.lf.bin' ) meta_mapping = { 'ap' : self . apmeta , 'lfp' : self . lfmeta } compressed_files = [] for bin_fp , band_type in zip ([ ap_file , lfp_file ], [ 'ap' , 'lfp' ]): if not bin_fp . exists (): raise FileNotFoundError ( f 'Compression error - \" { bin_fp } \" does not exist' ) cbin_fp = bin_fp . parent / f ' { bin_fp . stem } .cbin' ch_fp = bin_fp . parent / f ' { bin_fp . stem } .ch' if cbin_fp . exists (): assert ch_fp . exists () logger . info ( f 'Compressed file exists ( { cbin_fp } ), skipping...' ) continue try : mts_compress ( bin_fp , cbin_fp , ch_fp , sample_rate = meta_mapping [ band_type ][ 'sample_rate' ], n_channels = meta_mapping [ band_type ][ 'num_channels' ], dtype = np . memmap ( bin_fp ) . dtype ) except Exception as e : cbin_fp . unlink ( missing_ok = True ) ch_fp . unlink ( missing_ok = True ) raise e else : compressed_files . append (( cbin_fp , ch_fp )) return compressed_files def decompress ( self ): from mtscomp import decompress as mts_decompress ap_file = self . root_dir / ( self . root_name + '.ap.bin' ) lfp_file = self . root_dir / ( self . root_name + '.lf.bin' ) decompressed_files = [] for bin_fp , band_type in zip ([ ap_file , lfp_file ], [ 'ap' , 'lfp' ]): if bin_fp . exists (): logger . info ( f 'Decompressed file exists ( { bin_fp } ), skipping...' ) continue cbin_fp = bin_fp . parent / f ' { bin_fp . stem } .cbin' ch_fp = bin_fp . parent / f ' { bin_fp . stem } .ch' if not cbin_fp . exists (): raise FileNotFoundError ( f 'Decompression error - \" { cbin_fp } \" does not exist' ) try : decomp_arr = mts_decompress ( cbin_fp , ch_fp ) decomp_arr . tofile ( bin_fp ) except Exception as e : bin_fp . unlink ( missing_ok = True ) raise e else : decompressed_files . append ( bin_fp ) return decompressed_files", "title": "SpikeGLX"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.__init__", "text": "create neuropixels reader from 'root name' - e.g. the recording: 1 2 3 4 /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: 1 /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. Source code in element_array_ephys/readers/spikeglx.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , root_dir ): ''' create neuropixels reader from 'root name' - e.g. the recording: /data/rec_1/npx_g0_t0.imec.ap.meta /data/rec_1/npx_g0_t0.imec.ap.bin /data/rec_1/npx_g0_t0.imec.lf.meta /data/rec_1/npx_g0_t0.imec.lf.bin would have a 'root name' of: /data/rec_1/npx_g0_t0.imec only a single recording is read/loaded via the root name & associated meta - no interpretation of g0_t0.imec, etc is performed at this layer. ''' self . _apmeta , self . _ap_timeseries = None , None self . _lfmeta , self . _lf_timeseries = None , None self . root_dir = pathlib . Path ( root_dir ) try : meta_filepath = next ( pathlib . Path ( root_dir ) . glob ( '*.ap.meta' )) except StopIteration : raise FileNotFoundError ( f 'No SpikeGLX file (.ap.meta) found at: { root_dir } ' ) self . root_name = meta_filepath . name . replace ( '.ap.meta' , '' )", "title": "__init__()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.ap_timeseries", "text": "AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') Source code in element_array_ephys/readers/spikeglx.py 55 56 57 58 59 60 61 62 63 64 65 @property def ap_timeseries ( self ): \"\"\" AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap') \"\"\" if self . _ap_timeseries is None : self . validate_file ( 'ap' ) self . _ap_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.ap.bin' )) return self . _ap_timeseries", "title": "ap_timeseries()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.extract_spike_waveforms", "text": ":param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) Source code in element_array_ephys/readers/spikeglx.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def extract_spike_waveforms ( self , spikes , channel_ind , n_wf = 500 , wf_win = ( - 32 , 32 )): \"\"\" :param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike) \"\"\" channel_bit_volts = self . get_channel_bit_volts ( 'ap' )[ channel_ind ] data = self . ap_timeseries spikes = np . round ( spikes * self . apmeta . meta [ 'imSampRate' ]) . astype ( int ) # convert to sample # ignore spikes at the beginning or end of raw data spikes = spikes [ np . logical_and ( spikes > - wf_win [ 0 ], spikes < data . shape [ 0 ] - wf_win [ - 1 ])] np . random . shuffle ( spikes ) spikes = spikes [: n_wf ] if len ( spikes ) > 0 : # waveform at each spike: (sample x channel x spike) spike_wfs = np . dstack ([ data [ int ( spk + wf_win [ 0 ]): int ( spk + wf_win [ - 1 ]), channel_ind ] * channel_bit_volts for spk in spikes ]) return spike_wfs else : # if no spike found, return NaN of size (sample x channel x 1) return np . full (( len ( range ( * wf_win )), len ( channel_ind ), 1 ), np . nan )", "title": "extract_spike_waveforms()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.get_channel_bit_volts", "text": "Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain Source code in element_array_ephys/readers/spikeglx.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_channel_bit_volts ( self , band = 'ap' ): \"\"\" Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip dataVolts = dataInt * Vmax / Imax / gain \"\"\" vmax = float ( self . apmeta . meta [ 'imAiRangeMax' ]) if band == 'ap' : imax = IMAX [ self . apmeta . probe_model ] imroTbl_data = self . apmeta . imroTbl [ 'data' ] imroTbl_idx = 3 chn_ind = self . apmeta . get_recording_channels_indices ( exclude_sync = True ) elif band == 'lf' : imax = IMAX [ self . lfmeta . probe_model ] imroTbl_data = self . lfmeta . imroTbl [ 'data' ] imroTbl_idx = 4 chn_ind = self . lfmeta . get_recording_channels_indices ( exclude_sync = True ) else : raise ValueError ( f 'Unsupported band: { band } - Must be \"ap\" or \"lf\"' ) # extract channels' gains if 'imDatPrb_dock' in self . apmeta . meta : # NP 2.0; APGain = 80 for all AP (LF is computed from AP) chn_gains = [ AP_GAIN ] * len ( imroTbl_data ) else : # 3A, 3B1, 3B2 (NP 1.0) chn_gains = [ c [ imroTbl_idx ] for c in imroTbl_data ] chn_gains = np . array ( chn_gains )[ chn_ind ] return vmax / imax / chn_gains * 1e6 # convert to uV as well", "title": "get_channel_bit_volts()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.lf_timeseries", "text": "LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') Source code in element_array_ephys/readers/spikeglx.py 73 74 75 76 77 78 79 80 81 82 83 @property def lf_timeseries ( self ): \"\"\" LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf') \"\"\" if self . _lf_timeseries is None : self . validate_file ( 'lf' ) self . _lf_timeseries = self . _read_bin ( self . root_dir / ( self . root_name + '.lf.bin' )) return self . _lf_timeseries", "title": "lf_timeseries()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta", "text": "Source code in element_array_ephys/readers/spikeglx.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 class SpikeGLXMeta : def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 1100 : self . probe_model = 'neuropixels UHD' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )] @staticmethod def _parse_chanmap ( raw ): ''' https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#channel-map Parse channel map header structure. Converts: '(x,y,z)(c0,x:y)...(cI,x:y),(sy0;x:y)' e.g: '(384,384,1)(AP0;0:0)...(AP383;383:383)(SY0;768:768)' into dict of form: {'shape': [x,y,z], 'c0': [x,y], ... } ''' res = {} for u in ( i . rstrip ( ')' ) . split ( ';' ) for i in raw . split ( '(' ) if i != '' ): if ( len ( u )) == 1 : res [ 'shape' ] = u [ 0 ] . split ( ',' ) else : res [ u [ 0 ]] = u [ 1 ] . split ( ':' ) return res @staticmethod def _parse_shankmap ( raw ): \"\"\" The shankmap contains details on the shank info for each electrode sites of the sites being recorded only https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#shank-map Parse shank map header structure. Converts: '(x,y,z)(a:b:c:d)...(a:b:c:d)' e.g: '(1,2,480)(0:0:192:1)...(0:1:191:1)' into dict of form: {'shape': [x,y,z], 'data': [[a,b,c,d],...]} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ':' )]) return res @staticmethod def _parse_imrotbl ( raw ): \"\"\" The imro table contains info for all electrode sites (no sync) for a particular electrode configuration (all 384 sites) Note: not all of these 384 sites are necessarily recorded https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#imro-per-channel-settings Parse imro tbl structure. Converts: '(X,Y,Z)(A B C D E)...(A B C D E)' e.g.: '(641251209,3,384)(0 1 0 500 250)...(383 0 0 500 250)' into dict of form: {'shape': (x,y,z), 'data': []} \"\"\" res = { 'shape' : None , 'data' : []} for u in ( i . rstrip ( ')' ) for i in raw . split ( '(' ) if i != '' ): if ',' in u : res [ 'shape' ] = [ int ( d ) for d in u . split ( ',' )] else : res [ 'data' ] . append ([ int ( d ) for d in u . split ( ' ' )]) return res def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels", "title": "SpikeGLXMeta"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.__init__", "text": "Some good processing references https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m Source code in element_array_ephys/readers/spikeglx.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def __init__ ( self , meta_filepath ): \"\"\" Some good processing references: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m \"\"\" self . fname = meta_filepath self . meta = _read_meta ( meta_filepath ) # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0) probe_model = self . meta . get ( 'imDatPrb_type' , 1 ) if probe_model <= 1 : if 'typeEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3A' elif 'typeImEnabled' in self . meta : self . probe_model = 'neuropixels 1.0 - 3B' elif probe_model == 1100 : self . probe_model = 'neuropixels UHD' elif probe_model == 21 : self . probe_model = 'neuropixels 2.0 - SS' elif probe_model == 24 : self . probe_model = 'neuropixels 2.0 - MS' else : self . probe_model = str ( probe_model ) # Get recording time self . recording_time = datetime . strptime ( self . meta . get ( 'fileCreateTime_original' , self . meta [ 'fileCreateTime' ]), '%Y-%m- %d T%H:%M:%S' ) self . recording_duration = self . meta . get ( 'fileTimeSecs' ) # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B try : self . probe_SN = self . meta . get ( 'imProbeSN' , self . meta . get ( 'imDatPrb_sn' )) except KeyError : raise KeyError ( 'Probe Serial Number not found in' ' either \"imProbeSN\" or \"imDatPrb_sn\"' ) self . chanmap = ( self . _parse_chanmap ( self . meta [ '~snsChanMap' ]) if '~snsChanMap' in self . meta else None ) self . shankmap = ( self . _parse_shankmap ( self . meta [ '~snsShankMap' ]) if '~snsShankMap' in self . meta else None ) self . imroTbl = ( self . _parse_imrotbl ( self . meta [ '~imroTbl' ]) if '~imroTbl' in self . meta else None ) # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap self . recording_channels = np . arange ( len ( self . imroTbl [ 'data' ]))[ self . get_recording_channels_indices ( exclude_sync = True )]", "title": "__init__()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_original_chans", "text": "Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function Source code in element_array_ephys/readers/spikeglx.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def get_original_chans ( self ): \"\"\" Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index. Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip OriginalChans() function \"\"\" if self . meta [ 'snsSaveChanSubset' ] == 'all' : # output = int32, 0 to nSavedChans - 1 channels = np . arange ( 0 , int ( self . meta [ 'nSavedChans' ])) else : # parse the channel list self.meta['snsSaveChanSubset'] channels = np . arange ( 0 ) # empty array for channel_range in self . meta [ 'snsSaveChanSubset' ] . split ( ',' ): # a block of contiguous channels specified as chan or chan1:chan2 inclusive ix = [ int ( r ) for r in channel_range . split ( ':' )] assert len ( ix ) in ( 1 , 2 ), f \"Invalid channel range spec ' { channel_range } '\" channels = np . append ( channels , np . r_ [ ix [ 0 ]: ix [ - 1 ] + 1 ]) return channels", "title": "get_original_chans()"}, {"location": "api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_recording_channels_indices", "text": "The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table Source code in element_array_ephys/readers/spikeglx.py 365 366 367 368 369 370 371 372 373 374 375 376 def get_recording_channels_indices ( self , exclude_sync = False ): \"\"\" The indices of recorded channels (in chanmap) with respect to the channels listed in the imro table \"\"\" recorded_chns_ind = [ int ( v [ 0 ]) for k , v in self . chanmap . items () if k != 'shape' and ( not k . startswith ( 'SY' ) if exclude_sync else True )] orig_chns_ind = self . get_original_chans () _ , _ , chns_ind = np . intersect1d ( orig_chns_ind , recorded_chns_ind , return_indices = True ) return chns_ind", "title": "get_recording_channels_indices()"}, {"location": "api/element_array_ephys/readers/utils/", "text": "", "title": "utils.py"}, {"location": "api/workflow_array_ephys/analysis/", "text": "SpikesAlignment \u00b6 Bases: dj . Computed Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH Source code in workflow_array_ephys/analysis.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 @schema class SpikesAlignment ( dj . Computed ): \"\"\"Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH\"\"\" definition = \"\"\" -> SpikesAlignmentCondition \"\"\" class AlignedTrialSpikes ( dj . Part ): \"\"\"Spike activity aligned to the event time within the designated window Attributes: SpikesAlignmentCondition.Trial (foreign key) ephys.CuratedClustering.Unit (foreign key) SpikesAlignmentCondition.Trial (foreign key) aligned_spike_times (longblob): (s) spike times relative to alignment event \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit -> SpikesAlignmentCondition.Trial --- aligned_spike_times: longblob # (s) spike times relative to alignment event time \"\"\" class UnitPSTH ( dj . Part ): \"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit Attributes: SpikesAlignment (foreign key) ephys.CuratedClustering.Unit (foreign key) psth (longblob): event-aligned spike peristimulus time histogram (PSTH) psth_edges (longblob) \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit --- psth: longblob # event-aligned spike peristimulus time histogram (PSTH) psth_edges: longblob \"\"\" def make ( self , key : dict ): \"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH Args: key (dict): Dict uniquely identifying one SpikesAlignmentCondition \"\"\" unit_keys , unit_spike_times = ( _linking_module . ephys . CuratedClustering . Unit & key ) . fetch ( \"KEY\" , \"spike_times\" , order_by = \"unit\" ) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trialized_event_times = ( _linking_module . trial . get_trialized_alignment_event_times ( key , _linking_module . trial . Trial & ( SpikesAlignmentCondition . Trial & key ), ) ) min_limit = ( trialized_event_times . event - trialized_event_times . start ) . max () max_limit = ( trialized_event_times . end - trialized_event_times . event ) . max () # Spike raster aligned_trial_spikes = [] units_spike_raster = { u [ \"unit\" ]: { ** key , ** u , \"aligned_spikes\" : []} for u in unit_keys } for _ , r in trialized_event_times . iterrows (): if np . isnan ( r . event ): continue alignment_start_time = r . event - min_limit alignment_end_time = r . event + max_limit for unit_key , spikes in zip ( unit_keys , unit_spike_times ): aligned_spikes = ( spikes [ ( alignment_start_time <= spikes ) & ( spikes < alignment_end_time ) ] - r . event ) aligned_trial_spikes . append ( { ** key , ** unit_key , ** r . trial_key , \"aligned_spike_times\" : aligned_spikes , } ) units_spike_raster [ unit_key [ \"unit\" ]][ \"aligned_spikes\" ] . append ( aligned_spikes ) # PSTH for unit_spike_raster in units_spike_raster . values (): spikes = np . concatenate ( unit_spike_raster [ \"aligned_spikes\" ]) psth , edges = np . histogram ( spikes , bins = np . arange ( - min_limit , max_limit , bin_size ) ) unit_spike_raster [ \"psth\" ] = ( psth / len ( unit_spike_raster . pop ( \"aligned_spikes\" )) / bin_size ) unit_spike_raster [ \"psth_edges\" ] = edges [ 1 :] self . insert1 ( key ) self . AlignedTrialSpikes . insert ( aligned_trial_spikes ) self . UnitPSTH . insert ( list ( units_spike_raster . values ())) def plot ( self , key : dict , unit : int , axs : tuple = None ) -> plt . figure . Figure : \"\"\"Plot event-aligned and trial-averaged spiking Args: key (dict): key of SpikesAlignmentCondition master table unit (int): ID of ephys.CuratedClustering.Unit table axs (tuple, optional): Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) Returns: fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes \"\"\" from .plotting import plot_psth fig = None if axs is None : fig , axs = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trial_ids , aligned_spikes = ( self . AlignedTrialSpikes & key & { \"unit\" : unit } ) . fetch ( \"trial_id\" , \"aligned_spike_times\" ) psth , psth_edges = ( self . UnitPSTH & key & { \"unit\" : unit }) . fetch1 ( \"psth\" , \"psth_edges\" ) xlim = psth_edges [ 0 ], psth_edges [ - 1 ] plot_psth . _plot_spike_raster ( aligned_spikes , trial_ids = trial_ids , ax = axs [ 0 ], title = f \" { dict ( ** key , unit = unit ) } \" , xlim = xlim , ) plot_psth . _plot_psth ( psth , psth_edges , bin_size , ax = axs [ 1 ], title = \"\" , xlim = xlim ) return fig AlignedTrialSpikes \u00b6 Bases: dj . Part Spike activity aligned to the event time within the designated window Attributes: Name Type Description aligned_spike_times longblob (s) spike times relative to alignment event Source code in workflow_array_ephys/analysis.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class AlignedTrialSpikes ( dj . Part ): \"\"\"Spike activity aligned to the event time within the designated window Attributes: SpikesAlignmentCondition.Trial (foreign key) ephys.CuratedClustering.Unit (foreign key) SpikesAlignmentCondition.Trial (foreign key) aligned_spike_times (longblob): (s) spike times relative to alignment event \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit -> SpikesAlignmentCondition.Trial --- aligned_spike_times: longblob # (s) spike times relative to alignment event time \"\"\" UnitPSTH \u00b6 Bases: dj . Part Event-aligned spike peristimulus time histogram (PSTH) by unit Attributes: Name Type Description psth longblob event-aligned spike peristimulus time histogram (PSTH) Source code in workflow_array_ephys/analysis.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class UnitPSTH ( dj . Part ): \"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit Attributes: SpikesAlignment (foreign key) ephys.CuratedClustering.Unit (foreign key) psth (longblob): event-aligned spike peristimulus time histogram (PSTH) psth_edges (longblob) \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit --- psth: longblob # event-aligned spike peristimulus time histogram (PSTH) psth_edges: longblob \"\"\" make ( key ) \u00b6 Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH Parameters: Name Type Description Default key dict Dict uniquely identifying one SpikesAlignmentCondition required Source code in workflow_array_ephys/analysis.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def make ( self , key : dict ): \"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH Args: key (dict): Dict uniquely identifying one SpikesAlignmentCondition \"\"\" unit_keys , unit_spike_times = ( _linking_module . ephys . CuratedClustering . Unit & key ) . fetch ( \"KEY\" , \"spike_times\" , order_by = \"unit\" ) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trialized_event_times = ( _linking_module . trial . get_trialized_alignment_event_times ( key , _linking_module . trial . Trial & ( SpikesAlignmentCondition . Trial & key ), ) ) min_limit = ( trialized_event_times . event - trialized_event_times . start ) . max () max_limit = ( trialized_event_times . end - trialized_event_times . event ) . max () # Spike raster aligned_trial_spikes = [] units_spike_raster = { u [ \"unit\" ]: { ** key , ** u , \"aligned_spikes\" : []} for u in unit_keys } for _ , r in trialized_event_times . iterrows (): if np . isnan ( r . event ): continue alignment_start_time = r . event - min_limit alignment_end_time = r . event + max_limit for unit_key , spikes in zip ( unit_keys , unit_spike_times ): aligned_spikes = ( spikes [ ( alignment_start_time <= spikes ) & ( spikes < alignment_end_time ) ] - r . event ) aligned_trial_spikes . append ( { ** key , ** unit_key , ** r . trial_key , \"aligned_spike_times\" : aligned_spikes , } ) units_spike_raster [ unit_key [ \"unit\" ]][ \"aligned_spikes\" ] . append ( aligned_spikes ) # PSTH for unit_spike_raster in units_spike_raster . values (): spikes = np . concatenate ( unit_spike_raster [ \"aligned_spikes\" ]) psth , edges = np . histogram ( spikes , bins = np . arange ( - min_limit , max_limit , bin_size ) ) unit_spike_raster [ \"psth\" ] = ( psth / len ( unit_spike_raster . pop ( \"aligned_spikes\" )) / bin_size ) unit_spike_raster [ \"psth_edges\" ] = edges [ 1 :] self . insert1 ( key ) self . AlignedTrialSpikes . insert ( aligned_trial_spikes ) self . UnitPSTH . insert ( list ( units_spike_raster . values ())) plot ( key , unit , axs = None ) \u00b6 Plot event-aligned and trial-averaged spiking Parameters: Name Type Description Default key dict key of SpikesAlignmentCondition master table required unit int ID of ephys.CuratedClustering.Unit table required axs tuple Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) None Returns: Name Type Description fig matplotlib . figure . Figure Plot event-aligned and trial-averaged spikes Source code in workflow_array_ephys/analysis.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def plot ( self , key : dict , unit : int , axs : tuple = None ) -> plt . figure . Figure : \"\"\"Plot event-aligned and trial-averaged spiking Args: key (dict): key of SpikesAlignmentCondition master table unit (int): ID of ephys.CuratedClustering.Unit table axs (tuple, optional): Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) Returns: fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes \"\"\" from .plotting import plot_psth fig = None if axs is None : fig , axs = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trial_ids , aligned_spikes = ( self . AlignedTrialSpikes & key & { \"unit\" : unit } ) . fetch ( \"trial_id\" , \"aligned_spike_times\" ) psth , psth_edges = ( self . UnitPSTH & key & { \"unit\" : unit }) . fetch1 ( \"psth\" , \"psth_edges\" ) xlim = psth_edges [ 0 ], psth_edges [ - 1 ] plot_psth . _plot_spike_raster ( aligned_spikes , trial_ids = trial_ids , ax = axs [ 0 ], title = f \" { dict ( ** key , unit = unit ) } \" , xlim = xlim , ) plot_psth . _plot_psth ( psth , psth_edges , bin_size , ax = axs [ 1 ], title = \"\" , xlim = xlim ) return fig SpikesAlignmentCondition \u00b6 Bases: dj . Manual Alignment activity table Attributes: Name Type Description trial_condition varchar(128) # user-friendly name of condition condition_description varchar(1000), nullable condition description bin_size float Bin-size (in second) used to compute the PSTH Default 0.04 Source code in workflow_array_ephys/analysis.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @schema class SpikesAlignmentCondition ( dj . Manual ): \"\"\"Alignment activity table Attributes: ephys.CuratedClustering (foreign key) event.AlignmentEvent (foreign key) trial_condition: varchar(128) # user-friendly name of condition condition_description ( varchar(1000), nullable): condition description bin_size (float, optional): Bin-size (in second) used to compute the PSTH Default 0.04 \"\"\" definition = \"\"\" -> ephys.CuratedClustering -> event.AlignmentEvent trial_condition: varchar(128) # user-friendly name of condition --- condition_description='': varchar(1000) bin_size=0.04: float # bin-size (in second) used to compute the PSTH \"\"\" class Trial ( dj . Part ): \"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\" definition = \"\"\" # Trials on which to compute event-aligned spikes and PSTH -> master -> trial.Trial \"\"\" Trial \u00b6 Bases: dj . Part Trials on which to compute event-aligned spikes and PSTH Source code in workflow_array_ephys/analysis.py 64 65 66 67 68 69 70 class Trial ( dj . Part ): \"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\" definition = \"\"\" # Trials on which to compute event-aligned spikes and PSTH -> master -> trial.Trial \"\"\" activate ( schema_name , * , create_schema = True , create_tables = True , linking_module = None ) \u00b6 Activate this schema. Parameters: Name Type Description Default schema_name str schema name on the database server required create_schema bool when True (default), create schema in the database if it does not yet exist. True create_tables str when True (default), create schema tables in the database if they do not yet exist. True linking_module str a module (or name) containing the required dependencies. None Source code in workflow_array_ephys/analysis.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def activate ( schema_name , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\"Activate this schema. Args: schema_name (str): schema name on the database server create_schema (bool): when True (default), create schema in the database if it does not yet exist. create_tables (str): when True (default), create schema tables in the database if they do not yet exist. linking_module (str): a module (or name) containing the required dependencies. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), ( \"The argument 'dependency' must \" + \"be a module's name or a module\" ) global _linking_module _linking_module = linking_module schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = linking_module . __dict__ , )", "title": "analysis.py"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment", "text": "Bases: dj . Computed Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH Source code in workflow_array_ephys/analysis.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 @schema class SpikesAlignment ( dj . Computed ): \"\"\"Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH\"\"\" definition = \"\"\" -> SpikesAlignmentCondition \"\"\" class AlignedTrialSpikes ( dj . Part ): \"\"\"Spike activity aligned to the event time within the designated window Attributes: SpikesAlignmentCondition.Trial (foreign key) ephys.CuratedClustering.Unit (foreign key) SpikesAlignmentCondition.Trial (foreign key) aligned_spike_times (longblob): (s) spike times relative to alignment event \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit -> SpikesAlignmentCondition.Trial --- aligned_spike_times: longblob # (s) spike times relative to alignment event time \"\"\" class UnitPSTH ( dj . Part ): \"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit Attributes: SpikesAlignment (foreign key) ephys.CuratedClustering.Unit (foreign key) psth (longblob): event-aligned spike peristimulus time histogram (PSTH) psth_edges (longblob) \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit --- psth: longblob # event-aligned spike peristimulus time histogram (PSTH) psth_edges: longblob \"\"\" def make ( self , key : dict ): \"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH Args: key (dict): Dict uniquely identifying one SpikesAlignmentCondition \"\"\" unit_keys , unit_spike_times = ( _linking_module . ephys . CuratedClustering . Unit & key ) . fetch ( \"KEY\" , \"spike_times\" , order_by = \"unit\" ) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trialized_event_times = ( _linking_module . trial . get_trialized_alignment_event_times ( key , _linking_module . trial . Trial & ( SpikesAlignmentCondition . Trial & key ), ) ) min_limit = ( trialized_event_times . event - trialized_event_times . start ) . max () max_limit = ( trialized_event_times . end - trialized_event_times . event ) . max () # Spike raster aligned_trial_spikes = [] units_spike_raster = { u [ \"unit\" ]: { ** key , ** u , \"aligned_spikes\" : []} for u in unit_keys } for _ , r in trialized_event_times . iterrows (): if np . isnan ( r . event ): continue alignment_start_time = r . event - min_limit alignment_end_time = r . event + max_limit for unit_key , spikes in zip ( unit_keys , unit_spike_times ): aligned_spikes = ( spikes [ ( alignment_start_time <= spikes ) & ( spikes < alignment_end_time ) ] - r . event ) aligned_trial_spikes . append ( { ** key , ** unit_key , ** r . trial_key , \"aligned_spike_times\" : aligned_spikes , } ) units_spike_raster [ unit_key [ \"unit\" ]][ \"aligned_spikes\" ] . append ( aligned_spikes ) # PSTH for unit_spike_raster in units_spike_raster . values (): spikes = np . concatenate ( unit_spike_raster [ \"aligned_spikes\" ]) psth , edges = np . histogram ( spikes , bins = np . arange ( - min_limit , max_limit , bin_size ) ) unit_spike_raster [ \"psth\" ] = ( psth / len ( unit_spike_raster . pop ( \"aligned_spikes\" )) / bin_size ) unit_spike_raster [ \"psth_edges\" ] = edges [ 1 :] self . insert1 ( key ) self . AlignedTrialSpikes . insert ( aligned_trial_spikes ) self . UnitPSTH . insert ( list ( units_spike_raster . values ())) def plot ( self , key : dict , unit : int , axs : tuple = None ) -> plt . figure . Figure : \"\"\"Plot event-aligned and trial-averaged spiking Args: key (dict): key of SpikesAlignmentCondition master table unit (int): ID of ephys.CuratedClustering.Unit table axs (tuple, optional): Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) Returns: fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes \"\"\" from .plotting import plot_psth fig = None if axs is None : fig , axs = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trial_ids , aligned_spikes = ( self . AlignedTrialSpikes & key & { \"unit\" : unit } ) . fetch ( \"trial_id\" , \"aligned_spike_times\" ) psth , psth_edges = ( self . UnitPSTH & key & { \"unit\" : unit }) . fetch1 ( \"psth\" , \"psth_edges\" ) xlim = psth_edges [ 0 ], psth_edges [ - 1 ] plot_psth . _plot_spike_raster ( aligned_spikes , trial_ids = trial_ids , ax = axs [ 0 ], title = f \" { dict ( ** key , unit = unit ) } \" , xlim = xlim , ) plot_psth . _plot_psth ( psth , psth_edges , bin_size , ax = axs [ 1 ], title = \"\" , xlim = xlim ) return fig", "title": "SpikesAlignment"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.AlignedTrialSpikes", "text": "Bases: dj . Part Spike activity aligned to the event time within the designated window Attributes: Name Type Description aligned_spike_times longblob (s) spike times relative to alignment event Source code in workflow_array_ephys/analysis.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class AlignedTrialSpikes ( dj . Part ): \"\"\"Spike activity aligned to the event time within the designated window Attributes: SpikesAlignmentCondition.Trial (foreign key) ephys.CuratedClustering.Unit (foreign key) SpikesAlignmentCondition.Trial (foreign key) aligned_spike_times (longblob): (s) spike times relative to alignment event \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit -> SpikesAlignmentCondition.Trial --- aligned_spike_times: longblob # (s) spike times relative to alignment event time \"\"\"", "title": "AlignedTrialSpikes"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.UnitPSTH", "text": "Bases: dj . Part Event-aligned spike peristimulus time histogram (PSTH) by unit Attributes: Name Type Description psth longblob event-aligned spike peristimulus time histogram (PSTH) Source code in workflow_array_ephys/analysis.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class UnitPSTH ( dj . Part ): \"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit Attributes: SpikesAlignment (foreign key) ephys.CuratedClustering.Unit (foreign key) psth (longblob): event-aligned spike peristimulus time histogram (PSTH) psth_edges (longblob) \"\"\" definition = \"\"\" -> master -> ephys.CuratedClustering.Unit --- psth: longblob # event-aligned spike peristimulus time histogram (PSTH) psth_edges: longblob \"\"\"", "title": "UnitPSTH"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.make", "text": "Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH Parameters: Name Type Description Default key dict Dict uniquely identifying one SpikesAlignmentCondition required Source code in workflow_array_ephys/analysis.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def make ( self , key : dict ): \"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH Args: key (dict): Dict uniquely identifying one SpikesAlignmentCondition \"\"\" unit_keys , unit_spike_times = ( _linking_module . ephys . CuratedClustering . Unit & key ) . fetch ( \"KEY\" , \"spike_times\" , order_by = \"unit\" ) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trialized_event_times = ( _linking_module . trial . get_trialized_alignment_event_times ( key , _linking_module . trial . Trial & ( SpikesAlignmentCondition . Trial & key ), ) ) min_limit = ( trialized_event_times . event - trialized_event_times . start ) . max () max_limit = ( trialized_event_times . end - trialized_event_times . event ) . max () # Spike raster aligned_trial_spikes = [] units_spike_raster = { u [ \"unit\" ]: { ** key , ** u , \"aligned_spikes\" : []} for u in unit_keys } for _ , r in trialized_event_times . iterrows (): if np . isnan ( r . event ): continue alignment_start_time = r . event - min_limit alignment_end_time = r . event + max_limit for unit_key , spikes in zip ( unit_keys , unit_spike_times ): aligned_spikes = ( spikes [ ( alignment_start_time <= spikes ) & ( spikes < alignment_end_time ) ] - r . event ) aligned_trial_spikes . append ( { ** key , ** unit_key , ** r . trial_key , \"aligned_spike_times\" : aligned_spikes , } ) units_spike_raster [ unit_key [ \"unit\" ]][ \"aligned_spikes\" ] . append ( aligned_spikes ) # PSTH for unit_spike_raster in units_spike_raster . values (): spikes = np . concatenate ( unit_spike_raster [ \"aligned_spikes\" ]) psth , edges = np . histogram ( spikes , bins = np . arange ( - min_limit , max_limit , bin_size ) ) unit_spike_raster [ \"psth\" ] = ( psth / len ( unit_spike_raster . pop ( \"aligned_spikes\" )) / bin_size ) unit_spike_raster [ \"psth_edges\" ] = edges [ 1 :] self . insert1 ( key ) self . AlignedTrialSpikes . insert ( aligned_trial_spikes ) self . UnitPSTH . insert ( list ( units_spike_raster . values ()))", "title": "make()"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.plot", "text": "Plot event-aligned and trial-averaged spiking Parameters: Name Type Description Default key dict key of SpikesAlignmentCondition master table required unit int ID of ephys.CuratedClustering.Unit table required axs tuple Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) None Returns: Name Type Description fig matplotlib . figure . Figure Plot event-aligned and trial-averaged spikes Source code in workflow_array_ephys/analysis.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def plot ( self , key : dict , unit : int , axs : tuple = None ) -> plt . figure . Figure : \"\"\"Plot event-aligned and trial-averaged spiking Args: key (dict): key of SpikesAlignmentCondition master table unit (int): ID of ephys.CuratedClustering.Unit table axs (tuple, optional): Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8)) Returns: fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes \"\"\" from .plotting import plot_psth fig = None if axs is None : fig , axs = plt . subplots ( 2 , 1 , figsize = ( 12 , 8 )) bin_size = ( SpikesAlignmentCondition & key ) . fetch1 ( \"bin_size\" ) trial_ids , aligned_spikes = ( self . AlignedTrialSpikes & key & { \"unit\" : unit } ) . fetch ( \"trial_id\" , \"aligned_spike_times\" ) psth , psth_edges = ( self . UnitPSTH & key & { \"unit\" : unit }) . fetch1 ( \"psth\" , \"psth_edges\" ) xlim = psth_edges [ 0 ], psth_edges [ - 1 ] plot_psth . _plot_spike_raster ( aligned_spikes , trial_ids = trial_ids , ax = axs [ 0 ], title = f \" { dict ( ** key , unit = unit ) } \" , xlim = xlim , ) plot_psth . _plot_psth ( psth , psth_edges , bin_size , ax = axs [ 1 ], title = \"\" , xlim = xlim ) return fig", "title": "plot()"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignmentCondition", "text": "Bases: dj . Manual Alignment activity table Attributes: Name Type Description trial_condition varchar(128) # user-friendly name of condition condition_description varchar(1000), nullable condition description bin_size float Bin-size (in second) used to compute the PSTH Default 0.04 Source code in workflow_array_ephys/analysis.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @schema class SpikesAlignmentCondition ( dj . Manual ): \"\"\"Alignment activity table Attributes: ephys.CuratedClustering (foreign key) event.AlignmentEvent (foreign key) trial_condition: varchar(128) # user-friendly name of condition condition_description ( varchar(1000), nullable): condition description bin_size (float, optional): Bin-size (in second) used to compute the PSTH Default 0.04 \"\"\" definition = \"\"\" -> ephys.CuratedClustering -> event.AlignmentEvent trial_condition: varchar(128) # user-friendly name of condition --- condition_description='': varchar(1000) bin_size=0.04: float # bin-size (in second) used to compute the PSTH \"\"\" class Trial ( dj . Part ): \"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\" definition = \"\"\" # Trials on which to compute event-aligned spikes and PSTH -> master -> trial.Trial \"\"\"", "title": "SpikesAlignmentCondition"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignmentCondition.Trial", "text": "Bases: dj . Part Trials on which to compute event-aligned spikes and PSTH Source code in workflow_array_ephys/analysis.py 64 65 66 67 68 69 70 class Trial ( dj . Part ): \"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\" definition = \"\"\" # Trials on which to compute event-aligned spikes and PSTH -> master -> trial.Trial \"\"\"", "title": "Trial"}, {"location": "api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.activate", "text": "Activate this schema. Parameters: Name Type Description Default schema_name str schema name on the database server required create_schema bool when True (default), create schema in the database if it does not yet exist. True create_tables str when True (default), create schema tables in the database if they do not yet exist. True linking_module str a module (or name) containing the required dependencies. None Source code in workflow_array_ephys/analysis.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def activate ( schema_name , * , create_schema = True , create_tables = True , linking_module = None ): \"\"\"Activate this schema. Args: schema_name (str): schema name on the database server create_schema (bool): when True (default), create schema in the database if it does not yet exist. create_tables (str): when True (default), create schema tables in the database if they do not yet exist. linking_module (str): a module (or name) containing the required dependencies. \"\"\" if isinstance ( linking_module , str ): linking_module = importlib . import_module ( linking_module ) assert inspect . ismodule ( linking_module ), ( \"The argument 'dependency' must \" + \"be a module's name or a module\" ) global _linking_module _linking_module = linking_module schema . activate ( schema_name , create_schema = create_schema , create_tables = create_tables , add_objects = linking_module . __dict__ , )", "title": "activate()"}, {"location": "api/workflow_array_ephys/export/", "text": "For didactic purposes, import upstream NWB export functions Real use-cases should import these functions directly.", "title": "export.py"}, {"location": "api/workflow_array_ephys/ingest/", "text": "ingest_alignment ( alignment_csv_path = './user_data/alignments.csv' , skip_duplicates = True , verbose = True ) \u00b6 Ingest event alignment data from local CSVs Note: This is duplicated across wf-array-ephys and wf-calcium-imaging Parameters: Name Type Description Default alignment_csv_path str Relative path to event alignment csv. Defaults to \"./user_data/alignments.csv\". './user_data/alignments.csv' skip_duplicates bool See DataJoint insert function. Default True. True verbose bool Print number inserted (i.e., table length change). Defaults to True. True Source code in workflow_array_ephys/ingest.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def ingest_alignment ( alignment_csv_path : str = \"./user_data/alignments.csv\" , skip_duplicates : bool = True , verbose : bool = True , ): \"\"\"Ingest event alignment data from local CSVs Note: This is duplicated across wf-array-ephys and wf-calcium-imaging Args: alignment_csv_path (str, optional): Relative path to event alignment csv. Defaults to \"./user_data/alignments.csv\". skip_duplicates (bool, optional): See DataJoint `insert` function. Default True. verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. \"\"\" csvs = [ alignment_csv_path ] tables = [ event . AlignmentEvent ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose ) ingest_events ( recording_csv_path = './user_data/behavior_recordings.csv' , block_csv_path = './user_data/blocks.csv' , trial_csv_path = './user_data/trials.csv' , event_csv_path = './user_data/events.csv' , skip_duplicates = True , verbose = True ) \u00b6 Ingest each level of experiment hierarchy for element-trial Ingestion hierarchy includes recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial). Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging Parameters: Name Type Description Default recording_csv_path str Relative path to recording csv. Defaults to \"./user_data/behavior_recordings.csv\". './user_data/behavior_recordings.csv' block_csv_path str Relative path to block csv. Defaults to \"./user_data/blocks.csv\". './user_data/blocks.csv' trial_csv_path str Relative path to trial csv. Defaults to \"./user_data/trials.csv\". './user_data/trials.csv' event_csv_path str Relative path to event csv. Defaults to \"./user_data/events.csv\". './user_data/events.csv' skip_duplicates bool See DataJoint insert function. Default True. True verbose bool Print number inserted (i.e., table length change). Defaults to True. True Source code in workflow_array_ephys/ingest.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def ingest_events ( recording_csv_path : str = \"./user_data/behavior_recordings.csv\" , block_csv_path : str = \"./user_data/blocks.csv\" , trial_csv_path : str = \"./user_data/trials.csv\" , event_csv_path : str = \"./user_data/events.csv\" , skip_duplicates : bool = True , verbose : bool = True , ): \"\"\"Ingest each level of experiment hierarchy for element-trial Ingestion hierarchy includes: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial). Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging Args: recording_csv_path (str, optional): Relative path to recording csv. Defaults to \"./user_data/behavior_recordings.csv\". block_csv_path (str, optional): Relative path to block csv. Defaults to \"./user_data/blocks.csv\". trial_csv_path (str, optional): Relative path to trial csv. Defaults to \"./user_data/trials.csv\". event_csv_path (str, optional): Relative path to event csv. Defaults to \"./user_data/events.csv\". skip_duplicates (bool, optional): See DataJoint `insert` function. Default True. verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. \"\"\" csvs = [ recording_csv_path , recording_csv_path , block_csv_path , block_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , event_csv_path , event_csv_path , event_csv_path , ] tables = [ event . BehaviorRecording (), event . BehaviorRecording . File (), trial . Block (), trial . Block . Attribute (), trial . TrialType (), trial . Trial (), trial . Trial . Attribute (), trial . BlockTrial (), event . EventType (), event . Event (), trial . TrialEvent (), ] # Allow direct insert required because element-event has Imported that should be Manual ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose , allow_direct_insert = True , ) ingest_sessions ( session_csv_path = './user_data/sessions.csv' , verbose = True ) \u00b6 Ingest SpikeGLX and OpenEphys files from directories listed in csv Parameters: Name Type Description Default session_csv_path str List of sessions. Defaults to \"./user_data/sessions.csv\". './user_data/sessions.csv' verbose bool Print number inserted (i.e., table length change). Defaults to True. True Raises: Type Description FileNotFoundError Neither SpikeGLX nor OpenEphys recording files found in dir NotImplementedError Acquisition software provided does not match those implemented - SpikeGLX and OpenEphys Source code in workflow_array_ephys/ingest.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def ingest_sessions ( session_csv_path : str = \"./user_data/sessions.csv\" , verbose : bool = True ): \"\"\"Ingest SpikeGLX and OpenEphys files from directories listed in csv Args: session_csv_path (str, optional): List of sessions. Defaults to \"./user_data/sessions.csv\". verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. Raises: FileNotFoundError: Neither SpikeGLX nor OpenEphys recording files found in dir NotImplementedError: Acquisition software provided does not match those implemented - SpikeGLX and OpenEphys \"\"\" # ---------- Insert new \"Session\" and \"ProbeInsertion\" --------- with open ( session_csv_path , newline = \"\" ) as f : input_sessions = list ( csv . DictReader ( f , delimiter = \",\" )) # Folder structure: root / subject / session / probe / .ap.meta session_list , session_dir_list , = ( [], [], ) session_note_list , session_experimenter_list , lab_user_list = [], [], [] probe_list , probe_insertion_list = [], [] for sess in input_sessions : session_dir = find_full_path ( get_ephys_root_data_dir (), sess [ \"session_dir\" ]) session_datetimes , insertions = [], [] # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in zip ( [ \"*.ap.meta\" , \"*.oebin\" ], [ \"SpikeGLX\" , \"OpenEphys\" ] ): ephys_meta_filepaths = [ fp for fp in session_dir . rglob ( ephys_pattern )] if len ( ephys_meta_filepaths ): acq_software = ephys_acq_type break else : raise FileNotFoundError ( \"Ephys recording data not found! Neither SpikeGLX \" + \"nor OpenEphys recording files found in: \" + f \" { session_dir } \" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) insertions . append ( { \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) session_datetimes . append ( spikeglx_meta . recording_time ) elif acq_software == \"OpenEphys\" : loaded_oe = openephys . OpenEphys ( session_dir ) session_datetimes . append ( loaded_oe . experiment . datetime ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) insertions . append ( { \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx } ) else : raise NotImplementedError ( \"Unknown acquisition software: \" + f \" { acq_software } \" ) # new session/probe-insertion session_key = { \"subject\" : sess [ \"subject\" ], \"session_datetime\" : min ( session_datetimes ), } if session_key not in session . Session (): session_list . append ( session_key ) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) session_dir_list . append ( { ** session_key , \"session_dir\" : session_dir . relative_to ( root_dir ) . as_posix (), } ) session_note_list . append ( { ** session_key , \"session_note\" : sess [ \"session_note\" ]} ) session_experimenter_list . append ({ ** session_key , \"user\" : sess [ \"user\" ]}) lab_user_list . append (( sess [ \"user\" ], \"\" , \"\" )) # empty email/phone probe_insertion_list . extend ( [{ ** session_key , ** insertion } for insertion in insertions ] ) session . Session . insert ( session_list ) lab . User . insert ( lab_user_list , skip_duplicates = True ) session . SessionDirectory . insert ( session_dir_list ) session . SessionNote . insert ( session_note_list ) session . SessionExperimenter . insert ( session_experimenter_list ) if verbose : print ( f \" \\n ---- Insert { len ( session_list ) } entry(s) into session.Session ----\" ) probe . Probe . insert ( probe_list ) if verbose : print ( f \" \\n ---- Insert { len ( probe_list ) } entry(s) into probe.Probe ----\" ) ephys . ProbeInsertion . insert ( probe_insertion_list ) if verbose : print ( f \" \\n ---- Insert { len ( probe_insertion_list ) } entry(s) into \" + \"ephys.ProbeInsertion ----\" ) print ( \" \\n ---- Successfully completed ingest_subjects ----\" ) ingest_subjects ( subject_csv_path = './user_data/subjects.csv' , verbose = True ) \u00b6 Ingest subjects listed in the subject column of ./user_data/subjects.csv Parameters: Name Type Description Default subject_csv_path str Relative path to subject csv. Defaults to \"./user_data/subjects.csv\". './user_data/subjects.csv' verbose bool Print number inserted (i.e., table length change). Defaults to True. True Source code in workflow_array_ephys/ingest.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def ingest_subjects ( subject_csv_path : str = \"./user_data/subjects.csv\" , verbose : bool = True ): \"\"\"Ingest subjects listed in the subject column of ./user_data/subjects.csv Args: subject_csv_path (str, optional): Relative path to subject csv. Defaults to \"./user_data/subjects.csv\". verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. \"\"\" # -------------- Insert new \"Subject\" -------------- with open ( subject_csv_path , newline = \"\" ) as f : input_subjects = list ( csv . DictReader ( f , delimiter = \",\" )) if verbose : previous_length = len ( subject . Subject . fetch ()) subject . Subject . insert ( input_subjects , skip_duplicates = True ) if verbose : insert_length = len ( subject . Subject . fetch ()) - previous_length print ( f \" \\n ---- Insert { insert_length } entry(s) into \" + \"subject.Subject ----\" ) print ( \" \\n ---- Successfully completed ingest_subjects ----\" )", "title": "ingest.py"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_alignment", "text": "Ingest event alignment data from local CSVs Note: This is duplicated across wf-array-ephys and wf-calcium-imaging Parameters: Name Type Description Default alignment_csv_path str Relative path to event alignment csv. Defaults to \"./user_data/alignments.csv\". './user_data/alignments.csv' skip_duplicates bool See DataJoint insert function. Default True. True verbose bool Print number inserted (i.e., table length change). Defaults to True. True Source code in workflow_array_ephys/ingest.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def ingest_alignment ( alignment_csv_path : str = \"./user_data/alignments.csv\" , skip_duplicates : bool = True , verbose : bool = True , ): \"\"\"Ingest event alignment data from local CSVs Note: This is duplicated across wf-array-ephys and wf-calcium-imaging Args: alignment_csv_path (str, optional): Relative path to event alignment csv. Defaults to \"./user_data/alignments.csv\". skip_duplicates (bool, optional): See DataJoint `insert` function. Default True. verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. \"\"\" csvs = [ alignment_csv_path ] tables = [ event . AlignmentEvent ()] ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose )", "title": "ingest_alignment()"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_events", "text": "Ingest each level of experiment hierarchy for element-trial Ingestion hierarchy includes recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial). Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging Parameters: Name Type Description Default recording_csv_path str Relative path to recording csv. Defaults to \"./user_data/behavior_recordings.csv\". './user_data/behavior_recordings.csv' block_csv_path str Relative path to block csv. Defaults to \"./user_data/blocks.csv\". './user_data/blocks.csv' trial_csv_path str Relative path to trial csv. Defaults to \"./user_data/trials.csv\". './user_data/trials.csv' event_csv_path str Relative path to event csv. Defaults to \"./user_data/events.csv\". './user_data/events.csv' skip_duplicates bool See DataJoint insert function. Default True. True verbose bool Print number inserted (i.e., table length change). Defaults to True. True Source code in workflow_array_ephys/ingest.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def ingest_events ( recording_csv_path : str = \"./user_data/behavior_recordings.csv\" , block_csv_path : str = \"./user_data/blocks.csv\" , trial_csv_path : str = \"./user_data/trials.csv\" , event_csv_path : str = \"./user_data/events.csv\" , skip_duplicates : bool = True , verbose : bool = True , ): \"\"\"Ingest each level of experiment hierarchy for element-trial Ingestion hierarchy includes: recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial). Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging Args: recording_csv_path (str, optional): Relative path to recording csv. Defaults to \"./user_data/behavior_recordings.csv\". block_csv_path (str, optional): Relative path to block csv. Defaults to \"./user_data/blocks.csv\". trial_csv_path (str, optional): Relative path to trial csv. Defaults to \"./user_data/trials.csv\". event_csv_path (str, optional): Relative path to event csv. Defaults to \"./user_data/events.csv\". skip_duplicates (bool, optional): See DataJoint `insert` function. Default True. verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. \"\"\" csvs = [ recording_csv_path , recording_csv_path , block_csv_path , block_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , trial_csv_path , event_csv_path , event_csv_path , event_csv_path , ] tables = [ event . BehaviorRecording (), event . BehaviorRecording . File (), trial . Block (), trial . Block . Attribute (), trial . TrialType (), trial . Trial (), trial . Trial . Attribute (), trial . BlockTrial (), event . EventType (), event . Event (), trial . TrialEvent (), ] # Allow direct insert required because element-event has Imported that should be Manual ingest_csv_to_table ( csvs , tables , skip_duplicates = skip_duplicates , verbose = verbose , allow_direct_insert = True , )", "title": "ingest_events()"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_sessions", "text": "Ingest SpikeGLX and OpenEphys files from directories listed in csv Parameters: Name Type Description Default session_csv_path str List of sessions. Defaults to \"./user_data/sessions.csv\". './user_data/sessions.csv' verbose bool Print number inserted (i.e., table length change). Defaults to True. True Raises: Type Description FileNotFoundError Neither SpikeGLX nor OpenEphys recording files found in dir NotImplementedError Acquisition software provided does not match those implemented - SpikeGLX and OpenEphys Source code in workflow_array_ephys/ingest.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def ingest_sessions ( session_csv_path : str = \"./user_data/sessions.csv\" , verbose : bool = True ): \"\"\"Ingest SpikeGLX and OpenEphys files from directories listed in csv Args: session_csv_path (str, optional): List of sessions. Defaults to \"./user_data/sessions.csv\". verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. Raises: FileNotFoundError: Neither SpikeGLX nor OpenEphys recording files found in dir NotImplementedError: Acquisition software provided does not match those implemented - SpikeGLX and OpenEphys \"\"\" # ---------- Insert new \"Session\" and \"ProbeInsertion\" --------- with open ( session_csv_path , newline = \"\" ) as f : input_sessions = list ( csv . DictReader ( f , delimiter = \",\" )) # Folder structure: root / subject / session / probe / .ap.meta session_list , session_dir_list , = ( [], [], ) session_note_list , session_experimenter_list , lab_user_list = [], [], [] probe_list , probe_insertion_list = [], [] for sess in input_sessions : session_dir = find_full_path ( get_ephys_root_data_dir (), sess [ \"session_dir\" ]) session_datetimes , insertions = [], [] # search session dir and determine acquisition software for ephys_pattern , ephys_acq_type in zip ( [ \"*.ap.meta\" , \"*.oebin\" ], [ \"SpikeGLX\" , \"OpenEphys\" ] ): ephys_meta_filepaths = [ fp for fp in session_dir . rglob ( ephys_pattern )] if len ( ephys_meta_filepaths ): acq_software = ephys_acq_type break else : raise FileNotFoundError ( \"Ephys recording data not found! Neither SpikeGLX \" + \"nor OpenEphys recording files found in: \" + f \" { session_dir } \" ) if acq_software == \"SpikeGLX\" : for meta_filepath in ephys_meta_filepaths : spikeglx_meta = spikeglx . SpikeGLXMeta ( meta_filepath ) probe_key = { \"probe_type\" : spikeglx_meta . probe_model , \"probe\" : spikeglx_meta . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) probe_dir = meta_filepath . parent probe_number = re . search ( \"(imec)?\\d {1} $\" , probe_dir . name ) . group () probe_number = int ( probe_number . replace ( \"imec\" , \"\" )) insertions . append ( { \"probe\" : spikeglx_meta . probe_SN , \"insertion_number\" : int ( probe_number ), } ) session_datetimes . append ( spikeglx_meta . recording_time ) elif acq_software == \"OpenEphys\" : loaded_oe = openephys . OpenEphys ( session_dir ) session_datetimes . append ( loaded_oe . experiment . datetime ) for probe_idx , oe_probe in enumerate ( loaded_oe . probes . values ()): probe_key = { \"probe_type\" : oe_probe . probe_model , \"probe\" : oe_probe . probe_SN , } if ( probe_key [ \"probe\" ] not in [ p [ \"probe\" ] for p in probe_list ] and probe_key not in probe . Probe () ): probe_list . append ( probe_key ) insertions . append ( { \"probe\" : oe_probe . probe_SN , \"insertion_number\" : probe_idx } ) else : raise NotImplementedError ( \"Unknown acquisition software: \" + f \" { acq_software } \" ) # new session/probe-insertion session_key = { \"subject\" : sess [ \"subject\" ], \"session_datetime\" : min ( session_datetimes ), } if session_key not in session . Session (): session_list . append ( session_key ) root_dir = find_root_directory ( get_ephys_root_data_dir (), session_dir ) session_dir_list . append ( { ** session_key , \"session_dir\" : session_dir . relative_to ( root_dir ) . as_posix (), } ) session_note_list . append ( { ** session_key , \"session_note\" : sess [ \"session_note\" ]} ) session_experimenter_list . append ({ ** session_key , \"user\" : sess [ \"user\" ]}) lab_user_list . append (( sess [ \"user\" ], \"\" , \"\" )) # empty email/phone probe_insertion_list . extend ( [{ ** session_key , ** insertion } for insertion in insertions ] ) session . Session . insert ( session_list ) lab . User . insert ( lab_user_list , skip_duplicates = True ) session . SessionDirectory . insert ( session_dir_list ) session . SessionNote . insert ( session_note_list ) session . SessionExperimenter . insert ( session_experimenter_list ) if verbose : print ( f \" \\n ---- Insert { len ( session_list ) } entry(s) into session.Session ----\" ) probe . Probe . insert ( probe_list ) if verbose : print ( f \" \\n ---- Insert { len ( probe_list ) } entry(s) into probe.Probe ----\" ) ephys . ProbeInsertion . insert ( probe_insertion_list ) if verbose : print ( f \" \\n ---- Insert { len ( probe_insertion_list ) } entry(s) into \" + \"ephys.ProbeInsertion ----\" ) print ( \" \\n ---- Successfully completed ingest_subjects ----\" )", "title": "ingest_sessions()"}, {"location": "api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_subjects", "text": "Ingest subjects listed in the subject column of ./user_data/subjects.csv Parameters: Name Type Description Default subject_csv_path str Relative path to subject csv. Defaults to \"./user_data/subjects.csv\". './user_data/subjects.csv' verbose bool Print number inserted (i.e., table length change). Defaults to True. True Source code in workflow_array_ephys/ingest.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def ingest_subjects ( subject_csv_path : str = \"./user_data/subjects.csv\" , verbose : bool = True ): \"\"\"Ingest subjects listed in the subject column of ./user_data/subjects.csv Args: subject_csv_path (str, optional): Relative path to subject csv. Defaults to \"./user_data/subjects.csv\". verbose (bool, optional): Print number inserted (i.e., table length change). Defaults to True. \"\"\" # -------------- Insert new \"Subject\" -------------- with open ( subject_csv_path , newline = \"\" ) as f : input_subjects = list ( csv . DictReader ( f , delimiter = \",\" )) if verbose : previous_length = len ( subject . Subject . fetch ()) subject . Subject . insert ( input_subjects , skip_duplicates = True ) if verbose : insert_length = len ( subject . Subject . fetch ()) - previous_length print ( f \" \\n ---- Insert { insert_length } entry(s) into \" + \"subject.Subject ----\" ) print ( \" \\n ---- Successfully completed ingest_subjects ----\" )", "title": "ingest_subjects()"}, {"location": "api/workflow_array_ephys/localization/", "text": "Load CCF files. When imported, checks to see that CCF (nrrd and query.csv) files in the ephys_root_data_dir have been loaded into element_electrode_localization.coordinate_framework. Default voxel resolution is 100. To load other resolutions, please modify this script. get_electrode_localization_dir ( probe_insertion_key ) \u00b6 Return root directory of localization data for a given probe Parameters: Name Type Description Default probe_insertion_key dict key uniquely identifying one ephys.EphysRecording required Returns: Name Type Description path str Full path to localization data for either SpikeGLX or OpenEphys Source code in workflow_array_ephys/paths.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_electrode_localization_dir ( probe_insertion_key : dict ) -> str : \"\"\"Return root directory of localization data for a given probe Args: probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording Returns: path (str): Full path to localization data for either SpikeGLX or OpenEphys \"\"\" from .pipeline import ephys acq_software = ( ephys . EphysRecording & probe_insertion_key ) . fetch1 ( \"acq_software\" ) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = pathlib . Path ( ( ephys . EphysRecording . EphysFile & probe_insertion_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) probe_dir = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : probe_path = ( ephys . EphysRecording . EphysFile & probe_insertion_key ) . fetch1 ( \"file_path\" ) probe_dir = find_full_path ( get_ephys_root_data_dir (), probe_path ) return probe_dir get_ephys_root_data_dir () \u00b6 Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: Name Type Description path any List of path(s) if available or None Source code in workflow_array_ephys/paths.py 6 7 8 9 10 11 12 def get_ephys_root_data_dir (): \"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: path (any): List of path(s) if available or None \"\"\" return dj . config . get ( \"custom\" , {}) . get ( \"ephys_root_data_dir\" , None ) get_session_directory ( session_key ) \u00b6 Return relative path from SessionDirectory table given key Parameters: Name Type Description Default session_key dict Key uniquely identifying a session required Returns: Name Type Description path str Relative path of session directory Source code in workflow_array_ephys/paths.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_session_directory ( session_key : dict ) -> str : \"\"\"Return relative path from SessionDirectory table given key Args: session_key (dict): Key uniquely identifying a session Returns: path (str): Relative path of session directory \"\"\" from .pipeline import session session_dir = ( session . SessionDirectory & session_key ) . fetch1 ( \"session_dir\" ) return session_dir", "title": "localization.py"}, {"location": "api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_electrode_localization_dir", "text": "Return root directory of localization data for a given probe Parameters: Name Type Description Default probe_insertion_key dict key uniquely identifying one ephys.EphysRecording required Returns: Name Type Description path str Full path to localization data for either SpikeGLX or OpenEphys Source code in workflow_array_ephys/paths.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_electrode_localization_dir ( probe_insertion_key : dict ) -> str : \"\"\"Return root directory of localization data for a given probe Args: probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording Returns: path (str): Full path to localization data for either SpikeGLX or OpenEphys \"\"\" from .pipeline import ephys acq_software = ( ephys . EphysRecording & probe_insertion_key ) . fetch1 ( \"acq_software\" ) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = pathlib . Path ( ( ephys . EphysRecording . EphysFile & probe_insertion_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) probe_dir = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : probe_path = ( ephys . EphysRecording . EphysFile & probe_insertion_key ) . fetch1 ( \"file_path\" ) probe_dir = find_full_path ( get_ephys_root_data_dir (), probe_path ) return probe_dir", "title": "get_electrode_localization_dir()"}, {"location": "api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_ephys_root_data_dir", "text": "Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: Name Type Description path any List of path(s) if available or None Source code in workflow_array_ephys/paths.py 6 7 8 9 10 11 12 def get_ephys_root_data_dir (): \"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: path (any): List of path(s) if available or None \"\"\" return dj . config . get ( \"custom\" , {}) . get ( \"ephys_root_data_dir\" , None )", "title": "get_ephys_root_data_dir()"}, {"location": "api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_session_directory", "text": "Return relative path from SessionDirectory table given key Parameters: Name Type Description Default session_key dict Key uniquely identifying a session required Returns: Name Type Description path str Relative path of session directory Source code in workflow_array_ephys/paths.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_session_directory ( session_key : dict ) -> str : \"\"\"Return relative path from SessionDirectory table given key Args: session_key (dict): Key uniquely identifying a session Returns: path (str): Relative path of session directory \"\"\" from .pipeline import session session_dir = ( session . SessionDirectory & session_key ) . fetch1 ( \"session_dir\" ) return session_dir", "title": "get_session_directory()"}, {"location": "api/workflow_array_ephys/paths/", "text": "get_electrode_localization_dir ( probe_insertion_key ) \u00b6 Return root directory of localization data for a given probe Parameters: Name Type Description Default probe_insertion_key dict key uniquely identifying one ephys.EphysRecording required Returns: Name Type Description path str Full path to localization data for either SpikeGLX or OpenEphys Source code in workflow_array_ephys/paths.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_electrode_localization_dir ( probe_insertion_key : dict ) -> str : \"\"\"Return root directory of localization data for a given probe Args: probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording Returns: path (str): Full path to localization data for either SpikeGLX or OpenEphys \"\"\" from .pipeline import ephys acq_software = ( ephys . EphysRecording & probe_insertion_key ) . fetch1 ( \"acq_software\" ) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = pathlib . Path ( ( ephys . EphysRecording . EphysFile & probe_insertion_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) probe_dir = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : probe_path = ( ephys . EphysRecording . EphysFile & probe_insertion_key ) . fetch1 ( \"file_path\" ) probe_dir = find_full_path ( get_ephys_root_data_dir (), probe_path ) return probe_dir get_ephys_root_data_dir () \u00b6 Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: Name Type Description path any List of path(s) if available or None Source code in workflow_array_ephys/paths.py 6 7 8 9 10 11 12 def get_ephys_root_data_dir (): \"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: path (any): List of path(s) if available or None \"\"\" return dj . config . get ( \"custom\" , {}) . get ( \"ephys_root_data_dir\" , None ) get_session_directory ( session_key ) \u00b6 Return relative path from SessionDirectory table given key Parameters: Name Type Description Default session_key dict Key uniquely identifying a session required Returns: Name Type Description path str Relative path of session directory Source code in workflow_array_ephys/paths.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_session_directory ( session_key : dict ) -> str : \"\"\"Return relative path from SessionDirectory table given key Args: session_key (dict): Key uniquely identifying a session Returns: path (str): Relative path of session directory \"\"\" from .pipeline import session session_dir = ( session . SessionDirectory & session_key ) . fetch1 ( \"session_dir\" ) return session_dir", "title": "paths.py"}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_electrode_localization_dir", "text": "Return root directory of localization data for a given probe Parameters: Name Type Description Default probe_insertion_key dict key uniquely identifying one ephys.EphysRecording required Returns: Name Type Description path str Full path to localization data for either SpikeGLX or OpenEphys Source code in workflow_array_ephys/paths.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_electrode_localization_dir ( probe_insertion_key : dict ) -> str : \"\"\"Return root directory of localization data for a given probe Args: probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording Returns: path (str): Full path to localization data for either SpikeGLX or OpenEphys \"\"\" from .pipeline import ephys acq_software = ( ephys . EphysRecording & probe_insertion_key ) . fetch1 ( \"acq_software\" ) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = pathlib . Path ( ( ephys . EphysRecording . EphysFile & probe_insertion_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) probe_dir = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : probe_path = ( ephys . EphysRecording . EphysFile & probe_insertion_key ) . fetch1 ( \"file_path\" ) probe_dir = find_full_path ( get_ephys_root_data_dir (), probe_path ) return probe_dir", "title": "get_electrode_localization_dir()"}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_ephys_root_data_dir", "text": "Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: Name Type Description path any List of path(s) if available or None Source code in workflow_array_ephys/paths.py 6 7 8 9 10 11 12 def get_ephys_root_data_dir (): \"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: path (any): List of path(s) if available or None \"\"\" return dj . config . get ( \"custom\" , {}) . get ( \"ephys_root_data_dir\" , None )", "title": "get_ephys_root_data_dir()"}, {"location": "api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_session_directory", "text": "Return relative path from SessionDirectory table given key Parameters: Name Type Description Default session_key dict Key uniquely identifying a session required Returns: Name Type Description path str Relative path of session directory Source code in workflow_array_ephys/paths.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_session_directory ( session_key : dict ) -> str : \"\"\"Return relative path from SessionDirectory table given key Args: session_key (dict): Key uniquely identifying a session Returns: path (str): Relative path of session directory \"\"\" from .pipeline import session session_dir = ( session . SessionDirectory & session_key ) . fetch1 ( \"session_dir\" ) return session_dir", "title": "get_session_directory()"}, {"location": "api/workflow_array_ephys/pipeline/", "text": "get_electrode_localization_dir ( probe_insertion_key ) \u00b6 Return root directory of localization data for a given probe Parameters: Name Type Description Default probe_insertion_key dict key uniquely identifying one ephys.EphysRecording required Returns: Name Type Description path str Full path to localization data for either SpikeGLX or OpenEphys Source code in workflow_array_ephys/paths.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_electrode_localization_dir ( probe_insertion_key : dict ) -> str : \"\"\"Return root directory of localization data for a given probe Args: probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording Returns: path (str): Full path to localization data for either SpikeGLX or OpenEphys \"\"\" from .pipeline import ephys acq_software = ( ephys . EphysRecording & probe_insertion_key ) . fetch1 ( \"acq_software\" ) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = pathlib . Path ( ( ephys . EphysRecording . EphysFile & probe_insertion_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) probe_dir = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : probe_path = ( ephys . EphysRecording . EphysFile & probe_insertion_key ) . fetch1 ( \"file_path\" ) probe_dir = find_full_path ( get_ephys_root_data_dir (), probe_path ) return probe_dir get_ephys_root_data_dir () \u00b6 Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: Name Type Description path any List of path(s) if available or None Source code in workflow_array_ephys/paths.py 6 7 8 9 10 11 12 def get_ephys_root_data_dir (): \"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: path (any): List of path(s) if available or None \"\"\" return dj . config . get ( \"custom\" , {}) . get ( \"ephys_root_data_dir\" , None ) get_session_directory ( session_key ) \u00b6 Return relative path from SessionDirectory table given key Parameters: Name Type Description Default session_key dict Key uniquely identifying a session required Returns: Name Type Description path str Relative path of session directory Source code in workflow_array_ephys/paths.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_session_directory ( session_key : dict ) -> str : \"\"\"Return relative path from SessionDirectory table given key Args: session_key (dict): Key uniquely identifying a session Returns: path (str): Relative path of session directory \"\"\" from .pipeline import session session_dir = ( session . SessionDirectory & session_key ) . fetch1 ( \"session_dir\" ) return session_dir", "title": "pipeline.py"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_electrode_localization_dir", "text": "Return root directory of localization data for a given probe Parameters: Name Type Description Default probe_insertion_key dict key uniquely identifying one ephys.EphysRecording required Returns: Name Type Description path str Full path to localization data for either SpikeGLX or OpenEphys Source code in workflow_array_ephys/paths.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def get_electrode_localization_dir ( probe_insertion_key : dict ) -> str : \"\"\"Return root directory of localization data for a given probe Args: probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording Returns: path (str): Full path to localization data for either SpikeGLX or OpenEphys \"\"\" from .pipeline import ephys acq_software = ( ephys . EphysRecording & probe_insertion_key ) . fetch1 ( \"acq_software\" ) if acq_software == \"SpikeGLX\" : spikeglx_meta_filepath = pathlib . Path ( ( ephys . EphysRecording . EphysFile & probe_insertion_key & 'file_path LIKE \"%.ap.meta\"' ) . fetch1 ( \"file_path\" ) ) probe_dir = find_full_path ( get_ephys_root_data_dir (), spikeglx_meta_filepath . parent ) elif acq_software == \"Open Ephys\" : probe_path = ( ephys . EphysRecording . EphysFile & probe_insertion_key ) . fetch1 ( \"file_path\" ) probe_dir = find_full_path ( get_ephys_root_data_dir (), probe_path ) return probe_dir", "title": "get_electrode_localization_dir()"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_ephys_root_data_dir", "text": "Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: Name Type Description path any List of path(s) if available or None Source code in workflow_array_ephys/paths.py 6 7 8 9 10 11 12 def get_ephys_root_data_dir (): \"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config Returns: path (any): List of path(s) if available or None \"\"\" return dj . config . get ( \"custom\" , {}) . get ( \"ephys_root_data_dir\" , None )", "title": "get_ephys_root_data_dir()"}, {"location": "api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_session_directory", "text": "Return relative path from SessionDirectory table given key Parameters: Name Type Description Default session_key dict Key uniquely identifying a session required Returns: Name Type Description path str Relative path of session directory Source code in workflow_array_ephys/paths.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_session_directory ( session_key : dict ) -> str : \"\"\"Return relative path from SessionDirectory table given key Args: session_key (dict): Key uniquely identifying a session Returns: path (str): Relative path of session directory \"\"\" from .pipeline import session session_dir = ( session . SessionDirectory & session_key ) . fetch1 ( \"session_dir\" ) return session_dir", "title": "get_session_directory()"}, {"location": "api/workflow_array_ephys/process/", "text": "run ( display_progress = True , reserve_jobs = False , suppress_errors = False ) \u00b6 Execute all populate commands in Element Array Ephys Parameters: Name Type Description Default display_progress bool See DataJoint populate . Defaults to True. True reserve_jobs bool See DataJoint populate . Defaults to False. False suppress_errors bool See DataJoint populate . Defaults to False. False Source code in workflow_array_ephys/process.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def run ( display_progress : bool = True , reserve_jobs : bool = False , suppress_errors : bool = False , ): \"\"\"Execute all populate commands in Element Array Ephys Args: display_progress (bool, optional): See DataJoint `populate`. Defaults to True. reserve_jobs (bool, optional): See DataJoint `populate`. Defaults to False. suppress_errors (bool, optional): See DataJoint `populate`. Defaults to False. \"\"\" populate_settings = { \"display_progress\" : display_progress , \"reserve_jobs\" : reserve_jobs , \"suppress_errors\" : suppress_errors , } print ( \" \\n ---- Populate ephys.EphysRecording ----\" ) ephys . EphysRecording . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.LFP ----\" ) ephys . LFP . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.Clustering ----\" ) ephys . Clustering . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.CuratedClustering ----\" ) ephys . CuratedClustering . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.WaveformSet ----\" ) ephys . WaveformSet . populate ( ** populate_settings )", "title": "process.py"}, {"location": "api/workflow_array_ephys/process/#workflow_array_ephys.process.run", "text": "Execute all populate commands in Element Array Ephys Parameters: Name Type Description Default display_progress bool See DataJoint populate . Defaults to True. True reserve_jobs bool See DataJoint populate . Defaults to False. False suppress_errors bool See DataJoint populate . Defaults to False. False Source code in workflow_array_ephys/process.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def run ( display_progress : bool = True , reserve_jobs : bool = False , suppress_errors : bool = False , ): \"\"\"Execute all populate commands in Element Array Ephys Args: display_progress (bool, optional): See DataJoint `populate`. Defaults to True. reserve_jobs (bool, optional): See DataJoint `populate`. Defaults to False. suppress_errors (bool, optional): See DataJoint `populate`. Defaults to False. \"\"\" populate_settings = { \"display_progress\" : display_progress , \"reserve_jobs\" : reserve_jobs , \"suppress_errors\" : suppress_errors , } print ( \" \\n ---- Populate ephys.EphysRecording ----\" ) ephys . EphysRecording . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.LFP ----\" ) ephys . LFP . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.Clustering ----\" ) ephys . Clustering . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.CuratedClustering ----\" ) ephys . CuratedClustering . populate ( ** populate_settings ) print ( \" \\n ---- Populate ephys.WaveformSet ----\" ) ephys . WaveformSet . populate ( ** populate_settings )", "title": "run()"}, {"location": "api/workflow_array_ephys/version/", "text": "Package metadata Update the Docker image tag in docker-compose.yaml to match", "title": "version.py"}, {"location": "api/workflow_array_ephys/plotting/plot_psth/", "text": "", "title": "plot_psth.py"}]}