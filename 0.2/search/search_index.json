{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Element Array Electrophysiology","text":"<p>This Element features DataJoint schemas for analyzing extracellular array electrophysiology data acquired with Neuropixels probes and spike sorted using Kilosort spike sorter. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p> <p></p> <p>The Element is comprised of <code>probe</code> and <code>ephys</code> schemas. Several <code>ephys</code> schemas are developed to handle various use cases of this pipeline and workflow: </p> <ul> <li><code>ephys_acute</code>: A probe is inserted into a new location during each session. </li> </ul> <ul> <li><code>ephys_chronic</code>: A probe is inserted once and used to record across multiple   sessions. </li> </ul> <ul> <li><code>ephys_precluster</code>: A probe is inserted into a new location during each session.   Pre-clustering steps are performed on the data from each probe prior to Kilosort   analysis.</li> </ul> <ul> <li><code>ephys_no_curation</code>: A probe is inserted into a new location during each session and   Kilosort-triggered clustering is performed without the option to manually curate the   results. </li> </ul> <p>Visit the Concepts page for more information about the use cases of <code>ephys</code> schemas and an explanation of the tables. To get started with building your own data pipeline, visit the Tutorials page.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>../../CHANGELOG.md</p>"},{"location":"citation/","title":"Citation","text":"<p>If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID):</p> <ul> <li>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>DataJoint Elements (RRID:SCR_021894) - Element Array Electrophysiology (version 0.2.2)</li> </ul>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#acquisition-tools-for-electrophysiology","title":"Acquisition Tools for Electrophysiology","text":""},{"location":"concepts/#neuropixels-probes","title":"Neuropixels Probes","text":"<p>Neuropixels probes were developed by a collaboration between HHMI Janelia, industry partners, and others1. Since their initial release in October 2018, 300 labs have ordered 1200 probes. Since the rollout of Neuropixels 2.0 in October 2020, IMEC has been shipping 100+ probes monthly (correspondence with Tim Harris).</p> <p>Neuropixels probes offer 960 electrode sites along a 10mm long shank, with 384 recordable channels per probe that can record hundreds of units spanning multiple brain regions (Neuropixels 2.0 version is a 4-shank probe with 1280 electrode sites per shank). Such large recording capacity has offered tremendous opportunities for the field of neurophysiology research, yet this is accompanied by an equally great challenge in terms of data and computation management.</p>"},{"location":"concepts/#data-acquisition-tools","title":"Data Acquisition Tools","text":"<p>Some commonly used acquisiton tools and systems by the neuroscience research community include:</p> <ul> <li>Neuropixels probes</li> <li>Tetrodes</li> <li>SpikeGLX</li> <li>OpenEphys</li> <li>Neuralynx</li> <li>Axona</li> <li>...</li> </ul>"},{"location":"concepts/#data-preprocessing-tools","title":"Data Preprocessing Tools","text":"<p>The preprocessing pipeline includes bandpass filtering for LFP extraction, bandpass filtering for spike sorting, spike sorting, manual curation of the spike sorting results, and calculation of quality control metrics. In trial-based experiments, the spike trains are aligned and separated into trials. Standard processing may include PSTH computation aligned to trial onset or other events, and often grouped by different trial types. Neuroscience groups have traditionally developed custom home-made toolchains.</p> <p>In recent years, several leaders have been emerging as de facto standards with significant community uptake:</p> <ul> <li>Kilosort</li> <li>pyKilosort</li> <li>JRClust</li> <li>KlustaKwik</li> <li>Mountainsort</li> <li>spikeinterface (wrapper)</li> <li>spyking-circus</li> <li>spikeforest</li> <li>...</li> </ul> <p>Kilosort provides most automation and has gained significant popularity, being adopted as one of the key spike sorting methods in the majority of the teams/collaborations we have worked with. As part of our Year-1 NIH U24 effort, we provide support for data ingestion of spike sorting results from Kilosort. Further effort will be devoted for the ingestion support of other spike sorting methods. On this end, a framework for unifying existing spike sorting methods, named SpikeInterface, has been developed by Alessio Buccino, et al. SpikeInterface provides a convenient Python-based wrapper to invoke, extract, compare spike sorting results from different sorting algorithms.</p>"},{"location":"concepts/#key-partnerships","title":"Key Partnerships","text":"<p>Over the past few years, several labs have developed DataJoint-based data management and processing pipelines for Neuropixels probes. Our team collaborated with several of them during their projects. Additionally, we interviewed these teams to understand their experimental workflow, pipeline design, associated tools, and interfaces. These teams include:</p> <ul> <li>International Brain Lab - https://github.com/int-brain-lab/IBL-pipeline</li> </ul> <ul> <li>Mesoscale Activity Project (HHMI Janelia) - https://github.com/mesoscale-activity-map/map-ephys</li> </ul> <ul> <li>Moser Group (Norwegian University of Science and Technology) - see pipeline design</li> </ul> <ul> <li>Andreas Tolias Lab (Baylor College of Medicine)</li> </ul> <ul> <li>BrainCoGs (Princeton Neuroscience Institute)</li> </ul> <ul> <li>Brody Lab (Princeton University)</li> </ul>"},{"location":"concepts/#element-architecture","title":"Element Architecture","text":"<p>Each of the DataJoint Elements creates a set of tables for common neuroscience data modalities to organize, preprocess, and analyze data. Each node in the following diagram is a table within the Element or a table connected to the Element.</p>"},{"location":"concepts/#ephys_acute-module","title":"<code>ephys_acute</code> module","text":""},{"location":"concepts/#ephys_chronic-module","title":"<code>ephys_chronic</code> module","text":""},{"location":"concepts/#ephys_precluster-module","title":"<code>ephys_precluster</code> module","text":""},{"location":"concepts/#subject-schema-api-docs","title":"<code>subject</code> schema (API docs)","text":"<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</li> </ul> Table Description Subject A table containing basic information of the research subject."},{"location":"concepts/#session-schema-api-docs","title":"<code>session</code> schema (API docs)","text":"Table Description Session A table for unique experimental session identifiers."},{"location":"concepts/#probe-schema-api-docs","title":"<code>probe</code> schema (API docs)","text":"<p>Tables related to the Neuropixels probe and electrode configuration.</p> Table Description ProbeType A lookup table specifying the type of Neuropixels probe (e.g. \"neuropixels 1.0\", \"neuropixels 2.0 single-shank\"). ProbeType.Electrode A table containing electrodes and their properties for a particular probe type. Probe A record of an actual physical probe. ElectrodeConfig A record of a particular electrode configuration to be used for ephys recording. ElectrodeConfig.Electrode A record of electrodes out of those in <code>ProbeType.Electrode</code> that are used for recording."},{"location":"concepts/#ephys-schema-api-docs","title":"<code>ephys</code> schema (API docs)","text":"<p>Tables related to information about physiological recordings and automatic ingestion of spike sorting results.</p> Table Description ProbeInsertion A record of surgical insertions of a probe in the brain. EphysRecording A table with metadata about each electrophysiogical recording. Clustering A table with clustering data for spike sorting extracellular electrophysiology data. Curation A table to declare optional manual curation of spike sorting results. CuratedClustering A table with metadata for sorted data generated after each curation. CuratedClusting.Unit A part table containing single unit information after spike sorting and optional curation. WaveformSet A table containing spike waveforms for single units."},{"location":"concepts/#ephys_report-schema-api-docs","title":"<code>ephys_report</code> schema (API docs)","text":"<p>Tables for storing probe or unit-level visualization results.</p> Table Description ProbeLevelReport A table to store drift map figures generated from each recording probe. UnitLevelReport A table to store figures (waveforms, autocorrelogram, peak waveform + neighbors) generated for each unit."},{"location":"concepts/#element-development","title":"Element Development","text":"<p>Through our interviews and direct collaboration on the precursor projects, we identified the common motifs to create the Array Electrophysiology Element.</p> <p>Major features of the Array Electrophysiology Element include:</p> <ul> <li>Pipeline architecture detailed by:<p>+ Probe, electrode configuration compatible with Neuropixels probes and generalizable to other types of probes (e.g. tetrodes) - supporting both <code>chronic</code> and <code>acute</code> probe insertion modes.</p> <p>+ Probe-insertion, ephys-recordings, LFP extraction, clusterings, curations, sorted units and the associated data (e.g. spikes, waveforms, etc.).</p> <p>+ Store/track/manage different curations of the spike sorting results - supporting both curated clustering and kilosort triggered clustering (i.e., <code>no_curation</code>).</p> </li> </ul> <ul> <li>Ingestion support for data acquired with SpikeGLX and OpenEphys acquisition systems. </li> <li>Ingestion support for spike sorting outputs from Kilosort.</li> <li>Triggering support for workflow integrated Kilosort processing.</li> <li>Sample data and complete test suite for quality assurance.</li> </ul>"},{"location":"concepts/#data-export-and-publishing","title":"Data Export and Publishing","text":"<p>Element Array Electrophysiology supports exporting of all data into standard Neurodata Without Borders (NWB) files. This makes it easy to share files with collaborators and publish results on DANDI Archive. NWB, as an organization, is dedicated to standardizing data formats and maximizing interoperability across tools for neurophysiology. For more information on uploading NWB files to DANDI within the DataJoint Elements ecosystem, visit our documentation for the DANDI upload feature of Element Array Electrophysiology.</p>"},{"location":"concepts/#roadmap","title":"Roadmap","text":"<p>Incorporation of SpikeInterface into the Array Electrophysiology Element will be on DataJoint Elements development roadmap. Dr. Loren Frank has led a development effort of a DataJoint pipeline with SpikeInterface framework and NeurodataWithoutBorders format integrated https://github.com/LorenFrankLab/nwb_datajoint.</p> <p>Future additions to this element will add functionality to support large (&gt; 48 hours) neuropixel recordings via an overlapping segmented processing approach. </p> <p>Further development of this Element is community driven. Upon user requests we will continue adding features to this Element.</p>"},{"location":"concepts/#references","title":"References","text":"<p>[1]: Jun, J., Steinmetz, N., Siegle, J. et al. Fully integrated silicon probes for high-density recording of neural activity. Nature 551, 232\u2013236 (2017). https://doi.org/10.1038/nature24636.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Coming soon!</p>"},{"location":"api/navigation/","title":"Navigation","text":"<ul> <li>element_array_ephys     * ephys_acute.py     * ephys_chronic.py     * ephys_no_curation.py     * ephys_precluster.py     * ephys_report.py     * export         * nwb             * nwb.py     * plotting         * corr.py         * probe_level.py         * qc.py         * unit_level.py         * widget.py     * probe.py     * readers         * kilosort.py         * kilosort_triggering.py         * openephys.py         * spikeglx.py         * utils.py     * version.py</li> <li>workflow_array_ephys     * analysis.py     * export.py     * ingest.py     * localization.py     * paths.py     * pipeline.py     * plotting         * plot_psth.py     * process.py     * version.py</li> </ul>"},{"location":"api/element_array_ephys/ephys_acute/","title":"ephys_acute.py","text":""},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.AcquisitionSoftware","title":"<code>AcquisitionSoftware</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Software used for recording of neuropixels probes\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusterQualityLabel","title":"<code>ClusterQualityLabel</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code> varchar(4000) </code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description ( varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)  # cluster quality type - e.g. 'good', 'MUA', 'noise', etc.\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering","title":"<code>Clustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code> varchar(16) </code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version ( varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ClusteringTask.infer_output_dir(\n                key, relative=True, mkdir=True)\n            # update clustering_output_dir\n            ClusteringTask.update1(\n                {**key, \"clustering_output_dir\": output_dir.as_posix()}\n            )\n\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n        elif task_mode == \"trigger\":\n            acq_software, clustering_method, params = (\n                ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n            ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n            if \"kilosort\" in clustering_method:\n                from element_array_ephys.readers import kilosort_triggering\n\n                # add additional probe-recording and channels details into `params`\n                params = {**params, **get_recording_channels_details(key)}\n                params[\"fs\"] = params[\"sample_rate\"]\n\n                if acq_software == \"SpikeGLX\":\n                    spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                    spikeglx_recording = spikeglx.SpikeGLX(\n                        spikeglx_meta_filepath.parent\n                    )\n                    spikeglx_recording.validate_file(\"ap\")\n                    run_CatGT = (\n                        params.pop(\"run_CatGT\", True)\n                        and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                    )\n\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=spikeglx_recording.root_dir\n                            / (spikeglx_recording.root_name + \".ap.bin\"),\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                            npx_input_dir=spikeglx_meta_filepath.parent,\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                            run_CatGT=run_CatGT,\n                        )\n                        run_kilosort.run_modules()\n                elif acq_software == \"Open Ephys\":\n                    oe_probe = get_openephys_probe_data(key)\n\n                    assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                    # run kilosort\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=pathlib.Path(\n                                oe_probe.recording_info[\"recording_files\"][0]\n                            )\n                            / \"continuous.dat\",\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                            npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        )\n                        run_kilosort.run_modules()\n            else:\n                raise NotImplementedError(\n                    f\"Automatic triggering of {clustering_method}\"\n                    f\" clustering analysis is not yet supported\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Clustering.make","title":"<code>make(key)</code>","text":"<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ClusteringTask.infer_output_dir(\n            key, relative=True, mkdir=True)\n        # update clustering_output_dir\n        ClusteringTask.update1(\n            {**key, \"clustering_output_dir\": output_dir.as_posix()}\n        )\n\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n    elif task_mode == \"trigger\":\n        acq_software, clustering_method, params = (\n            ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n        ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n        if \"kilosort\" in clustering_method:\n            from element_array_ephys.readers import kilosort_triggering\n\n            # add additional probe-recording and channels details into `params`\n            params = {**params, **get_recording_channels_details(key)}\n            params[\"fs\"] = params[\"sample_rate\"]\n\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                spikeglx_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent\n                )\n                spikeglx_recording.validate_file(\"ap\")\n                run_CatGT = (\n                    params.pop(\"run_CatGT\", True)\n                    and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                )\n\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=spikeglx_recording.root_dir\n                        / (spikeglx_recording.root_name + \".ap.bin\"),\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                        npx_input_dir=spikeglx_meta_filepath.parent,\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        run_CatGT=run_CatGT,\n                    )\n                    run_kilosort.run_modules()\n            elif acq_software == \"Open Ephys\":\n                oe_probe = get_openephys_probe_data(key)\n\n                assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                # run kilosort\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=pathlib.Path(\n                            oe_probe.recording_info[\"recording_files\"][0]\n                        )\n                        / \"continuous.dat\",\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                        npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                    )\n                    run_kilosort.run_modules()\n        else:\n            raise NotImplementedError(\n                f\"Automatic triggering of {clustering_method}\"\n                f\" clustering analysis is not yet supported\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringMethod","title":"<code>ClusteringMethod</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n        (\"kilosort2.5\", \"kilosort2.5 clustering method\"),\n        (\"kilosort3\", \"kilosort3 clustering method\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringParamSet","title":"<code>ClusteringParamSet</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameters for clustering with Kilosort.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob): Parameters for clustering with Kilosort.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        clustering_method: str,\n        paramset_desc: str,\n        params: dict,\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"clustering_method\": clustering_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(\n                {**params, \"clustering_method\": clustering_method}\n            ),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringParamSet.insert_new_params","title":"<code>insert_new_params(clustering_method, paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>","text":"<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    clustering_method: str,\n    paramset_desc: str,\n    params: dict,\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"clustering_method\": clustering_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(\n            {**params, \"clustering_method\": clustering_method}\n        ),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask","title":"<code>ClusteringTask</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_output_dir</code> <code> varchar (255) </code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_output_dir ( varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir='': varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key: dict, relative: bool = False, mkdir: bool = False):\n\"\"\"Infer output directory if it is not provided.\n\n        Args:\n            key (dict): ClusteringTask primary key.\n\n        Returns:\n            Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n            e.g.: sub4/sess1/probe_2/kilosort2_0\n        \"\"\"\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n        root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n        method = (\n            (ClusteringParamSet * ClusteringMethod &amp; key)\n            .fetch1(\"clustering_method\")\n            .replace(\".\", \"-\")\n        )\n\n        output_dir = (\n            processed_dir\n            / session_dir.relative_to(root_dir)\n            / f'probe_{key[\"insertion_number\"]}'\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            log.info(f\"{output_dir} created!\")\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n        Args:\n            ephys_recording_key (dict): EphysRecording primary key.\n            paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n        \"\"\"\n        key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = ClusteringTask.infer_output_dir(\n            key, relative=False, mkdir=True)\n\n        try:\n            kilosort.Kilosort(\n                output_dir\n            )  # check if the directory is a valid Kilosort output\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"clustering_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask.auto_generate_entries","title":"<code>auto_generate_entries(ephys_recording_key, paramset_idx=0)</code>  <code>classmethod</code>","text":"<p>Autogenerate entries based on a particular ephys recording.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_recording_key</code> <code>dict</code> <p>EphysRecording primary key.</p> required <code>paramset_idx</code> <code>int</code> <p>Parameter index to use for clustering task. Defaults to 0.</p> <code>0</code> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n    Args:\n        ephys_recording_key (dict): EphysRecording primary key.\n        paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n    \"\"\"\n    key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = ClusteringTask.infer_output_dir(\n        key, relative=False, mkdir=True)\n\n    try:\n        kilosort.Kilosort(\n            output_dir\n        )  # check if the directory is a valid Kilosort output\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"clustering_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ClusteringTask.infer_output_dir","title":"<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>","text":"<p>Infer output directory if it is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>ClusteringTask primary key.</p> required <p>Returns:</p> Type Description <p>Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}</p> <p>e.g.: sub4/sess1/probe_2/kilosort2_0</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key: dict, relative: bool = False, mkdir: bool = False):\n\"\"\"Infer output directory if it is not provided.\n\n    Args:\n        key (dict): ClusteringTask primary key.\n\n    Returns:\n        Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n        e.g.: sub4/sess1/probe_2/kilosort2_0\n    \"\"\"\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n    root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n    method = (\n        (ClusteringParamSet * ClusteringMethod &amp; key)\n        .fetch1(\"clustering_method\")\n        .replace(\".\", \"-\")\n    )\n\n    output_dir = (\n        processed_dir\n        / session_dir.relative_to(root_dir)\n        / f'probe_{key[\"insertion_number\"]}'\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        log.info(f\"{output_dir} created!\")\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering","title":"<code>CuratedClustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Curation primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Curation (foreign key): Curation primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of a curation.\n    -&gt; Curation\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n            \"acq_software\", \"sampling_rate\"\n        )\n\n        sample_rate = kilosort_dataset.data[\"params\"].get(\n            \"sample_rate\", sample_rate)\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / sample_rate\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ]\n                        if spike_depths is not None\n                        else None,\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering.Unit","title":"<code>Unit</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.CuratedClustering.make","title":"<code>make(key)</code>","text":"<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    sample_rate = kilosort_dataset.data[\"params\"].get(\n        \"sample_rate\", sample_rate)\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / sample_rate\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ]\n                    if spike_depths is not None\n                    else None,\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation","title":"<code>Curation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Curation procedure table.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> <code>curation_id</code> <code>foreign key, int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time when curation results are generated.</p> <code>curation_output_dir</code> <code> varchar(255) </code> <p>Output directory of the curated results.</p> <code>quality_control</code> <code>bool</code> <p>If True, this clustering result has undergone quality control.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this clustering result.</p> <code>curation_note</code> <code> varchar(2000) </code> <p>Notes about the curation task.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curation procedure table.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n        curation_id (foreign key, int): Unique curation ID.\n        curation_time (datetime): Time when curation results are generated.\n        curation_output_dir ( varchar(255) ): Output directory of the curated results.\n        quality_control (bool): If True, this clustering result has undergone quality control.\n        manual_curation (bool): If True, manual curation has been performed on this clustering result.\n        curation_note ( varchar(2000) ): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual curation procedure\n    -&gt; Clustering\n    curation_id: int\n    ---\n    curation_time: datetime             # time of generation of this set of curated clustering results\n    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n    quality_control: bool               # has this clustering result undergone quality control?\n    manual_curation: bool               # has manual curation been performed on this clustering result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_clustering_task(self, key, curation_note=\"\"):\n\"\"\"\n        A function to create a new corresponding \"Curation\" for a particular\n        \"ClusteringTask\"\n        \"\"\"\n        if key not in Clustering():\n            raise ValueError(\n                f\"No corresponding entry in Clustering available\"\n                f\" for: {key}; do `Clustering.populate(key)`\"\n            )\n\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n            kilosort_dir\n        )\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": creation_time,\n                \"curation_output_dir\": output_dir,\n                \"quality_control\": is_qc,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.Curation.create1_from_clustering_task","title":"<code>create1_from_clustering_task(key, curation_note='')</code>","text":"<p>A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\"</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def create1_from_clustering_task(self, key, curation_note=\"\"):\n\"\"\"\n    A function to create a new corresponding \"Curation\" for a particular\n    \"ClusteringTask\"\n    \"\"\"\n    if key not in Clustering():\n        raise ValueError(\n            f\"No corresponding entry in Clustering available\"\n            f\" for: {key}; do `Clustering.populate(key)`\"\n        )\n\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n        kilosort_dir\n    )\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": creation_time,\n            \"curation_output_dir\": output_dir,\n            \"quality_control\": is_qc,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording","title":"<code>EphysRecording</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found\"\n                f\" in {session_dir}\"\n            )\n\n        supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        key)\n                )\n\n            if spikeglx_meta.probe_model in supported_probe_types:\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\n                    \"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key,\n                    \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(\n                        key)\n                )\n\n            if not probe_data.ap_meta:\n                raise IOError(\n                    'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n                )\n\n            if probe_data.probe_model in supported_probe_types:\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\n                    \"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(\n                        probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n            # explicitly garbage collect \"dataset\"\n            # as these may have large memory footprint and may not be cleared fast enough\n            del probe_data, dataset\n            gc.collect()\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording.EphysFile","title":"<code>EphysFile</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.EphysRecording.make","title":"<code>make(key)</code>","text":"<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found\"\n            f\" in {session_dir}\"\n        )\n\n    supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No SpikeGLX data found for probe insertion: {}\".format(\n                    key)\n            )\n\n        if spikeglx_meta.probe_model in supported_probe_types:\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\n                \"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key,\n                \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(\n                    key)\n            )\n\n        if not probe_data.ap_meta:\n            raise IOError(\n                'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n            )\n\n        if probe_data.probe_model in supported_probe_types:\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\n                \"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(\n                    probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n        # explicitly garbage collect \"dataset\"\n        # as these may have large memory footprint and may not be cleared fast enough\n        del probe_data, dataset\n        gc.collect()\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.InsertionLocation","title":"<code>InsertionLocation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP","title":"<code>LFP</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; EphysRecording\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software = (EphysRecording * ProbeInsertion &amp;\n                        key).fetch1(\"acq_software\")\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(\n                spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1:: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp *\n                spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(\n                    probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            oe_probe = get_openephys_probe_data(key)\n\n            lfp_channel_ind = np.r_[\n                len(oe_probe.lfp_meta[\"channels_indices\"])\n                - 1: 0: -self._skip_channel_counts\n            ]\n\n            # (sample x channel)\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n            lfp = (\n                lfp *\n                np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_keys.extend(\n                probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n            )\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP.Electrode","title":"<code>Electrode</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.LFP.make","title":"<code>make(key)</code>","text":"<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software = (EphysRecording * ProbeInsertion &amp;\n                    key).fetch1(\"acq_software\")\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(\n            spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1:: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp *\n            spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(\n                probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(key)\n\n        lfp_channel_ind = np.r_[\n            len(oe_probe.lfp_meta[\"channels_indices\"])\n            - 1: 0: -self._skip_channel_counts\n        ]\n\n        # (sample x channel)\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n        lfp = (\n            lfp *\n            np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        electrode_keys.extend(\n            probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n        )\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ProbeInsertion","title":"<code>ProbeInsertion</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion implanted into an animal for a given session.\n    -&gt; Session\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    \"\"\"\n\n    @classmethod\n    def auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(session_key)\n        )\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n            )\n\n        probe_list, probe_insertion_list = [], []\n        if acq_software == \"SpikeGLX\":\n            for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                probe_key = {\n                    \"probe_type\": spikeglx_meta.probe_model,\n                    \"probe\": spikeglx_meta.probe_SN,\n                }\n                if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                    probe_list.append(probe_key)\n\n                probe_dir = meta_filepath.parent\n                try:\n                    probe_number = re.search(\n                        \"(imec)?\\d{1}$\", probe_dir.name).group()\n                    probe_number = int(probe_number.replace(\"imec\", \"\"))\n                except AttributeError:\n                    probe_number = meta_fp_idx\n\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": spikeglx_meta.probe_SN,\n                        \"insertion_number\": int(probe_number),\n                    }\n                )\n        elif acq_software == \"Open Ephys\":\n            loaded_oe = openephys.OpenEphys(session_dir)\n            for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n                probe_key = {\n                    \"probe_type\": oe_probe.probe_model,\n                    \"probe\": oe_probe.probe_SN,\n                }\n                if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                    probe_list.append(probe_key)\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": oe_probe.probe_SN,\n                        \"insertion_number\": probe_idx,\n                    }\n                )\n        else:\n            raise NotImplementedError(\n                f\"Unknown acquisition software: {acq_software}\")\n\n        probe.Probe.insert(probe_list, skip_duplicates=True)\n        cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.ProbeInsertion.auto_generate_entries","title":"<code>auto_generate_entries(session_key)</code>  <code>classmethod</code>","text":"<p>Automatically populate entries in ProbeInsertion table for a session.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(session_key)\n    )\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n        )\n\n    probe_list, probe_insertion_list = [], []\n    if acq_software == \"SpikeGLX\":\n        for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n            probe_key = {\n                \"probe_type\": spikeglx_meta.probe_model,\n                \"probe\": spikeglx_meta.probe_SN,\n            }\n            if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                probe_list.append(probe_key)\n\n            probe_dir = meta_filepath.parent\n            try:\n                probe_number = re.search(\n                    \"(imec)?\\d{1}$\", probe_dir.name).group()\n                probe_number = int(probe_number.replace(\"imec\", \"\"))\n            except AttributeError:\n                probe_number = meta_fp_idx\n\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": spikeglx_meta.probe_SN,\n                    \"insertion_number\": int(probe_number),\n                }\n            )\n    elif acq_software == \"Open Ephys\":\n        loaded_oe = openephys.OpenEphys(session_dir)\n        for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n            probe_key = {\n                \"probe_type\": oe_probe.probe_model,\n                \"probe\": oe_probe.probe_SN,\n            }\n            if probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]:\n                probe_list.append(probe_key)\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": oe_probe.probe_SN,\n                    \"insertion_number\": probe_idx,\n                }\n            )\n    else:\n        raise NotImplementedError(\n            f\"Unknown acquisition software: {acq_software}\")\n\n    probe.Probe.insert(probe_list, skip_duplicates=True)\n    cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics","title":"<code>QualityMetrics</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.Cluster","title":"<code>Cluster</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.QualityMetrics.make","title":"<code>make(key)</code>","text":"<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet","title":"<code>WaveformSet</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if is_qc:\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            if unit_peak_waveform:\n                self.PeakWaveform.insert1(\n                    unit_peak_waveform, ignore_extra_fields=True)\n            if unit_electrode_waveforms:\n                self.Waveform.insert(\n                    unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.PeakWaveform","title":"<code>PeakWaveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.WaveformSet.make","title":"<code>make(key)</code>","text":"<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if is_qc:\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(\n                spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        if unit_peak_waveform:\n            self.PeakWaveform.insert1(\n                unit_peak_waveform, ignore_extra_fields=True)\n        if unit_electrode_waveforms:\n            self.Waveform.insert(\n                unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.activate","title":"<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>","text":"<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe scehma.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe scehma.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n        get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.generate_electrode_config","title":"<code>generate_electrode_config(probe_type, electrode_keys)</code>","text":"<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid(\n        {k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    root_directories = _linking_module.get_ephys_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_neuropixels_channel2electrode_map","title":"<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>","text":"<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(\n            ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        probe_dataset = get_openephys_probe_data(ephys_recording_key)\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_indices\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_openephys_probe_data","title":"<code>get_openephys_probe_data(ephys_recording_key)</code>","text":"<p>Get OpenEphys probe data from file.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_openephys_probe_data(ephys_recording_key: dict) -&gt; list:\n\"\"\"Get OpenEphys probe data from file.\"\"\"\n    inserted_probe_serial_number = (\n        ProbeInsertion * probe.Probe &amp; ephys_recording_key\n    ).fetch1(\"probe\")\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n    )\n    loaded_oe = openephys.OpenEphys(session_dir)\n    probe_data = loaded_oe.probes[inserted_probe_serial_number]\n\n    # explicitly garbage collect \"loaded_oe\"\n    # as these may have large memory footprint and may not be cleared fast enough\n    del loaded_oe\n    gc.collect()\n\n    return probe_data\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_processed_root_data_dir","title":"<code>get_processed_root_data_dir()</code>","text":"<p>Retrieve the root directory for all processed data.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string for the full path to the root directory for processed data.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_processed_root_data_dir() -&gt; str:\n\"\"\"Retrieve the root directory for all processed data.\n\n    Returns:\n        A string for the full path to the root directory for processed data.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_ephys_root_data_dir()[0]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_recording_channels_details","title":"<code>get_recording_channels_details(ephys_recording_key)</code>","text":"<p>Get details of recording channels for a given recording.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_recording_channels_details(ephys_recording_key: dict) -&gt; np.array:\n\"\"\"Get details of recording channels for a given recording.\"\"\"\n    channels_details = {}\n\n    acq_software, sample_rate = (EphysRecording &amp; ephys_recording_key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    probe_type = (ProbeInsertion * probe.Probe &amp; ephys_recording_key).fetch1(\n        \"probe_type\"\n    )\n    channels_details[\"probe_type\"] = {\n        \"neuropixels 1.0 - 3A\": \"3A\",\n        \"neuropixels 1.0 - 3B\": \"NP1\",\n        \"neuropixels UHD\": \"NP1100\",\n        \"neuropixels 2.0 - SS\": \"NP21\",\n        \"neuropixels 2.0 - MS\": \"NP24\",\n    }[probe_type]\n\n    electrode_config_key = (\n        probe.ElectrodeConfig * EphysRecording &amp; ephys_recording_key\n    ).fetch1(\"KEY\")\n    (\n        channels_details[\"channel_ind\"],\n        channels_details[\"x_coords\"],\n        channels_details[\"y_coords\"],\n        channels_details[\"shank_ind\"],\n    ) = (\n        probe.ElectrodeConfig.Electrode * probe.ProbeType.Electrode\n        &amp; electrode_config_key\n    ).fetch(\n        \"electrode\", \"x_coord\", \"y_coord\", \"shank\"\n    )\n    channels_details[\"sample_rate\"] = sample_rate\n    channels_details[\"num_channels\"] = len(channels_details[\"channel_ind\"])\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(\n            ephys_recording_key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        channels_details[\"uVPerBit\"] = spikeglx_recording.get_channel_bit_volts(\"ap\")[\n            0]\n        channels_details[\"connected\"] = np.array(\n            [v for *_, v in spikeglx_recording.apmeta.shankmap[\"data\"]]\n        )\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(ephys_recording_key)\n        channels_details[\"uVPerBit\"] = oe_probe.ap_meta[\"channels_gains\"][0]\n        channels_details[\"connected\"] = np.array(\n            [\n                int(v == 1)\n                for c, v in oe_probe.channels_connected.items()\n                if c in channels_details[\"channel_ind\"]\n            ]\n        )\n\n    return channels_details\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_acute/#element_array_ephys.ephys_acute.get_spikeglx_meta_filepath","title":"<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>","text":"<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_acute.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = pathlib.Path(\n        (\n            EphysRecording.EphysFile\n            &amp; ephys_recording_key\n            &amp; 'file_path LIKE \"%.ap.meta\"'\n        ).fetch1(\"file_path\")\n    )\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [\n                fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/","title":"ephys_chronic.py","text":""},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.AcquisitionSoftware","title":"<code>AcquisitionSoftware</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Software used for recording of neuropixels probes\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusterQualityLabel","title":"<code>ClusterQualityLabel</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code>varchar(4000)</code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description (varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)  # cluster quality type - e.g. 'good', 'MUA', 'noise', etc.\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering","title":"<code>Clustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version (varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ClusteringTask.infer_output_dir(\n                key, relative=True, mkdir=True)\n            # update clustering_output_dir\n            ClusteringTask.update1(\n                {**key, \"clustering_output_dir\": output_dir.as_posix()}\n            )\n\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n        elif task_mode == \"trigger\":\n            acq_software, clustering_method, params = (\n                ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n            ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n            if \"kilosort\" in clustering_method:\n                from element_array_ephys.readers import kilosort_triggering\n\n                # add additional probe-recording and channels details into `params`\n                params = {**params, **get_recording_channels_details(key)}\n                params[\"fs\"] = params[\"sample_rate\"]\n\n                if acq_software == \"SpikeGLX\":\n                    spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                    spikeglx_recording = spikeglx.SpikeGLX(\n                        spikeglx_meta_filepath.parent\n                    )\n                    spikeglx_recording.validate_file(\"ap\")\n                    run_CatGT = (\n                        params.pop(\"run_CatGT\", True)\n                        and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                    )\n\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=spikeglx_recording.root_dir\n                            / (spikeglx_recording.root_name + \".ap.bin\"),\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                            npx_input_dir=spikeglx_meta_filepath.parent,\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                            run_CatGT=run_CatGT,\n                        )\n                        run_kilosort.run_modules()\n                elif acq_software == \"Open Ephys\":\n                    oe_probe = get_openephys_probe_data(key)\n\n                    assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                    # run kilosort\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=pathlib.Path(\n                                oe_probe.recording_info[\"recording_files\"][0]\n                            )\n                            / \"continuous.dat\",\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                            npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        )\n                        run_kilosort.run_modules()\n            else:\n                raise NotImplementedError(\n                    f\"Automatic triggering of {clustering_method}\"\n                    f\" clustering analysis is not yet supported\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Clustering.make","title":"<code>make(key)</code>","text":"<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ClusteringTask.infer_output_dir(\n            key, relative=True, mkdir=True)\n        # update clustering_output_dir\n        ClusteringTask.update1(\n            {**key, \"clustering_output_dir\": output_dir.as_posix()}\n        )\n\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n    elif task_mode == \"trigger\":\n        acq_software, clustering_method, params = (\n            ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n        ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n        if \"kilosort\" in clustering_method:\n            from element_array_ephys.readers import kilosort_triggering\n\n            # add additional probe-recording and channels details into `params`\n            params = {**params, **get_recording_channels_details(key)}\n            params[\"fs\"] = params[\"sample_rate\"]\n\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                spikeglx_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent\n                )\n                spikeglx_recording.validate_file(\"ap\")\n                run_CatGT = (\n                    params.pop(\"run_CatGT\", True)\n                    and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                )\n\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=spikeglx_recording.root_dir\n                        / (spikeglx_recording.root_name + \".ap.bin\"),\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                        npx_input_dir=spikeglx_meta_filepath.parent,\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        run_CatGT=run_CatGT,\n                    )\n                    run_kilosort.run_modules()\n            elif acq_software == \"Open Ephys\":\n                oe_probe = get_openephys_probe_data(key)\n\n                assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                # run kilosort\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=pathlib.Path(\n                            oe_probe.recording_info[\"recording_files\"][0]\n                        )\n                        / \"continuous.dat\",\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                        npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                    )\n                    run_kilosort.run_modules()\n        else:\n            raise NotImplementedError(\n                f\"Automatic triggering of {clustering_method}\"\n                f\" clustering analysis is not yet supported\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringMethod","title":"<code>ClusteringMethod</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n        (\"kilosort2.5\", \"kilosort2.5 clustering method\"),\n        (\"kilosort3\", \"kilosort3 clustering method\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringParamSet","title":"<code>ClusteringParamSet</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> <code>params</code> <code>longblob</code> <p>Parameters for clustering with Kilosort.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob): Parameters for clustering with Kilosort.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        clustering_method: str,\n        paramset_desc: str,\n        params: dict,\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"clustering_method\": clustering_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(\n                {**params, \"clustering_method\": clustering_method}\n            ),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringParamSet.insert_new_params","title":"<code>insert_new_params(clustering_method, paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>","text":"<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    clustering_method: str,\n    paramset_desc: str,\n    params: dict,\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"clustering_method\": clustering_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(\n            {**params, \"clustering_method\": clustering_method}\n        ),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask","title":"<code>ClusteringTask</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_outdir_dir</code> <code>varchar(255)</code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_outdir_dir (varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir='': varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer output directory if it is not provided.\n\n        Args:\n            key (dict): ClusteringTask primary key.\n\n        Returns:\n            Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n            e.g.: sub4/sess1/probe_2/kilosort2_0\n        \"\"\"\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        sess_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key))\n        root_dir = find_root_directory(get_ephys_root_data_dir(), sess_dir)\n\n        method = (\n            (ClusteringParamSet * ClusteringMethod &amp; key)\n            .fetch1(\"clustering_method\")\n            .replace(\".\", \"-\")\n        )\n\n        output_dir = (\n            processed_dir\n            / sess_dir.relative_to(root_dir)\n            / f'probe_{key[\"insertion_number\"]}'\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            log.info(f\"{output_dir} created!\")\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n        Args:\n            ephys_recording_key (dict): EphysRecording primary key.\n            paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n        \"\"\"\n        key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = ClusteringTask.infer_output_dir(\n            key, relative=False, mkdir=True)\n\n        try:\n            kilosort.Kilosort(\n                output_dir\n            )  # check if the directory is a valid Kilosort output\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"clustering_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask.auto_generate_entries","title":"<code>auto_generate_entries(ephys_recording_key, paramset_idx=0)</code>  <code>classmethod</code>","text":"<p>Autogenerate entries based on a particular ephys recording.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_recording_key</code> <code>dict</code> <p>EphysRecording primary key.</p> required <code>paramset_idx</code> <code>int</code> <p>Parameter index to use for clustering task. Defaults to 0.</p> <code>0</code> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n    Args:\n        ephys_recording_key (dict): EphysRecording primary key.\n        paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n    \"\"\"\n    key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = ClusteringTask.infer_output_dir(\n        key, relative=False, mkdir=True)\n\n    try:\n        kilosort.Kilosort(\n            output_dir\n        )  # check if the directory is a valid Kilosort output\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"clustering_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ClusteringTask.infer_output_dir","title":"<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>","text":"<p>Infer output directory if it is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>ClusteringTask primary key.</p> required <p>Returns:</p> Type Description <p>Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}</p> <p>e.g.: sub4/sess1/probe_2/kilosort2_0</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n\"\"\"Infer output directory if it is not provided.\n\n    Args:\n        key (dict): ClusteringTask primary key.\n\n    Returns:\n        Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n        e.g.: sub4/sess1/probe_2/kilosort2_0\n    \"\"\"\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    sess_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key))\n    root_dir = find_root_directory(get_ephys_root_data_dir(), sess_dir)\n\n    method = (\n        (ClusteringParamSet * ClusteringMethod &amp; key)\n        .fetch1(\"clustering_method\")\n        .replace(\".\", \"-\")\n    )\n\n    output_dir = (\n        processed_dir\n        / sess_dir.relative_to(root_dir)\n        / f'probe_{key[\"insertion_number\"]}'\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        log.info(f\"{output_dir} created!\")\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering","title":"<code>CuratedClustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Curation primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Curation (foreign key): Curation primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of a curation.\n    -&gt; Curation\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n            \"acq_software\", \"sampling_rate\"\n        )\n\n        sample_rate = kilosort_dataset.data[\"params\"].get(\n            \"sample_rate\", sample_rate)\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / sample_rate\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ]\n                        if spike_depths is not None\n                        else None,\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering.Unit","title":"<code>Unit</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.CuratedClustering.make","title":"<code>make(key)</code>","text":"<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    sample_rate = kilosort_dataset.data[\"params\"].get(\n        \"sample_rate\", sample_rate)\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / sample_rate\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ]\n                    if spike_depths is not None\n                    else None,\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation","title":"<code>Curation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Curation procedure table.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> <code>curation_id</code> <code>foreign key, int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time when curation results are generated.</p> <code>curation_output_dir</code> <code>varchar(255)</code> <p>Output directory of the curated results.</p> <code>quality_control</code> <code>bool</code> <p>If True, this clustering result has undergone quality control.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this clustering result.</p> <code>curation_note</code> <code>varchar(2000)</code> <p>Notes about the curation task.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curation procedure table.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n        curation_id (foreign key, int): Unique curation ID.\n        curation_time (datetime): Time when curation results are generated.\n        curation_output_dir (varchar(255) ): Output directory of the curated results.\n        quality_control (bool): If True, this clustering result has undergone quality control.\n        manual_curation (bool): If True, manual curation has been performed on this clustering result.\n        curation_note (varchar(2000) ): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual curation procedure\n    -&gt; Clustering\n    curation_id: int\n    ---\n    curation_time: datetime             # time of generation of this set of curated clustering results\n    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n    quality_control: bool               # has this clustering result undergone quality control?\n    manual_curation: bool               # has manual curation been performed on this clustering result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n        A function to create a new corresponding \"Curation\" for a particular\n        \"ClusteringTask\"\n        \"\"\"\n        if key not in Clustering():\n            raise ValueError(\n                f\"No corresponding entry in Clustering available\"\n                f\" for: {key}; do `Clustering.populate(key)`\"\n            )\n\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n            kilosort_dir\n        )\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": creation_time,\n                \"curation_output_dir\": output_dir,\n                \"quality_control\": is_qc,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.Curation.create1_from_clustering_task","title":"<code>create1_from_clustering_task(key, curation_note='')</code>","text":"<p>A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\"</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n    A function to create a new corresponding \"Curation\" for a particular\n    \"ClusteringTask\"\n    \"\"\"\n    if key not in Clustering():\n        raise ValueError(\n            f\"No corresponding entry in Clustering available\"\n            f\" for: {key}; do `Clustering.populate(key)`\"\n        )\n\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n        kilosort_dir\n    )\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": creation_time,\n            \"curation_output_dir\": output_dir,\n            \"quality_control\": is_qc,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording","title":"<code>EphysRecording</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; Session\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found\"\n                f\" in {session_dir}\"\n            )\n\n        supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    f\"No SpikeGLX data found for probe insertion: {key}\"\n                    + \" The probe serial number does not match.\"\n                )\n\n            if spikeglx_meta.probe_model in supported_probe_types:\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\n                    \"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key,\n                    \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(\n                        key)\n                )\n\n            if not probe_data.ap_meta:\n                raise IOError(\n                    'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n                )\n\n            if probe_data.probe_model in supported_probe_types:\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\n                    \"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(\n                        probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n            # explicitly garbage collect \"dataset\"\n            # as these may have large memory footprint and may not be cleared fast enough\n            del probe_data, dataset\n            gc.collect()\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording.EphysFile","title":"<code>EphysFile</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.EphysRecording.make","title":"<code>make(key)</code>","text":"<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found\"\n            f\" in {session_dir}\"\n        )\n\n    supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                f\"No SpikeGLX data found for probe insertion: {key}\"\n                + \" The probe serial number does not match.\"\n            )\n\n        if spikeglx_meta.probe_model in supported_probe_types:\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\n                \"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key,\n                \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(\n                    key)\n            )\n\n        if not probe_data.ap_meta:\n            raise IOError(\n                'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n            )\n\n        if probe_data.probe_model in supported_probe_types:\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\n                \"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(\n                    probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n        # explicitly garbage collect \"dataset\"\n        # as these may have large memory footprint and may not be cleared fast enough\n        del probe_data, dataset\n        gc.collect()\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.InsertionLocation","title":"<code>InsertionLocation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP","title":"<code>LFP</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; EphysRecording\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software = (EphysRecording * ProbeInsertion &amp;\n                        key).fetch1(\"acq_software\")\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(\n                spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1:: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp *\n                spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(\n                    probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            oe_probe = get_openephys_probe_data(key)\n\n            lfp_channel_ind = np.r_[\n                len(oe_probe.lfp_meta[\"channels_indices\"])\n                - 1: 0: -self._skip_channel_counts\n            ]\n\n            # (sample x channel)\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n            lfp = (\n                lfp *\n                np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_keys.extend(\n                probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n            )\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP.Electrode","title":"<code>Electrode</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.LFP.make","title":"<code>make(key)</code>","text":"<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software = (EphysRecording * ProbeInsertion &amp;\n                    key).fetch1(\"acq_software\")\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(\n            spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1:: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp *\n            spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(\n                probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(key)\n\n        lfp_channel_ind = np.r_[\n            len(oe_probe.lfp_meta[\"channels_indices\"])\n            - 1: 0: -self._skip_channel_counts\n        ]\n\n        # (sample x channel)\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]\n        lfp = (\n            lfp *\n            np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        electrode_keys.extend(\n            probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n        )\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.ProbeInsertion","title":"<code>ProbeInsertion</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion chronically implanted into an animal.\n    -&gt; Subject\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    insertion_datetime=null: datetime\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics","title":"<code>QualityMetrics</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.Cluster","title":"<code>Cluster</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.QualityMetrics.make","title":"<code>make(key)</code>","text":"<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet","title":"<code>WaveformSet</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if is_qc:\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            if unit_peak_waveform:\n                self.PeakWaveform.insert1(\n                    unit_peak_waveform, ignore_extra_fields=True)\n            if unit_electrode_waveforms:\n                self.Waveform.insert(\n                    unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.PeakWaveform","title":"<code>PeakWaveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.WaveformSet.make","title":"<code>make(key)</code>","text":"<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if is_qc:\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(\n                spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        if unit_peak_waveform:\n            self.PeakWaveform.insert1(\n                unit_peak_waveform, ignore_extra_fields=True)\n        if unit_electrode_waveforms:\n            self.Waveform.insert(\n                unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.activate","title":"<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>","text":"<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe scehma.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe scehma.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n        get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.generate_electrode_config","title":"<code>generate_electrode_config(probe_type, electrode_keys)</code>","text":"<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid(\n        {k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    root_directories = _linking_module.get_ephys_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_neuropixels_channel2electrode_map","title":"<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>","text":"<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(\n            ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        probe_dataset = get_openephys_probe_data(ephys_recording_key)\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_indices\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_openephys_probe_data","title":"<code>get_openephys_probe_data(ephys_recording_key)</code>","text":"<p>Get OpenEphys probe data from file.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_openephys_probe_data(ephys_recording_key: dict) -&gt; list:\n\"\"\"Get OpenEphys probe data from file.\"\"\"\n    inserted_probe_serial_number = (\n        ProbeInsertion * probe.Probe &amp; ephys_recording_key\n    ).fetch1(\"probe\")\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n    )\n    loaded_oe = openephys.OpenEphys(session_dir)\n    probe_data = loaded_oe.probes[inserted_probe_serial_number]\n\n    # explicitly garbage collect \"loaded_oe\"\n    # as these may have large memory footprint and may not be cleared fast enough\n    del loaded_oe\n    gc.collect()\n\n    return probe_data\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_processed_root_data_dir","title":"<code>get_processed_root_data_dir()</code>","text":"<p>Retrieve the root directory for all processed data.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string for the full path to the root directory for processed data.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_processed_root_data_dir() -&gt; str:\n\"\"\"Retrieve the root directory for all processed data.\n\n    Returns:\n        A string for the full path to the root directory for processed data.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_ephys_root_data_dir()[0]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_recording_channels_details","title":"<code>get_recording_channels_details(ephys_recording_key)</code>","text":"<p>Get details of recording channels for a given recording.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_recording_channels_details(ephys_recording_key: dict) -&gt; np.array:\n\"\"\"Get details of recording channels for a given recording.\"\"\"\n    channels_details = {}\n\n    acq_software, sample_rate = (EphysRecording &amp; ephys_recording_key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    probe_type = (ProbeInsertion * probe.Probe &amp; ephys_recording_key).fetch1(\n        \"probe_type\"\n    )\n    channels_details[\"probe_type\"] = {\n        \"neuropixels 1.0 - 3A\": \"3A\",\n        \"neuropixels 1.0 - 3B\": \"NP1\",\n        \"neuropixels UHD\": \"NP1100\",\n        \"neuropixels 2.0 - SS\": \"NP21\",\n        \"neuropixels 2.0 - MS\": \"NP24\",\n    }[probe_type]\n\n    electrode_config_key = (\n        probe.ElectrodeConfig * EphysRecording &amp; ephys_recording_key\n    ).fetch1(\"KEY\")\n    (\n        channels_details[\"channel_ind\"],\n        channels_details[\"x_coords\"],\n        channels_details[\"y_coords\"],\n        channels_details[\"shank_ind\"],\n    ) = (\n        probe.ElectrodeConfig.Electrode * probe.ProbeType.Electrode\n        &amp; electrode_config_key\n    ).fetch(\n        \"electrode\", \"x_coord\", \"y_coord\", \"shank\"\n    )\n    channels_details[\"sample_rate\"] = sample_rate\n    channels_details[\"num_channels\"] = len(channels_details[\"channel_ind\"])\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(\n            ephys_recording_key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        channels_details[\"uVPerBit\"] = spikeglx_recording.get_channel_bit_volts(\"ap\")[\n            0]\n        channels_details[\"connected\"] = np.array(\n            [v for *_, v in spikeglx_recording.apmeta.shankmap[\"data\"]]\n        )\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(ephys_recording_key)\n        channels_details[\"uVPerBit\"] = oe_probe.ap_meta[\"channels_gains\"][0]\n        channels_details[\"connected\"] = np.array(\n            [\n                int(v == 1)\n                for c, v in oe_probe.channels_connected.items()\n                if c in channels_details[\"channel_ind\"]\n            ]\n        )\n\n    return channels_details\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_chronic/#element_array_ephys.ephys_chronic.get_spikeglx_meta_filepath","title":"<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>","text":"<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_chronic.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = pathlib.Path(\n        (\n            EphysRecording.EphysFile\n            &amp; ephys_recording_key\n            &amp; 'file_path LIKE \"%.ap.meta\"'\n        ).fetch1(\"file_path\")\n    )\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [\n                fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/","title":"ephys_no_curation.py","text":""},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.AcquisitionSoftware","title":"<code>AcquisitionSoftware</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusterQualityLabel","title":"<code>ClusterQualityLabel</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code>varchar(4000)</code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description (varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)  # cluster quality type - e.g. 'good', 'MUA', 'noise', etc.\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.Clustering","title":"<code>Clustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version (varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n            # update clustering_output_dir\n            ClusteringTask.update1(\n                {**key, \"clustering_output_dir\": output_dir.as_posix()}\n            )\n\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n        elif task_mode == \"trigger\":\n            acq_software, clustering_method, params = (\n                ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n            ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n            if \"kilosort\" in clustering_method:\n                from element_array_ephys.readers import kilosort_triggering\n\n                # add additional probe-recording and channels details into `params`\n                params = {**params, **get_recording_channels_details(key)}\n                params[\"fs\"] = params[\"sample_rate\"]\n\n                if acq_software == \"SpikeGLX\":\n                    spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                    spikeglx_recording = spikeglx.SpikeGLX(\n                        spikeglx_meta_filepath.parent\n                    )\n                    spikeglx_recording.validate_file(\"ap\")\n                    run_CatGT = (\n                        params.pop(\"run_CatGT\", True)\n                        and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                    )\n\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=spikeglx_recording.root_dir\n                            / (spikeglx_recording.root_name + \".ap.bin\"),\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                            npx_input_dir=spikeglx_meta_filepath.parent,\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                            run_CatGT=run_CatGT,\n                        )\n                        run_kilosort.run_modules()\n                elif acq_software == \"Open Ephys\":\n                    oe_probe = get_openephys_probe_data(key)\n\n                    assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                    # run kilosort\n                    if clustering_method.startswith(\"pykilosort\"):\n                        kilosort_triggering.run_pykilosort(\n                            continuous_file=pathlib.Path(\n                                oe_probe.recording_info[\"recording_files\"][0]\n                            )\n                            / \"continuous.dat\",\n                            kilosort_output_directory=kilosort_dir,\n                            channel_ind=params.pop(\"channel_ind\"),\n                            x_coords=params.pop(\"x_coords\"),\n                            y_coords=params.pop(\"y_coords\"),\n                            shank_ind=params.pop(\"shank_ind\"),\n                            connected=params.pop(\"connected\"),\n                            sample_rate=params.pop(\"sample_rate\"),\n                            params=params,\n                        )\n                    else:\n                        run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                            npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                            ks_output_dir=kilosort_dir,\n                            params=params,\n                            KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        )\n                        run_kilosort.run_modules()\n            else:\n                raise NotImplementedError(\n                    f\"Automatic triggering of {clustering_method}\"\n                    f\" clustering analysis is not yet supported\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.Clustering.make","title":"<code>make(key)</code>","text":"<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = ClusteringTask.infer_output_dir(key, relative=True, mkdir=True)\n        # update clustering_output_dir\n        ClusteringTask.update1(\n            {**key, \"clustering_output_dir\": output_dir.as_posix()}\n        )\n\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n    elif task_mode == \"trigger\":\n        acq_software, clustering_method, params = (\n            ClusteringTask * EphysRecording * ClusteringParamSet &amp; key\n        ).fetch1(\"acq_software\", \"clustering_method\", \"params\")\n\n        if \"kilosort\" in clustering_method:\n            from element_array_ephys.readers import kilosort_triggering\n\n            # add additional probe-recording and channels details into `params`\n            params = {**params, **get_recording_channels_details(key)}\n            params[\"fs\"] = params[\"sample_rate\"]\n\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                spikeglx_recording = spikeglx.SpikeGLX(\n                    spikeglx_meta_filepath.parent\n                )\n                spikeglx_recording.validate_file(\"ap\")\n                run_CatGT = (\n                    params.pop(\"run_CatGT\", True)\n                    and \"_tcat.\" not in spikeglx_meta_filepath.stem\n                )\n\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=spikeglx_recording.root_dir\n                        / (spikeglx_recording.root_name + \".ap.bin\"),\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.SGLXKilosortPipeline(\n                        npx_input_dir=spikeglx_meta_filepath.parent,\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                        run_CatGT=run_CatGT,\n                    )\n                    run_kilosort.run_modules()\n            elif acq_software == \"Open Ephys\":\n                oe_probe = get_openephys_probe_data(key)\n\n                assert len(oe_probe.recording_info[\"recording_files\"]) == 1\n\n                # run kilosort\n                if clustering_method.startswith(\"pykilosort\"):\n                    kilosort_triggering.run_pykilosort(\n                        continuous_file=pathlib.Path(\n                            oe_probe.recording_info[\"recording_files\"][0]\n                        )\n                        / \"continuous.dat\",\n                        kilosort_output_directory=kilosort_dir,\n                        channel_ind=params.pop(\"channel_ind\"),\n                        x_coords=params.pop(\"x_coords\"),\n                        y_coords=params.pop(\"y_coords\"),\n                        shank_ind=params.pop(\"shank_ind\"),\n                        connected=params.pop(\"connected\"),\n                        sample_rate=params.pop(\"sample_rate\"),\n                        params=params,\n                    )\n                else:\n                    run_kilosort = kilosort_triggering.OpenEphysKilosortPipeline(\n                        npx_input_dir=oe_probe.recording_info[\"recording_files\"][0],\n                        ks_output_dir=kilosort_dir,\n                        params=params,\n                        KS2ver=f'{Decimal(clustering_method.replace(\"kilosort\", \"\")):.1f}',\n                    )\n                    run_kilosort.run_modules()\n        else:\n            raise NotImplementedError(\n                f\"Automatic triggering of {clustering_method}\"\n                f\" clustering analysis is not yet supported\"\n            )\n\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringMethod","title":"<code>ClusteringMethod</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n        (\"kilosort2.5\", \"kilosort2.5 clustering method\"),\n        (\"kilosort3\", \"kilosort3 clustering method\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringParamSet","title":"<code>ClusteringParamSet</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob)\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        clustering_method: str,\n        paramset_desc: str,\n        params: dict,\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"clustering_method\": clustering_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(\n                {**params, \"clustering_method\": clustering_method}\n            ),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringParamSet.insert_new_params","title":"<code>insert_new_params(clustering_method, paramset_desc, params, paramset_idx=None)</code>  <code>classmethod</code>","text":"<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    clustering_method: str,\n    paramset_desc: str,\n    params: dict,\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"clustering_method\": clustering_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(\n            {**params, \"clustering_method\": clustering_method}\n        ),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask","title":"<code>ClusteringTask</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_outdir_dir</code> <code>varchar(255)</code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_outdir_dir (varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir='': varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative: bool = False, mkdir: bool = False):\n\"\"\"Infer output directory if it is not provided.\n\n        Args:\n            key (dict): ClusteringTask primary key.\n\n        Returns:\n            Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n            e.g.: sub4/sess1/probe_2/kilosort2_0\n        \"\"\"\n        processed_dir = pathlib.Path(get_processed_root_data_dir())\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n        root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n        method = (\n            (ClusteringParamSet * ClusteringMethod &amp; key)\n            .fetch1(\"clustering_method\")\n            .replace(\".\", \"-\")\n        )\n\n        output_dir = (\n            processed_dir\n            / session_dir.relative_to(root_dir)\n            / f'probe_{key[\"insertion_number\"]}'\n            / f'{method}_{key[\"paramset_idx\"]}'\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n            log.info(f\"{output_dir} created!\")\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n        Args:\n            ephys_recording_key (dict): EphysRecording primary key.\n            paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n        \"\"\"\n        key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n        processed_dir = get_processed_root_data_dir()\n        output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n        try:\n            kilosort.Kilosort(\n                output_dir\n            )  # check if the directory is a valid Kilosort output\n        except FileNotFoundError:\n            task_mode = \"trigger\"\n        else:\n            task_mode = \"load\"\n\n        cls.insert1(\n            {\n                **key,\n                \"clustering_output_dir\": output_dir.relative_to(\n                    processed_dir\n                ).as_posix(),\n                \"task_mode\": task_mode,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask.auto_generate_entries","title":"<code>auto_generate_entries(ephys_recording_key, paramset_idx=0)</code>  <code>classmethod</code>","text":"<p>Autogenerate entries based on a particular ephys recording.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_recording_key</code> <code>dict</code> <p>EphysRecording primary key.</p> required <code>paramset_idx</code> <code>int</code> <p>Parameter index to use for clustering task. Defaults to 0.</p> <code>0</code> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, ephys_recording_key: dict, paramset_idx: int = 0):\n\"\"\"Autogenerate entries based on a particular ephys recording.\n\n    Args:\n        ephys_recording_key (dict): EphysRecording primary key.\n        paramset_idx (int, optional): Parameter index to use for clustering task. Defaults to 0.\n    \"\"\"\n    key = {**ephys_recording_key, \"paramset_idx\": paramset_idx}\n\n    processed_dir = get_processed_root_data_dir()\n    output_dir = ClusteringTask.infer_output_dir(key, relative=False, mkdir=True)\n\n    try:\n        kilosort.Kilosort(\n            output_dir\n        )  # check if the directory is a valid Kilosort output\n    except FileNotFoundError:\n        task_mode = \"trigger\"\n    else:\n        task_mode = \"load\"\n\n    cls.insert1(\n        {\n            **key,\n            \"clustering_output_dir\": output_dir.relative_to(\n                processed_dir\n            ).as_posix(),\n            \"task_mode\": task_mode,\n        }\n    )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ClusteringTask.infer_output_dir","title":"<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>","text":"<p>Infer output directory if it is not provided.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>ClusteringTask primary key.</p> required <p>Returns:</p> Type Description <p>Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}</p> <p>e.g.: sub4/sess1/probe_2/kilosort2_0</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative: bool = False, mkdir: bool = False):\n\"\"\"Infer output directory if it is not provided.\n\n    Args:\n        key (dict): ClusteringTask primary key.\n\n    Returns:\n        Pathlib.Path: Expected clustering_output_dir based on the following convention: processed_dir / session_dir / probe_{insertion_number} / {clustering_method}_{paramset_idx}\n        e.g.: sub4/sess1/probe_2/kilosort2_0\n    \"\"\"\n    processed_dir = pathlib.Path(get_processed_root_data_dir())\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n    root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n\n    method = (\n        (ClusteringParamSet * ClusteringMethod &amp; key)\n        .fetch1(\"clustering_method\")\n        .replace(\".\", \"-\")\n    )\n\n    output_dir = (\n        processed_dir\n        / session_dir.relative_to(root_dir)\n        / f'probe_{key[\"insertion_number\"]}'\n        / f'{method}_{key[\"paramset_idx\"]}'\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        log.info(f\"{output_dir} created!\")\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering","title":"<code>CuratedClustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of the spike sorting step.\n    -&gt; Clustering\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n            \"acq_software\", \"sampling_rate\"\n        )\n\n        sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / sample_rate\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering.Unit","title":"<code>Unit</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.CuratedClustering.make","title":"<code>make(key)</code>","text":"<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software, sample_rate = (EphysRecording &amp; key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    sample_rate = kilosort_dataset.data[\"params\"].get(\"sample_rate\", sample_rate)\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / sample_rate\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording","title":"<code>EphysRecording</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                \"Ephys recording data not found!\"\n                \" Neither SpikeGLX nor Open Ephys recording files found\"\n            )\n\n        supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n\n            if spikeglx_meta.probe_model in supported_probe_types:\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(key)\n                )\n\n            if not probe_data.ap_meta:\n                raise IOError(\n                    'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n                )\n\n            if probe_data.probe_model in supported_probe_types:\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n            # explicitly garbage collect \"dataset\"\n            # as these may have large memory footprint and may not be cleared fast enough\n            del probe_data, dataset\n            gc.collect()\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording.EphysFile","title":"<code>EphysFile</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.EphysRecording.make","title":"<code>make(key)</code>","text":"<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            \"Ephys recording data not found!\"\n            \" Neither SpikeGLX nor Open Ephys recording files found\"\n        )\n\n    supported_probe_types = probe.ProbeType.fetch(\"probe_type\")\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No SpikeGLX data found for probe insertion: {}\".format(key)\n            )\n\n        if spikeglx_meta.probe_model in supported_probe_types:\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(key)\n            )\n\n        if not probe_data.ap_meta:\n            raise IOError(\n                'No analog signals found - check \"structure.oebin\" file or \"continuous\" directory'\n            )\n\n        if probe_data.probe_model in supported_probe_types:\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_indices\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n        # explicitly garbage collect \"dataset\"\n        # as these may have large memory footprint and may not be cleared fast enough\n        del probe_data, dataset\n        gc.collect()\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.InsertionLocation","title":"<code>InsertionLocation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis\n\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP","title":"<code>LFP</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; EphysRecording\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n            oe_probe = get_openephys_probe_data(key)\n\n            lfp_channel_ind = np.r_[\n                len(oe_probe.lfp_meta[\"channels_indices\"])\n                - 1 : 0 : -self._skip_channel_counts\n            ]\n\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n            lfp = (\n                lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_keys.extend(\n                probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n            )\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP.Electrode","title":"<code>Electrode</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.LFP.make","title":"<code>make(key)</code>","text":"<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software = (EphysRecording * ProbeInsertion &amp; key).fetch1(\"acq_software\")\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(key)\n\n        lfp_channel_ind = np.r_[\n            len(oe_probe.lfp_meta[\"channels_indices\"])\n            - 1 : 0 : -self._skip_channel_counts\n        ]\n\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n        lfp = (\n            lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        electrode_keys.extend(\n            probe_electrodes[channel_idx] for channel_idx in lfp_channel_ind\n        )\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ProbeInsertion","title":"<code>ProbeInsertion</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion implanted into an animal for a given session.\n    -&gt; Session\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    \"\"\"\n\n    @classmethod\n    def auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(session_key)\n        )\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n            )\n\n        probe_list, probe_insertion_list = [], []\n        if acq_software == \"SpikeGLX\":\n            for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                probe_key = {\n                    \"probe_type\": spikeglx_meta.probe_model,\n                    \"probe\": spikeglx_meta.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n\n                probe_dir = meta_filepath.parent\n                try:\n                    probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                    probe_number = int(probe_number.replace(\"imec\", \"\"))\n                except AttributeError:\n                    probe_number = meta_fp_idx\n\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": spikeglx_meta.probe_SN,\n                        \"insertion_number\": int(probe_number),\n                    }\n                )\n        elif acq_software == \"Open Ephys\":\n            loaded_oe = openephys.OpenEphys(session_dir)\n            for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n                probe_key = {\n                    \"probe_type\": oe_probe.probe_model,\n                    \"probe\": oe_probe.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n                probe_insertion_list.append(\n                    {\n                        **session_key,\n                        \"probe\": oe_probe.probe_SN,\n                        \"insertion_number\": probe_idx,\n                    }\n                )\n        else:\n            raise NotImplementedError(f\"Unknown acquisition software: {acq_software}\")\n\n        probe.Probe.insert(probe_list)\n        cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.ProbeInsertion.auto_generate_entries","title":"<code>auto_generate_entries(session_key)</code>  <code>classmethod</code>","text":"<p>Automatically populate entries in ProbeInsertion table for a session.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@classmethod\ndef auto_generate_entries(cls, session_key):\n\"\"\"Automatically populate entries in ProbeInsertion table for a session.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(session_key)\n    )\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = list(session_dir.rglob(ephys_pattern))\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found in: {session_dir}\"\n        )\n\n    probe_list, probe_insertion_list = [], []\n    if acq_software == \"SpikeGLX\":\n        for meta_fp_idx, meta_filepath in enumerate(ephys_meta_filepaths):\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n            probe_key = {\n                \"probe_type\": spikeglx_meta.probe_model,\n                \"probe\": spikeglx_meta.probe_SN,\n            }\n            if (\n                probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                and probe_key not in probe.Probe()\n            ):\n                probe_list.append(probe_key)\n\n            probe_dir = meta_filepath.parent\n            try:\n                probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                probe_number = int(probe_number.replace(\"imec\", \"\"))\n            except AttributeError:\n                probe_number = meta_fp_idx\n\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": spikeglx_meta.probe_SN,\n                    \"insertion_number\": int(probe_number),\n                }\n            )\n    elif acq_software == \"Open Ephys\":\n        loaded_oe = openephys.OpenEphys(session_dir)\n        for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n            probe_key = {\n                \"probe_type\": oe_probe.probe_model,\n                \"probe\": oe_probe.probe_SN,\n            }\n            if (\n                probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                and probe_key not in probe.Probe()\n            ):\n                probe_list.append(probe_key)\n            probe_insertion_list.append(\n                {\n                    **session_key,\n                    \"probe\": oe_probe.probe_SN,\n                    \"insertion_number\": probe_idx,\n                }\n            )\n    else:\n        raise NotImplementedError(f\"Unknown acquisition software: {acq_software}\")\n\n    probe.Probe.insert(probe_list)\n    cls.insert(probe_insertion_list, skip_duplicates=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics","title":"<code>QualityMetrics</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n        metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.Cluster","title":"<code>Cluster</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.QualityMetrics.make","title":"<code>make(key)</code>","text":"<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n    metrics_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet","title":"<code>WaveformSet</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if (kilosort_dir / \"mean_waveforms.npy\").exists():\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            if unit_peak_waveform:\n                self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n            if unit_electrode_waveforms:\n                self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.PeakWaveform","title":"<code>PeakWaveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.WaveformSet.make","title":"<code>make(key)</code>","text":"<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if (kilosort_dir / \"mean_waveforms.npy\").exists():\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        if unit_peak_waveform:\n            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n        if unit_electrode_waveforms:\n            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.activate","title":"<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>","text":"<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe scehma.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings. get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe scehma.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n        get_processed_data_dir(): Optional. Returns absolute path for processed data. Defaults to root directory.\n\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.generate_electrode_config","title":"<code>generate_electrode_config(probe_type, electrode_keys)</code>","text":"<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid({k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    root_directories = _linking_module.get_ephys_root_data_dir()\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        root_directories.append(_linking_module.get_processed_root_data_dir())\n\n    return root_directories\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_neuropixels_channel2electrode_map","title":"<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>","text":"<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        probe_dataset = get_openephys_probe_data(ephys_recording_key)\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_indices\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_openephys_probe_data","title":"<code>get_openephys_probe_data(ephys_recording_key)</code>","text":"<p>Get OpenEphys probe data from file.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_openephys_probe_data(ephys_recording_key: dict) -&gt; list:\n\"\"\"Get OpenEphys probe data from file.\"\"\"\n    inserted_probe_serial_number = (\n        ProbeInsertion * probe.Probe &amp; ephys_recording_key\n    ).fetch1(\"probe\")\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n    )\n    loaded_oe = openephys.OpenEphys(session_dir)\n    probe_data = loaded_oe.probes[inserted_probe_serial_number]\n\n    # explicitly garbage collect \"loaded_oe\"\n    # as these may have large memory footprint and may not be cleared fast enough\n    del loaded_oe\n    gc.collect()\n\n    return probe_data\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_processed_root_data_dir","title":"<code>get_processed_root_data_dir()</code>","text":"<p>Retrieve the root directory for all processed data.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string for the full path to the root directory for processed data.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_processed_root_data_dir() -&gt; str:\n\"\"\"Retrieve the root directory for all processed data.\n\n    Returns:\n        A string for the full path to the root directory for processed data.\n    \"\"\"\n\n    if hasattr(_linking_module, \"get_processed_root_data_dir\"):\n        return _linking_module.get_processed_root_data_dir()\n    else:\n        return get_ephys_root_data_dir()[0]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_recording_channels_details","title":"<code>get_recording_channels_details(ephys_recording_key)</code>","text":"<p>Get details of recording channels for a given recording.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_recording_channels_details(ephys_recording_key: dict) -&gt; np.array:\n\"\"\"Get details of recording channels for a given recording.\"\"\"\n    channels_details = {}\n\n    acq_software, sample_rate = (EphysRecording &amp; ephys_recording_key).fetch1(\n        \"acq_software\", \"sampling_rate\"\n    )\n\n    probe_type = (ProbeInsertion * probe.Probe &amp; ephys_recording_key).fetch1(\n        \"probe_type\"\n    )\n    channels_details[\"probe_type\"] = {\n        \"neuropixels 1.0 - 3A\": \"3A\",\n        \"neuropixels 1.0 - 3B\": \"NP1\",\n        \"neuropixels UHD\": \"NP1100\",\n        \"neuropixels 2.0 - SS\": \"NP21\",\n        \"neuropixels 2.0 - MS\": \"NP24\",\n    }[probe_type]\n\n    electrode_config_key = (\n        probe.ElectrodeConfig * EphysRecording &amp; ephys_recording_key\n    ).fetch1(\"KEY\")\n    (\n        channels_details[\"channel_ind\"],\n        channels_details[\"x_coords\"],\n        channels_details[\"y_coords\"],\n        channels_details[\"shank_ind\"],\n    ) = (\n        probe.ElectrodeConfig.Electrode * probe.ProbeType.Electrode\n        &amp; electrode_config_key\n    ).fetch(\n        \"electrode\", \"x_coord\", \"y_coord\", \"shank\"\n    )\n    channels_details[\"sample_rate\"] = sample_rate\n    channels_details[\"num_channels\"] = len(channels_details[\"channel_ind\"])\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        channels_details[\"uVPerBit\"] = spikeglx_recording.get_channel_bit_volts(\"ap\")[0]\n        channels_details[\"connected\"] = np.array(\n            [v for *_, v in spikeglx_recording.apmeta.shankmap[\"data\"]]\n        )\n    elif acq_software == \"Open Ephys\":\n        oe_probe = get_openephys_probe_data(ephys_recording_key)\n        channels_details[\"uVPerBit\"] = oe_probe.ap_meta[\"channels_gains\"][0]\n        channels_details[\"connected\"] = np.array(\n            [\n                int(v == 1)\n                for c, v in oe_probe.channels_connected.items()\n                if c in channels_details[\"channel_ind\"]\n            ]\n        )\n\n    return channels_details\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_no_curation/#element_array_ephys.ephys_no_curation.get_spikeglx_meta_filepath","title":"<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>","text":"<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_no_curation.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = pathlib.Path(\n        (\n            EphysRecording.EphysFile\n            &amp; ephys_recording_key\n            &amp; 'file_path LIKE \"%.ap.meta\"'\n        ).fetch1(\"file_path\")\n    )\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/","title":"ephys_precluster.py","text":""},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.AcquisitionSoftware","title":"<code>AcquisitionSoftware</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Name of software used for recording electrophysiological data.</p> <p>Attributes:</p> Name Type Description <code>acq_software</code> <code> varchar(24) </code> <p>Acquisition software, e.g,. SpikeGLX, OpenEphys</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass AcquisitionSoftware(dj.Lookup):\n\"\"\"Name of software used for recording electrophysiological data.\n\n    Attributes:\n        acq_software ( varchar(24) ): Acquisition software, e.g,. SpikeGLX, OpenEphys\n    \"\"\"\n\n    definition = \"\"\"  # Name of software used for recording of neuropixels probes - SpikeGLX or Open Ephys\n    acq_software: varchar(24)\n    \"\"\"\n    contents = zip([\"SpikeGLX\", \"Open Ephys\"])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusterQualityLabel","title":"<code>ClusterQualityLabel</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Quality label for each spike sorted cluster.</p> <p>Attributes:</p> Name Type Description <code>cluster_quality_label</code> <code>foreign key, varchar(100) </code> <p>Cluster quality type.</p> <code>cluster_quality_description</code> <code>varchar(4000)</code> <p>Description of the cluster quality type.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusterQualityLabel(dj.Lookup):\n\"\"\"Quality label for each spike sorted cluster.\n\n    Attributes:\n        cluster_quality_label (foreign key, varchar(100) ): Cluster quality type.\n        cluster_quality_description (varchar(4000) ): Description of the cluster quality type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Quality\n    cluster_quality_label:  varchar(100)\n    ---\n    cluster_quality_description:  varchar(4000)\n    \"\"\"\n    contents = [\n        (\"good\", \"single unit\"),\n        (\"ok\", \"probably a single unit, but could be contaminated\"),\n        (\"mua\", \"multi-unit activity\"),\n        (\"noise\", \"bad unit\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering","title":"<code>Clustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each clustering task.</p> <p>Attributes:</p> Name Type Description <code>ClusteringTask</code> <code>foreign key</code> <p>ClusteringTask primary key.</p> <code>clustering_time</code> <code>datetime</code> <p>Time when clustering results are generated.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for a clustering analysis.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass Clustering(dj.Imported):\n\"\"\"A processing table to handle each clustering task.\n\n    Attributes:\n        ClusteringTask (foreign key): ClusteringTask primary key.\n        clustering_time (datetime): Time when clustering results are generated.\n        package_version (varchar(16) ): Package version used for a clustering analysis.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering Procedure\n    -&gt; ClusteringTask\n    ---\n    clustering_time: datetime  # time of generation of this set of clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"load\":\n            _ = kilosort.Kilosort(\n                kilosort_dir\n            )  # check if the directory is a valid Kilosort output\n            creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n        elif task_mode == \"trigger\":\n            raise NotImplementedError(\n                \"Automatic triggering of\" \" clustering analysis is not yet supported\"\n            )\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Clustering.make","title":"<code>make(key)</code>","text":"<p>Triggers or imports clustering analysis.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Triggers or imports clustering analysis.\"\"\"\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"load\":\n        _ = kilosort.Kilosort(\n            kilosort_dir\n        )  # check if the directory is a valid Kilosort output\n        creation_time, _, _ = kilosort.extract_clustering_info(kilosort_dir)\n    elif task_mode == \"trigger\":\n        raise NotImplementedError(\n            \"Automatic triggering of\" \" clustering analysis is not yet supported\"\n        )\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1({**key, \"clustering_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringMethod","title":"<code>ClusteringMethod</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Kilosort clustering method.</p> <p>Attributes:</p> Name Type Description <code>clustering_method</code> <code>foreign key, varchar(16) </code> <p>Kilosort clustering method.</p> <code>clustering_methods_desc</code> <code>varchar(1000)</code> <p>Additional description of the clustering method.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusteringMethod(dj.Lookup):\n\"\"\"Kilosort clustering method.\n\n    Attributes:\n        clustering_method (foreign key, varchar(16) ): Kilosort clustering method.\n        clustering_methods_desc (varchar(1000) ): Additional description of the clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for clustering\n    clustering_method: varchar(16)\n    ---\n    clustering_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [\n        (\"kilosort\", \"kilosort clustering method\"),\n        (\"kilosort2\", \"kilosort2 clustering method\"),\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringParamSet","title":"<code>ClusteringParamSet</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters to be used in clustering procedure for spike sorting.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique ID for the clustering parameter set.</p> <code>ClusteringMethod</code> <code>dict</code> <p>ClusteringMethod primary key.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description of the clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>UUID hash for the parameter set.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusteringParamSet(dj.Lookup):\n\"\"\"Parameters to be used in clustering procedure for spike sorting.\n\n    Attributes:\n        paramset_idx (foreign key): Unique ID for the clustering parameter set.\n        ClusteringMethod (dict): ClusteringMethod primary key.\n        paramset_desc (varchar(128) ): Description of the clustering parameter set.\n        param_set_hash (uuid): UUID hash for the parameter set.\n        params (longblob)\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; ClusteringMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls, processing_method: str, paramset_idx: int, paramset_desc: str, params: dict\n    ):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n        Args:\n            clustering_method (str): name of the clustering method.\n            paramset_desc (str): description of the parameter set\n            params (dict): clustering parameters\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        param_dict = {\n            \"clustering_method\": processing_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set\"\n                    \" already exists - paramset_idx: {}\".format(existing_paramset_idx)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringParamSet.insert_new_params","title":"<code>insert_new_params(processing_method, paramset_idx, paramset_desc, params)</code>  <code>classmethod</code>","text":"<p>Inserts new parameters into the ClusteringParamSet table.</p> <p>Parameters:</p> Name Type Description Default <code>clustering_method</code> <code>str</code> <p>name of the clustering method.</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> required <code>params</code> <code>dict</code> <p>clustering parameters</p> required <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> required Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls, processing_method: str, paramset_idx: int, paramset_desc: str, params: dict\n):\n\"\"\"Inserts new parameters into the ClusteringParamSet table.\n\n    Args:\n        clustering_method (str): name of the clustering method.\n        paramset_desc (str): description of the parameter set\n        params (dict): clustering parameters\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    param_dict = {\n        \"clustering_method\": processing_method,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"param_set_hash\": dict_to_uuid(params),\n    }\n    param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                \"The specified param-set\"\n                \" already exists - paramset_idx: {}\".format(existing_paramset_idx)\n            )\n    else:\n        cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ClusteringTask","title":"<code>ClusteringTask</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>A clustering task to spike sort electrophysiology datasets.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>ClusteringParamSet</code> <code>foreign key</code> <p>ClusteringParamSet primary key.</p> <code>clustering_outdir_dir</code> <code>varchar(255)</code> <p>Relative path to output clustering results.</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes clustering or and <code>load</code> imports existing data.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ClusteringTask(dj.Manual):\n\"\"\"A clustering task to spike sort electrophysiology datasets.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        ClusteringParamSet (foreign key): ClusteringParamSet primary key.\n        clustering_outdir_dir (varchar (255) ): Relative path to output clustering results.\n        task_mode (enum): `Trigger` computes clustering or and `load` imports existing data.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; PreCluster\n    -&gt; ClusteringParamSet\n    ---\n    clustering_output_dir: varchar(255)  #  clustering output directory relative to the clustering root data directory\n    task_mode='load': enum('load', 'trigger')  # 'load': load computed analysis results, 'trigger': trigger computation\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering","title":"<code>CuratedClustering</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering results after curation.</p> <p>Attributes:</p> Name Type Description <code>Curation</code> <code>foreign key</code> <p>Curation primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass CuratedClustering(dj.Imported):\n\"\"\"Clustering results after curation.\n\n    Attributes:\n        Curation (foreign key): Curation primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clustering results of a curation.\n    -&gt; Curation\n    \"\"\"\n\n    class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n        Attributes:\n            CuratedClustering (foreign key): CuratedClustering primary key.\n            unit (foreign key, int): Unique integer identifying a single unit.\n            probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n            ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n            spike_count (int): Number of spikes in this recording for this unit.\n            spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n            spike_sites (longblob): Array of electrode associated with each spike.\n            spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n        \"\"\"\n\n        definition = \"\"\"\n        # Properties of a given unit from a round of clustering (and curation)\n        -&gt; master\n        unit: int\n        ---\n        -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n        -&gt; ClusterQualityLabel\n        spike_count: int         # how many spikes in this recording for this unit\n        spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n        spike_sites : longblob   # array of electrode associated with each spike\n        spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n        acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n\n        # ---------- Unit ----------\n        # -- Remove 0-spike units\n        withspike_idx = [\n            i\n            for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n            if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n        ]\n        valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n        valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n        # -- Get channel and electrode-site mapping\n        channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n        # -- Spike-times --\n        # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n        spike_time_key = (\n            \"spike_times_sec_adj\"\n            if \"spike_times_sec_adj\" in kilosort_dataset.data\n            else \"spike_times_sec\"\n            if \"spike_times_sec\" in kilosort_dataset.data\n            else \"spike_times\"\n        )\n        spike_times = kilosort_dataset.data[spike_time_key]\n        kilosort_dataset.extract_spike_depths()\n\n        # -- Spike-sites and Spike-depths --\n        spike_sites = np.array(\n            [\n                channel2electrodes[s][\"electrode\"]\n                for s in kilosort_dataset.data[\"spike_sites\"]\n            ]\n        )\n        spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n        # -- Insert unit, label, peak-chn\n        units = []\n        for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n            if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n                unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n                unit_spike_times = (\n                    spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                    / kilosort_dataset.data[\"params\"][\"sample_rate\"]\n                )\n                spike_count = len(unit_spike_times)\n\n                units.append(\n                    {\n                        \"unit\": unit,\n                        \"cluster_quality_label\": unit_lbl,\n                        **channel2electrodes[unit_channel],\n                        \"spike_times\": unit_spike_times,\n                        \"spike_count\": spike_count,\n                        \"spike_sites\": spike_sites[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ],\n                        \"spike_depths\": spike_depths[\n                            kilosort_dataset.data[\"spike_clusters\"] == unit\n                        ]\n                        if spike_depths is not None\n                        else None,\n                    }\n                )\n\n        self.insert1(key)\n        self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering.Unit","title":"<code>Unit</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Single unit properties after clustering and curation.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> <code>unit</code> <code>foreign key, int</code> <p>Unique integer identifying a single unit.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>dict</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>ClusteringQualityLabel</code> <code>dict</code> <p>CLusteringQualityLabel primary key.</p> <code>spike_count</code> <code>int</code> <p>Number of spikes in this recording for this unit.</p> <code>spike_times</code> <code>longblob</code> <p>Spike times of this unit, relative to start time of EphysRecording.</p> <code>spike_sites</code> <code>longblob</code> <p>Array of electrode associated with each spike.</p> <code>spike_depths</code> <code>longblob</code> <p>Array of depths associated with each spike, relative to each spike.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Unit(dj.Part):\n\"\"\"Single unit properties after clustering and curation.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n        unit (foreign key, int): Unique integer identifying a single unit.\n        probe.ElectrodeConfig.Electrode (dict): probe.ElectrodeConfig.Electrode primary key.\n        ClusteringQualityLabel (dict): CLusteringQualityLabel primary key.\n        spike_count (int): Number of spikes in this recording for this unit.\n        spike_times (longblob): Spike times of this unit, relative to start time of EphysRecording.\n        spike_sites (longblob): Array of electrode associated with each spike.\n        spike_depths (longblob): Array of depths associated with each spike, relative to each spike.\n    \"\"\"\n\n    definition = \"\"\"\n    # Properties of a given unit from a round of clustering (and curation)\n    -&gt; master\n    unit: int\n    ---\n    -&gt; probe.ElectrodeConfig.Electrode  # electrode with highest waveform amplitude for this unit\n    -&gt; ClusterQualityLabel\n    spike_count: int         # how many spikes in this recording for this unit\n    spike_times: longblob    # (s) spike times of this unit, relative to the start of the EphysRecording\n    spike_sites : longblob   # array of electrode associated with each spike\n    spike_depths=null : longblob  # (um) array of depths associated with each spike, relative to the (0, 0) of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.CuratedClustering.make","title":"<code>make(key)</code>","text":"<p>Automated population of Unit information.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Automated population of Unit information.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n    acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n\n    # ---------- Unit ----------\n    # -- Remove 0-spike units\n    withspike_idx = [\n        i\n        for i, u in enumerate(kilosort_dataset.data[\"cluster_ids\"])\n        if (kilosort_dataset.data[\"spike_clusters\"] == u).any()\n    ]\n    valid_units = kilosort_dataset.data[\"cluster_ids\"][withspike_idx]\n    valid_unit_labels = kilosort_dataset.data[\"cluster_groups\"][withspike_idx]\n    # -- Get channel and electrode-site mapping\n    channel2electrodes = get_neuropixels_channel2electrode_map(key, acq_software)\n\n    # -- Spike-times --\n    # spike_times_sec_adj &gt; spike_times_sec &gt; spike_times\n    spike_time_key = (\n        \"spike_times_sec_adj\"\n        if \"spike_times_sec_adj\" in kilosort_dataset.data\n        else \"spike_times_sec\"\n        if \"spike_times_sec\" in kilosort_dataset.data\n        else \"spike_times\"\n    )\n    spike_times = kilosort_dataset.data[spike_time_key]\n    kilosort_dataset.extract_spike_depths()\n\n    # -- Spike-sites and Spike-depths --\n    spike_sites = np.array(\n        [\n            channel2electrodes[s][\"electrode\"]\n            for s in kilosort_dataset.data[\"spike_sites\"]\n        ]\n    )\n    spike_depths = kilosort_dataset.data[\"spike_depths\"]\n\n    # -- Insert unit, label, peak-chn\n    units = []\n    for unit, unit_lbl in zip(valid_units, valid_unit_labels):\n        if (kilosort_dataset.data[\"spike_clusters\"] == unit).any():\n            unit_channel, _ = kilosort_dataset.get_best_channel(unit)\n            unit_spike_times = (\n                spike_times[kilosort_dataset.data[\"spike_clusters\"] == unit]\n                / kilosort_dataset.data[\"params\"][\"sample_rate\"]\n            )\n            spike_count = len(unit_spike_times)\n\n            units.append(\n                {\n                    \"unit\": unit,\n                    \"cluster_quality_label\": unit_lbl,\n                    **channel2electrodes[unit_channel],\n                    \"spike_times\": unit_spike_times,\n                    \"spike_count\": spike_count,\n                    \"spike_sites\": spike_sites[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ],\n                    \"spike_depths\": spike_depths[\n                        kilosort_dataset.data[\"spike_clusters\"] == unit\n                    ]\n                    if spike_depths is not None\n                    else None,\n                }\n            )\n\n    self.insert1(key)\n    self.Unit.insert([{**key, **u} for u in units])\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation","title":"<code>Curation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Curation procedure table.</p> <p>Attributes:</p> Name Type Description <code>Clustering</code> <code>foreign key</code> <p>Clustering primary key.</p> <code>curation_id</code> <code>foreign key, int</code> <p>Unique curation ID.</p> <code>curation_time</code> <code>datetime</code> <p>Time when curation results are generated.</p> <code>curation_output_dir</code> <code>varchar(255)</code> <p>Output directory of the curated results.</p> <code>quality_control</code> <code>bool</code> <p>If True, this clustering result has undergone quality control.</p> <code>manual_curation</code> <code>bool</code> <p>If True, manual curation has been performed on this clustering result.</p> <code>curation_note</code> <code>varchar(2000)</code> <p>Notes about the curation task.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass Curation(dj.Manual):\n\"\"\"Curation procedure table.\n\n    Attributes:\n        Clustering (foreign key): Clustering primary key.\n        curation_id (foreign key, int): Unique curation ID.\n        curation_time (datetime): Time when curation results are generated.\n        curation_output_dir (varchar(255) ): Output directory of the curated results.\n        quality_control (bool): If True, this clustering result has undergone quality control.\n        manual_curation (bool): If True, manual curation has been performed on this clustering result.\n        curation_note (varchar(2000) ): Notes about the curation task.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual curation procedure\n    -&gt; Clustering\n    curation_id: int\n    ---\n    curation_time: datetime             # time of generation of this set of curated clustering results\n    curation_output_dir: varchar(255)   # output directory of the curated results, relative to root data directory\n    quality_control: bool               # has this clustering result undergone quality control?\n    manual_curation: bool               # has manual curation been performed on this clustering result?\n    curation_note='': varchar(2000)\n    \"\"\"\n\n    def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n        A function to create a new corresponding \"Curation\" for a particular\n        \"ClusteringTask\"\n        \"\"\"\n        if key not in Clustering():\n            raise ValueError(\n                f\"No corresponding entry in Clustering available\"\n                f\" for: {key}; do `Clustering.populate(key)`\"\n            )\n\n        task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n            \"task_mode\", \"clustering_output_dir\"\n        )\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n            kilosort_dir\n        )\n        # Synthesize curation_id\n        curation_id = (\n            dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n        )\n        self.insert1(\n            {\n                **key,\n                \"curation_id\": curation_id,\n                \"curation_time\": creation_time,\n                \"curation_output_dir\": output_dir,\n                \"quality_control\": is_qc,\n                \"manual_curation\": is_curated,\n                \"curation_note\": curation_note,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.Curation.create1_from_clustering_task","title":"<code>create1_from_clustering_task(key, curation_note='')</code>","text":"<p>A function to create a new corresponding \"Curation\" for a particular \"ClusteringTask\"</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def create1_from_clustering_task(self, key, curation_note: str = \"\"):\n\"\"\"\n    A function to create a new corresponding \"Curation\" for a particular\n    \"ClusteringTask\"\n    \"\"\"\n    if key not in Clustering():\n        raise ValueError(\n            f\"No corresponding entry in Clustering available\"\n            f\" for: {key}; do `Clustering.populate(key)`\"\n        )\n\n    task_mode, output_dir = (ClusteringTask &amp; key).fetch1(\n        \"task_mode\", \"clustering_output_dir\"\n    )\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    creation_time, is_curated, is_qc = kilosort.extract_clustering_info(\n        kilosort_dir\n    )\n    # Synthesize curation_id\n    curation_id = (\n        dj.U().aggr(self &amp; key, n=\"ifnull(max(curation_id)+1,1)\").fetch1(\"n\")\n    )\n    self.insert1(\n        {\n            **key,\n            \"curation_id\": curation_id,\n            \"curation_time\": creation_time,\n            \"curation_output_dir\": output_dir,\n            \"quality_control\": is_qc,\n            \"manual_curation\": is_curated,\n            \"curation_note\": curation_note,\n        }\n    )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording","title":"<code>EphysRecording</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Automated table with electrophysiology recording information for each probe inserted during an experimental session.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>probe.ElectrodeConfig</code> <code>dict</code> <p>probe.ElectrodeConfig primary key.</p> <code>AcquisitionSoftware</code> <code>dict</code> <p>AcquisitionSoftware primary key.</p> <code>sampling_rate</code> <code>float</code> <p>sampling rate of the recording in Hertz (Hz).</p> <code>recording_datetime</code> <code>datetime</code> <p>datetime of the recording from this probe.</p> <code>recording_duration</code> <code>float</code> <p>duration of the entire recording from this probe in seconds.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass EphysRecording(dj.Imported):\n\"\"\"Automated table with electrophysiology recording information for each probe inserted during an experimental session.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        probe.ElectrodeConfig (dict): probe.ElectrodeConfig primary key.\n        AcquisitionSoftware (dict): AcquisitionSoftware primary key.\n        sampling_rate (float): sampling rate of the recording in Hertz (Hz).\n        recording_datetime (datetime): datetime of the recording from this probe.\n        recording_duration (float): duration of the entire recording from this probe in seconds.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ephys recording from a probe insertion for a given session.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; probe.ElectrodeConfig\n    -&gt; AcquisitionSoftware\n    sampling_rate: float # (Hz)\n    recording_datetime: datetime # datetime of the recording from this probe\n    recording_duration: float # (seconds) duration of the recording from this probe\n    \"\"\"\n\n    class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n        Attributes:\n            EphysRecording (foreign key): EphysRecording primary key.\n            file_path (varchar(255) ): relative file path for electrophysiology recording.\n        \"\"\"\n\n        definition = \"\"\"\n        # Paths of files of a given EphysRecording round.\n        -&gt; master\n        file_path: varchar(255)  # filepath relative to root data directory\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in (\n            (\"*.ap.meta\", \"SpikeGLX\"),\n            (\"*.oebin\", \"Open Ephys\"),\n        ):\n            ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n            if ephys_meta_filepaths:\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                f\"Ephys recording data not found!\"\n                f\" Neither SpikeGLX nor Open Ephys recording files found\"\n                f\" in {session_dir}\"\n            )\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n\n            if re.search(\"(1.0|2.0)\", spikeglx_meta.probe_model):\n                probe_type = spikeglx_meta.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    (shank, shank_col, shank_row): key\n                    for key, shank, shank_col, shank_row in zip(\n                        *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                    )\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[(shank, shank_col, shank_row)]\n                    for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels probe model\"\n                    \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                    \"recording_datetime\": spikeglx_meta.recording_time,\n                    \"recording_duration\": (\n                        spikeglx_meta.recording_duration\n                        or spikeglx.retrieve_recording_duration(meta_filepath)\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n            self.EphysFile.insert1(\n                {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n            )\n        elif acq_software == \"Open Ephys\":\n            dataset = openephys.OpenEphys(session_dir)\n            for serial_number, probe_data in dataset.probes.items():\n                if str(serial_number) == inserted_probe_serial_number:\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No Open Ephys data found for probe insertion: {}\".format(key)\n                )\n\n            if re.search(\"(1.0|2.0)\", probe_data.probe_model):\n                probe_type = probe_data.probe_model\n                electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n                probe_electrodes = {\n                    key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n                }\n\n                electrode_group_members = [\n                    probe_electrodes[channel_idx]\n                    for channel_idx in probe_data.ap_meta[\"channels_ids\"]\n                ]\n            else:\n                raise NotImplementedError(\n                    \"Processing for neuropixels\"\n                    \" probe model {} not yet implemented\".format(probe_data.probe_model)\n                )\n\n            self.insert1(\n                {\n                    **key,\n                    **generate_electrode_config(probe_type, electrode_group_members),\n                    \"acq_software\": acq_software,\n                    \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                    \"recording_datetime\": probe_data.recording_info[\n                        \"recording_datetimes\"\n                    ][0],\n                    \"recording_duration\": np.sum(\n                        probe_data.recording_info[\"recording_durations\"]\n                    ),\n                }\n            )\n\n            root_dir = find_root_directory(\n                get_ephys_root_data_dir(),\n                probe_data.recording_info[\"recording_files\"][0],\n            )\n            self.EphysFile.insert(\n                [\n                    {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                    for fp in probe_data.recording_info[\"recording_files\"]\n                ]\n            )\n        else:\n            raise NotImplementedError(\n                f\"Processing ephys files from\"\n                f\" acquisition software of type {acq_software} is\"\n                f\" not yet implemented\"\n            )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording.EphysFile","title":"<code>EphysFile</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Paths of electrophysiology recording files for each insertion.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>file_path</code> <code>varchar(255)</code> <p>relative file path for electrophysiology recording.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class EphysFile(dj.Part):\n\"\"\"Paths of electrophysiology recording files for each insertion.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        file_path (varchar(255) ): relative file path for electrophysiology recording.\n    \"\"\"\n\n    definition = \"\"\"\n    # Paths of files of a given EphysRecording round.\n    -&gt; master\n    file_path: varchar(255)  # filepath relative to root data directory\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.EphysRecording.make","title":"<code>make(key)</code>","text":"<p>Populates table with electrophysiology recording information.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates table with electrophysiology recording information.\"\"\"\n    session_dir = find_full_path(\n        get_ephys_root_data_dir(), get_session_directory(key)\n    )\n\n    inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n        \"probe\"\n    )\n\n    # search session dir and determine acquisition software\n    for ephys_pattern, ephys_acq_type in (\n        (\"*.ap.meta\", \"SpikeGLX\"),\n        (\"*.oebin\", \"Open Ephys\"),\n    ):\n        ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n        if ephys_meta_filepaths:\n            acq_software = ephys_acq_type\n            break\n    else:\n        raise FileNotFoundError(\n            f\"Ephys recording data not found!\"\n            f\" Neither SpikeGLX nor Open Ephys recording files found\"\n            f\" in {session_dir}\"\n        )\n\n    if acq_software == \"SpikeGLX\":\n        for meta_filepath in ephys_meta_filepaths:\n            spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n            if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No SpikeGLX data found for probe insertion: {}\".format(key)\n            )\n\n        if re.search(\"(1.0|2.0)\", spikeglx_meta.probe_model):\n            probe_type = spikeglx_meta.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            electrode_group_members = [\n                probe_electrodes[(shank, shank_col, shank_row)]\n                for shank, shank_col, shank_row, _ in spikeglx_meta.shankmap[\"data\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels probe model\"\n                \" {} not yet implemented\".format(spikeglx_meta.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": spikeglx_meta.meta[\"imSampRate\"],\n                \"recording_datetime\": spikeglx_meta.recording_time,\n                \"recording_duration\": (\n                    spikeglx_meta.recording_duration\n                    or spikeglx.retrieve_recording_duration(meta_filepath)\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(get_ephys_root_data_dir(), meta_filepath)\n        self.EphysFile.insert1(\n            {**key, \"file_path\": meta_filepath.relative_to(root_dir).as_posix()}\n        )\n    elif acq_software == \"Open Ephys\":\n        dataset = openephys.OpenEphys(session_dir)\n        for serial_number, probe_data in dataset.probes.items():\n            if str(serial_number) == inserted_probe_serial_number:\n                break\n        else:\n            raise FileNotFoundError(\n                \"No Open Ephys data found for probe insertion: {}\".format(key)\n            )\n\n        if re.search(\"(1.0|2.0)\", probe_data.probe_model):\n            probe_type = probe_data.probe_model\n            electrode_query = probe.ProbeType.Electrode &amp; {\"probe_type\": probe_type}\n\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            electrode_group_members = [\n                probe_electrodes[channel_idx]\n                for channel_idx in probe_data.ap_meta[\"channels_ids\"]\n            ]\n        else:\n            raise NotImplementedError(\n                \"Processing for neuropixels\"\n                \" probe model {} not yet implemented\".format(probe_data.probe_model)\n            )\n\n        self.insert1(\n            {\n                **key,\n                **generate_electrode_config(probe_type, electrode_group_members),\n                \"acq_software\": acq_software,\n                \"sampling_rate\": probe_data.ap_meta[\"sample_rate\"],\n                \"recording_datetime\": probe_data.recording_info[\n                    \"recording_datetimes\"\n                ][0],\n                \"recording_duration\": np.sum(\n                    probe_data.recording_info[\"recording_durations\"]\n                ),\n            }\n        )\n\n        root_dir = find_root_directory(\n            get_ephys_root_data_dir(),\n            probe_data.recording_info[\"recording_files\"][0],\n        )\n        self.EphysFile.insert(\n            [\n                {**key, \"file_path\": fp.relative_to(root_dir).as_posix()}\n                for fp in probe_data.recording_info[\"recording_files\"]\n            ]\n        )\n    else:\n        raise NotImplementedError(\n            f\"Processing ephys files from\"\n            f\" acquisition software of type {acq_software} is\"\n            f\" not yet implemented\"\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.InsertionLocation","title":"<code>InsertionLocation</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Stereotaxic location information for each probe insertion.</p> <p>Attributes:</p> Name Type Description <code>ProbeInsertion</code> <code>foreign key</code> <p>ProbeInsertion primary key.</p> <code>SkullReference</code> <code>dict</code> <p>SkullReference primary key.</p> <code>ap_location</code> <code>decimal(6, 2)</code> <p>Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.</p> <code>ml_location</code> <code>decimal(6, 2)</code> <p>Medial-lateral location in micrometers. Reference is zero with right side values positive.</p> <code>depth</code> <code>decimal(6, 2)</code> <p>Manipulator depth relative to the surface of the brain at zero. Ventral is negative.</p> <code>Theta</code> <code>decimal(5, 2)</code> <p>elevation - rotation about the ml-axis in degrees relative to positive z-axis.</p> <code>phi</code> <code>decimal(5, 2)</code> <p>azimuth - rotation about the dv-axis in degrees relative to the positive x-axis</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass InsertionLocation(dj.Manual):\n\"\"\"Stereotaxic location information for each probe insertion.\n\n    Attributes:\n        ProbeInsertion (foreign key): ProbeInsertion primary key.\n        SkullReference (dict): SkullReference primary key.\n        ap_location (decimal (6, 2) ): Anterior-posterior location in micrometers. Reference is 0 with anterior values positive.\n        ml_location (decimal (6, 2) ): Medial-lateral location in micrometers. Reference is zero with right side values positive.\n        depth (decimal (6, 2) ): Manipulator depth relative to the surface of the brain at zero. Ventral is negative.\n        Theta (decimal (5, 2) ): elevation - rotation about the ml-axis in degrees relative to positive z-axis.\n        phi (decimal (5, 2) ): azimuth - rotation about the dv-axis in degrees relative to the positive x-axis\n\n    \"\"\"\n\n    definition = \"\"\"\n    # Brain Location of a given probe insertion.\n    -&gt; ProbeInsertion\n    ---\n    -&gt; SkullReference\n    ap_location: decimal(6, 2) # (um) anterior-posterior; ref is 0; more anterior is more positive\n    ml_location: decimal(6, 2) # (um) medial axis; ref is 0 ; more right is more positive\n    depth:       decimal(6, 2) # (um) manipulator depth relative to surface of the brain (0); more ventral is more negative\n    theta=null:  decimal(5, 2) # (deg) - elevation - rotation about the ml-axis [0, 180] - w.r.t the z+ axis\n    phi=null:    decimal(5, 2) # (deg) - azimuth - rotation about the dv-axis [0, 360] - w.r.t the x+ axis\n    beta=null:   decimal(5, 2) # (deg) rotation about the shank of the probe [-180, 180] - clockwise is increasing in degree - 0 is the probe-front facing anterior\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP","title":"<code>LFP</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Extracts local field potentials (LFP) from an electrophysiology recording.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>lfp_sampling_rate</code> <code>float</code> <p>Sampling rate for LFPs in Hz.</p> <code>lfp_time_stamps</code> <code>longblob</code> <p>Time stamps with respect to the start of the recording.</p> <code>lfp_mean</code> <code>longblob</code> <p>Overall mean LFP across electrodes.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass LFP(dj.Imported):\n\"\"\"Extracts local field potentials (LFP) from an electrophysiology recording.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        lfp_sampling_rate (float): Sampling rate for LFPs in Hz.\n        lfp_time_stamps (longblob): Time stamps with respect to the start of the recording.\n        lfp_mean (longblob): Overall mean LFP across electrodes.\n    \"\"\"\n\n    definition = \"\"\"\n    # Acquired local field potential (LFP) from a given Ephys recording.\n    -&gt; PreCluster\n    ---\n    lfp_sampling_rate: float   # (Hz)\n    lfp_time_stamps: longblob  # (s) timestamps with respect to the start of the recording (recording_timestamp)\n    lfp_mean: longblob         # (uV) mean of LFP across electrodes - shape (time,)\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n        Attributes:\n            LFP (foreign key): LFP primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            lfp (longblob): LFP recording at this electrode in microvolts.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        lfp: longblob               # (uV) recorded lfp at this electrode\n        \"\"\"\n\n    # Only store LFP for every 9th channel, due to high channel density,\n    # close-by channels exhibit highly similar LFP\n    _skip_channel_counts = 9\n\n    def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n        acq_software, probe_sn = (EphysRecording * ProbeInsertion &amp; key).fetch1(\n            \"acq_software\", \"probe\"\n        )\n\n        electrode_keys, lfp = [], []\n\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n            lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            # Extract LFP data at specified channels and convert to uV\n            lfp = spikeglx_recording.lf_timeseries[\n                :, lfp_channel_ind\n            ]  # (sample x channel)\n            lfp = (\n                lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n            ).T  # (channel x sample)\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                    lfp_time_stamps=(\n                        np.arange(lfp.shape[1])\n                        / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                    ),\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                (shank, shank_col, shank_row): key\n                for key, shank, shank_col, shank_row in zip(\n                    *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n                )\n            }\n\n            for recorded_site in lfp_channel_ind:\n                shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                    \"data\"\n                ][recorded_site]\n                electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n        elif acq_software == \"Open Ephys\":\n\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n\n            loaded_oe = openephys.OpenEphys(session_dir)\n            oe_probe = loaded_oe.probes[probe_sn]\n\n            lfp_channel_ind = np.arange(len(oe_probe.lfp_meta[\"channels_ids\"]))[\n                -1 :: -self._skip_channel_counts\n            ]\n\n            lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n            lfp = (\n                lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n            ).T  # (channel x sample)\n            lfp_timestamps = oe_probe.lfp_timestamps\n\n            self.insert1(\n                dict(\n                    key,\n                    lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                    lfp_time_stamps=lfp_timestamps,\n                    lfp_mean=lfp.mean(axis=0),\n                )\n            )\n\n            electrode_query = (\n                probe.ProbeType.Electrode\n                * probe.ElectrodeConfig.Electrode\n                * EphysRecording\n                &amp; key\n            )\n            probe_electrodes = {\n                key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n            }\n\n            for channel_idx in np.array(oe_probe.lfp_meta[\"channels_ids\"])[\n                lfp_channel_ind\n            ]:\n                electrode_keys.append(probe_electrodes[channel_idx])\n        else:\n            raise NotImplementedError(\n                f\"LFP extraction from acquisition software\"\n                f\" of type {acq_software} is not yet implemented\"\n            )\n\n        # single insert in loop to mitigate potential memory issue\n        for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n            self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP.Electrode","title":"<code>Electrode</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Saves local field potential data for each electrode.</p> <p>Attributes:</p> Name Type Description <code>LFP</code> <code>foreign key</code> <p>LFP primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>lfp</code> <code>longblob</code> <p>LFP recording at this electrode in microvolts.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Saves local field potential data for each electrode.\n\n    Attributes:\n        LFP (foreign key): LFP primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        lfp (longblob): LFP recording at this electrode in microvolts.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    lfp: longblob               # (uV) recorded lfp at this electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.LFP.make","title":"<code>make(key)</code>","text":"<p>Populates the LFP tables.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates the LFP tables.\"\"\"\n    acq_software, probe_sn = (EphysRecording * ProbeInsertion &amp; key).fetch1(\n        \"acq_software\", \"probe\"\n    )\n\n    electrode_keys, lfp = [], []\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n        spikeglx_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n\n        lfp_channel_ind = spikeglx_recording.lfmeta.recording_channels[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        # Extract LFP data at specified channels and convert to uV\n        lfp = spikeglx_recording.lf_timeseries[\n            :, lfp_channel_ind\n        ]  # (sample x channel)\n        lfp = (\n            lfp * spikeglx_recording.get_channel_bit_volts(\"lf\")[lfp_channel_ind]\n        ).T  # (channel x sample)\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=spikeglx_recording.lfmeta.meta[\"imSampRate\"],\n                lfp_time_stamps=(\n                    np.arange(lfp.shape[1])\n                    / spikeglx_recording.lfmeta.meta[\"imSampRate\"]\n                ),\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        for recorded_site in lfp_channel_ind:\n            shank, shank_col, shank_row, _ = spikeglx_recording.apmeta.shankmap[\n                \"data\"\n            ][recorded_site]\n            electrode_keys.append(probe_electrodes[(shank, shank_col, shank_row)])\n    elif acq_software == \"Open Ephys\":\n\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(key)\n        )\n\n        loaded_oe = openephys.OpenEphys(session_dir)\n        oe_probe = loaded_oe.probes[probe_sn]\n\n        lfp_channel_ind = np.arange(len(oe_probe.lfp_meta[\"channels_ids\"]))[\n            -1 :: -self._skip_channel_counts\n        ]\n\n        lfp = oe_probe.lfp_timeseries[:, lfp_channel_ind]  # (sample x channel)\n        lfp = (\n            lfp * np.array(oe_probe.lfp_meta[\"channels_gains\"])[lfp_channel_ind]\n        ).T  # (channel x sample)\n        lfp_timestamps = oe_probe.lfp_timestamps\n\n        self.insert1(\n            dict(\n                key,\n                lfp_sampling_rate=oe_probe.lfp_meta[\"sample_rate\"],\n                lfp_time_stamps=lfp_timestamps,\n                lfp_mean=lfp.mean(axis=0),\n            )\n        )\n\n        electrode_query = (\n            probe.ProbeType.Electrode\n            * probe.ElectrodeConfig.Electrode\n            * EphysRecording\n            &amp; key\n        )\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        for channel_idx in np.array(oe_probe.lfp_meta[\"channels_ids\"])[\n            lfp_channel_ind\n        ]:\n            electrode_keys.append(probe_electrodes[channel_idx])\n    else:\n        raise NotImplementedError(\n            f\"LFP extraction from acquisition software\"\n            f\" of type {acq_software} is not yet implemented\"\n        )\n\n    # single insert in loop to mitigate potential memory issue\n    for electrode_key, lfp_trace in zip(electrode_keys, lfp):\n        self.Electrode.insert1({**key, **electrode_key, \"lfp\": lfp_trace})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster","title":"<code>PreCluster</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A processing table to handle each PreClusterTask:</p> <p>Attributes:</p> Name Type Description <code>PreClusterTask</code> <code>foreign key</code> <p>PreClusterTask primary key.</p> <code>precluster_time</code> <code>datetime</code> <p>Time of generation of this set of pre-clustering results.</p> <code>package_version</code> <code>varchar(16)</code> <p>Package version used for performing pre-clustering.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreCluster(dj.Imported):\n\"\"\"\n    A processing table to handle each PreClusterTask:\n\n    Attributes:\n        PreClusterTask (foreign key): PreClusterTask primary key.\n        precluster_time (datetime): Time of generation of this set of pre-clustering results.\n        package_version (varchar(16) ): Package version used for performing pre-clustering.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; PreClusterTask\n    ---\n    precluster_time: datetime  # time of generation of this set of pre-clustering results\n    package_version='': varchar(16)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Populate pre-clustering tables.\"\"\"\n        task_mode, output_dir = (PreClusterTask &amp; key).fetch1(\n            \"task_mode\", \"precluster_output_dir\"\n        )\n        precluster_output_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        if task_mode == \"none\":\n            if len((PreClusterParamSteps.Step &amp; key).fetch()) &gt; 0:\n                raise ValueError(\n                    \"There are entries in the PreClusterParamSteps.Step \"\n                    \"table and task_mode=none\"\n                )\n            creation_time = (EphysRecording &amp; key).fetch1(\"recording_datetime\")\n        elif task_mode == \"load\":\n            acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n            inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n                \"probe\"\n            )\n\n            if acq_software == \"SpikeGLX\":\n                for meta_filepath in precluster_output_dir.rglob(\"*.ap.meta\"):\n                    spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                    if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                        creation_time = spikeglx_meta.recording_time\n                        break\n                else:\n                    raise FileNotFoundError(\n                        \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                    )\n            else:\n                raise NotImplementedError(\n                    f\"Pre-clustering analysis of {acq_software}\" \"is not yet supported.\"\n                )\n        elif task_mode == \"trigger\":\n            raise NotImplementedError(\n                \"Automatic triggering of\"\n                \" pre-clustering analysis is not yet supported.\"\n            )\n        else:\n            raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n        self.insert1({**key, \"precluster_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreCluster.make","title":"<code>make(key)</code>","text":"<p>Populate pre-clustering tables.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populate pre-clustering tables.\"\"\"\n    task_mode, output_dir = (PreClusterTask &amp; key).fetch1(\n        \"task_mode\", \"precluster_output_dir\"\n    )\n    precluster_output_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    if task_mode == \"none\":\n        if len((PreClusterParamSteps.Step &amp; key).fetch()) &gt; 0:\n            raise ValueError(\n                \"There are entries in the PreClusterParamSteps.Step \"\n                \"table and task_mode=none\"\n            )\n        creation_time = (EphysRecording &amp; key).fetch1(\"recording_datetime\")\n    elif task_mode == \"load\":\n        acq_software = (EphysRecording &amp; key).fetch1(\"acq_software\")\n        inserted_probe_serial_number = (ProbeInsertion * probe.Probe &amp; key).fetch1(\n            \"probe\"\n        )\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in precluster_output_dir.rglob(\"*.ap.meta\"):\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    creation_time = spikeglx_meta.recording_time\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(key)\n                )\n        else:\n            raise NotImplementedError(\n                f\"Pre-clustering analysis of {acq_software}\" \"is not yet supported.\"\n            )\n    elif task_mode == \"trigger\":\n        raise NotImplementedError(\n            \"Automatic triggering of\"\n            \" pre-clustering analysis is not yet supported.\"\n        )\n    else:\n        raise ValueError(f\"Unknown task mode: {task_mode}\")\n\n    self.insert1({**key, \"precluster_time\": creation_time})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterMethod","title":"<code>PreClusterMethod</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Pre-clustering method</p> <p>Attributes:</p> Name Type Description <code>precluster_method</code> <code>foreign key, varchar(16) </code> <p>Pre-clustering method for the dataset.</p> <code>precluster_method_desc(varchar(1000)</code> <p>Pre-clustering method description.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterMethod(dj.Lookup):\n\"\"\"Pre-clustering method\n\n    Attributes:\n        precluster_method (foreign key, varchar(16) ): Pre-clustering method for the dataset.\n        precluster_method_desc(varchar(1000) ): Pre-clustering method description.\n    \"\"\"\n\n    definition = \"\"\"\n    # Method for pre-clustering\n    precluster_method: varchar(16)\n    ---\n    precluster_method_desc: varchar(1000)\n    \"\"\"\n\n    contents = [(\"catgt\", \"Time shift, Common average referencing, Zeroing\")]\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSet","title":"<code>PreClusterParamSet</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Parameters for the pre-clustering method.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>foreign key</code> <p>Unique parameter set ID.</p> <code>PreClusterMethod</code> <code>dict</code> <p>PreClusterMethod query for this dataset.</p> <code>paramset_desc</code> <code>varchar(128)</code> <p>Description for the pre-clustering parameter set.</p> <code>param_set_hash</code> <code>uuid</code> <p>Unique hash for parameter set.</p> <code>params</code> <code>longblob</code> <p>All parameters for the pre-clustering method.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterParamSet(dj.Lookup):\n\"\"\"Parameters for the pre-clustering method.\n\n    Attributes:\n        paramset_idx (foreign key): Unique parameter set ID.\n        PreClusterMethod (dict): PreClusterMethod query for this dataset.\n        paramset_desc (varchar(128) ): Description for the pre-clustering parameter set.\n        param_set_hash (uuid): Unique hash for parameter set.\n        params (longblob): All parameters for the pre-clustering method.\n    \"\"\"\n\n    definition = \"\"\"\n    # Parameter set to be used in a clustering procedure\n    paramset_idx:  smallint\n    ---\n    -&gt; PreClusterMethod\n    paramset_desc: varchar(128)\n    param_set_hash: uuid\n    unique index (param_set_hash)\n    params: longblob  # dictionary of all applicable parameters\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls, precluster_method: str, paramset_idx: int, paramset_desc: str, params: dict\n    ):\n        param_dict = {\n            \"precluster_method\": precluster_method,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"param_set_hash\": dict_to_uuid(params),\n        }\n        param_query = cls &amp; {\"param_set_hash\": param_dict[\"param_set_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    \"The specified param-set\"\n                    \" already exists - paramset_idx: {}\".format(existing_paramset_idx)\n                )\n        else:\n            cls.insert1(param_dict)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSteps","title":"<code>PreClusterParamSteps</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Ordered list of parameter sets that will be run.</p> <p>Attributes:</p> Name Type Description <code>precluster_param_steps_id</code> <code>foreign key</code> <p>Unique ID for the pre-clustering parameter sets to be run.</p> <code>precluster_param_steps_name</code> <code>varchar(32)</code> <p>User-friendly name for the parameter steps.</p> <code>precluster_param_steps_desc</code> <code>varchar(128)</code> <p>Description of the parameter steps.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterParamSteps(dj.Manual):\n\"\"\"Ordered list of parameter sets that will be run.\n\n    Attributes:\n        precluster_param_steps_id (foreign key): Unique ID for the pre-clustering parameter sets to be run.\n        precluster_param_steps_name (varchar(32) ): User-friendly name for the parameter steps.\n        precluster_param_steps_desc (varchar(128) ): Description of the parameter steps.\n    \"\"\"\n\n    definition = \"\"\"\n    # Ordered list of paramset_idx that are to be run\n    # When pre-clustering is not performed, do not create an entry in `Step` Part table\n    precluster_param_steps_id: smallint\n    ---\n    precluster_param_steps_name: varchar(32)\n    precluster_param_steps_desc: varchar(128)\n    \"\"\"\n\n    class Step(dj.Part):\n\"\"\"Define the order of operations for parameter sets.\n\n        Attributes:\n            PreClusterParamSteps (foreign key): PreClusterParamSteps primary key.\n            step_number (foreign key, smallint): Order of operations.\n            PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        step_number: smallint                  # Order of operations\n        ---\n        -&gt; PreClusterParamSet\n        \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterParamSteps.Step","title":"<code>Step</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Define the order of operations for parameter sets.</p> <p>Attributes:</p> Name Type Description <code>PreClusterParamSteps</code> <code>foreign key</code> <p>PreClusterParamSteps primary key.</p> <code>step_number</code> <code>foreign key, smallint</code> <p>Order of operations.</p> <code>PreClusterParamSet</code> <code>dict</code> <p>PreClusterParamSet to be used in pre-clustering.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Step(dj.Part):\n\"\"\"Define the order of operations for parameter sets.\n\n    Attributes:\n        PreClusterParamSteps (foreign key): PreClusterParamSteps primary key.\n        step_number (foreign key, smallint): Order of operations.\n        PreClusterParamSet (dict): PreClusterParamSet to be used in pre-clustering.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    step_number: smallint                  # Order of operations\n    ---\n    -&gt; PreClusterParamSet\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.PreClusterTask","title":"<code>PreClusterTask</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Defines a pre-clusting task ready to be run.</p> <p>Attributes:</p> Name Type Description <code>EphysRecording</code> <code>foreign key</code> <p>EphysRecording primary key.</p> <code>PreclusterParamSteps</code> <code>foreign key</code> <p>PreClusterParam Steps primary key.</p> <code>precluster_output_dir</code> <code>varchar(255)</code> <p>relative path to directory for storing results of pre-clustering.</p> <code>task_mode</code> <code>enum</code> <p><code>none</code> (no pre-clustering), <code>load</code> results from file, or <code>trigger</code> automated pre-clustering.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass PreClusterTask(dj.Manual):\n\"\"\"Defines a pre-clusting task ready to be run.\n\n    Attributes:\n        EphysRecording (foreign key): EphysRecording primary key.\n        PreclusterParamSteps (foreign key): PreClusterParam Steps primary key.\n        precluster_output_dir (varchar(255) ): relative path to directory for storing results of pre-clustering.\n        task_mode (enum ): `none` (no pre-clustering), `load` results from file, or `trigger` automated pre-clustering.\n    \"\"\"\n\n    definition = \"\"\"\n    # Manual table for defining a clustering task ready to be run\n    -&gt; EphysRecording\n    -&gt; PreClusterParamSteps\n    ---\n    precluster_output_dir='': varchar(255)  #  pre-clustering output directory relative to the root data directory\n    task_mode='none': enum('none','load', 'trigger') # 'none': no pre-clustering analysis\n                                                     # 'load': load analysis results\n                                                     # 'trigger': trigger computation\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.ProbeInsertion","title":"<code>ProbeInsertion</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Information about probe insertion across subjects and sessions.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Session primary key.</p> <code>insertion_number</code> <code>foreign key, str</code> <p>Unique insertion number for each probe insertion for a given session.</p> <code>probe.Probe</code> <code>str</code> <p>probe.Probe primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass ProbeInsertion(dj.Manual):\n\"\"\"Information about probe insertion across subjects and sessions.\n\n    Attributes:\n        Session (foreign key): Session primary key.\n        insertion_number (foreign key, str): Unique insertion number for each probe insertion for a given session.\n        probe.Probe (str): probe.Probe primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Probe insertion implanted into an animal for a given session.\n    -&gt; Session\n    insertion_number: tinyint unsigned\n    ---\n    -&gt; probe.Probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics","title":"<code>QualityMetrics</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>Clustering and waveform quality metrics.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass QualityMetrics(dj.Imported):\n\"\"\"Clustering and waveform quality metrics.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # Clusters and waveforms metrics\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            firing_rate (float): Firing rate of the unit.\n            snr (float): Signal-to-noise ratio for a unit.\n            presence_ratio (float): Fraction of time where spikes are present.\n            isi_violation (float): rate of ISI violation as a fraction of overall rate.\n            number_violation (int): Total ISI violations.\n            amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n            isolation_distance (float): Distance to nearest cluster.\n            l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n            d_prime (float): Classification accuracy based on LDA.\n            nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n            nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n            silhouette_core (float): Maximum change in spike depth throughout recording.\n            cumulative_drift (float): Cumulative change in spike depth throughout recording.\n            contamination_rate (float): Frequency of spikes in the refractory period.\n        \"\"\"\n\n        definition = \"\"\"\n        # Cluster metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        firing_rate=null: float # (Hz) firing rate for a unit\n        snr=null: float  # signal-to-noise ratio for a unit\n        presence_ratio=null: float  # fraction of time in which spikes are present\n        isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n        number_violation=null: int  # total number of ISI violations\n        amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n        isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n        l_ratio=null: float  #\n        d_prime=null: float  # Classification accuracy based on LDA\n        nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n        nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n        silhouette_score=null: float  # Standard metric for cluster overlap\n        max_drift=null: float  # Maximum change in spike depth throughout recording\n        cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n        contamination_rate=null: float #\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n        Attributes:\n            QualityMetrics (foreign key): QualityMetrics primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n            duration (float): Time between waveform peak and trough in milliseconds.\n            halfwidth (float): Spike width at half max amplitude.\n            pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n            repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n            recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n            spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n            velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n            velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n        \"\"\"\n\n        definition = \"\"\"\n        # Waveform metrics for a particular unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        amplitude: float  # (uV) absolute difference between waveform peak and trough\n        duration: float  # (ms) time between waveform peak and trough\n        halfwidth=null: float  # (ms) spike width at half max amplitude\n        pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n        repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n        recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n        spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n        velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n        velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n        output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        metric_fp = kilosort_dir / \"metrics.csv\"\n\n        if not metric_fp.exists():\n            raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n        metrics_df = pd.read_csv(metric_fp)\n        metrics_df.set_index(\"cluster_id\", inplace=True)\n\n        metrics_list = [\n            dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n            for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n        ]\n\n        self.insert1(key)\n        self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n        self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.Cluster","title":"<code>Cluster</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Cluster metrics for a unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>firing_rate</code> <code>float</code> <p>Firing rate of the unit.</p> <code>snr</code> <code>float</code> <p>Signal-to-noise ratio for a unit.</p> <code>presence_ratio</code> <code>float</code> <p>Fraction of time where spikes are present.</p> <code>isi_violation</code> <code>float</code> <p>rate of ISI violation as a fraction of overall rate.</p> <code>number_violation</code> <code>int</code> <p>Total ISI violations.</p> <code>amplitude_cutoff</code> <code>float</code> <p>Estimate of miss rate based on amplitude histogram.</p> <code>isolation_distance</code> <code>float</code> <p>Distance to nearest cluster.</p> <code>l_ratio</code> <code>float</code> <p>Amount of empty space between a cluster and other spikes in dataset.</p> <code>d_prime</code> <code>float</code> <p>Classification accuracy based on LDA.</p> <code>nn_hit_rate</code> <code>float</code> <p>Fraction of neighbors for target cluster that are also in target cluster.</p> <code>nn_miss_rate</code> <code>float</code> <p>Fraction of neighbors outside target cluster that are in the target cluster.</p> <code>silhouette_core</code> <code>float</code> <p>Maximum change in spike depth throughout recording.</p> <code>cumulative_drift</code> <code>float</code> <p>Cumulative change in spike depth throughout recording.</p> <code>contamination_rate</code> <code>float</code> <p>Frequency of spikes in the refractory period.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Cluster(dj.Part):\n\"\"\"Cluster metrics for a unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        firing_rate (float): Firing rate of the unit.\n        snr (float): Signal-to-noise ratio for a unit.\n        presence_ratio (float): Fraction of time where spikes are present.\n        isi_violation (float): rate of ISI violation as a fraction of overall rate.\n        number_violation (int): Total ISI violations.\n        amplitude_cutoff (float): Estimate of miss rate based on amplitude histogram.\n        isolation_distance (float): Distance to nearest cluster.\n        l_ratio (float): Amount of empty space between a cluster and other spikes in dataset.\n        d_prime (float): Classification accuracy based on LDA.\n        nn_hit_rate (float): Fraction of neighbors for target cluster that are also in target cluster.\n        nn_miss_rate (float): Fraction of neighbors outside target cluster that are in the target cluster.\n        silhouette_core (float): Maximum change in spike depth throughout recording.\n        cumulative_drift (float): Cumulative change in spike depth throughout recording.\n        contamination_rate (float): Frequency of spikes in the refractory period.\n    \"\"\"\n\n    definition = \"\"\"\n    # Cluster metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    firing_rate=null: float # (Hz) firing rate for a unit\n    snr=null: float  # signal-to-noise ratio for a unit\n    presence_ratio=null: float  # fraction of time in which spikes are present\n    isi_violation=null: float   # rate of ISI violation as a fraction of overall rate\n    number_violation=null: int  # total number of ISI violations\n    amplitude_cutoff=null: float  # estimate of miss rate based on amplitude histogram\n    isolation_distance=null: float  # distance to nearest cluster in Mahalanobis space\n    l_ratio=null: float  #\n    d_prime=null: float  # Classification accuracy based on LDA\n    nn_hit_rate=null: float  # Fraction of neighbors for target cluster that are also in target cluster\n    nn_miss_rate=null: float # Fraction of neighbors outside target cluster that are in target cluster\n    silhouette_score=null: float  # Standard metric for cluster overlap\n    max_drift=null: float  # Maximum change in spike depth throughout recording\n    cumulative_drift=null: float  # Cumulative change in spike depth throughout recording\n    contamination_rate=null: float #\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Waveform metrics for a particular unit.</p> <p>Attributes:</p> Name Type Description <code>QualityMetrics</code> <code>foreign key</code> <p>QualityMetrics primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>amplitude</code> <code>float</code> <p>Absolute difference between waveform peak and trough in microvolts.</p> <code>duration</code> <code>float</code> <p>Time between waveform peak and trough in milliseconds.</p> <code>halfwidth</code> <code>float</code> <p>Spike width at half max amplitude.</p> <code>pt_ratio</code> <code>float</code> <p>Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.</p> <code>repolarization_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from trough to peak.</p> <code>recovery_slope</code> <code>float</code> <p>Slope of the regression line fit to first 30 microseconds from peak to tail.</p> <code>spread</code> <code>float</code> <p>The range with amplitude over 12-percent of maximum amplitude along the probe.</p> <code>velocity_above</code> <code>float</code> <p>inverse velocity of waveform propagation from soma to the top of the probe.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Waveform metrics for a particular unit.\n\n    Attributes:\n        QualityMetrics (foreign key): QualityMetrics primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        amplitude (float): Absolute difference between waveform peak and trough in microvolts.\n        duration (float): Time between waveform peak and trough in milliseconds.\n        halfwidth (float): Spike width at half max amplitude.\n        pt_ratio (float): Absolute amplitude of peak divided by absolute amplitude of trough relative to 0.\n        repolarization_slope (float): Slope of the regression line fit to first 30 microseconds from trough to peak.\n        recovery_slope (float): Slope of the regression line fit to first 30 microseconds from peak to tail.\n        spread (float): The range with amplitude over 12-percent of maximum amplitude along the probe.\n        velocity_above (float): inverse velocity of waveform propagation from soma to the top of the probe.\n        velocity_below (float) inverse velocity of waveform propagation from soma toward the bottom of the probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Waveform metrics for a particular unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    amplitude: float  # (uV) absolute difference between waveform peak and trough\n    duration: float  # (ms) time between waveform peak and trough\n    halfwidth=null: float  # (ms) spike width at half max amplitude\n    pt_ratio=null: float  # absolute amplitude of peak divided by absolute amplitude of trough relative to 0\n    repolarization_slope=null: float  # the repolarization slope was defined by fitting a regression line to the first 30us from trough to peak\n    recovery_slope=null: float  # the recovery slope was defined by fitting a regression line to the first 30us from peak to tail\n    spread=null: float  # (um) the range with amplitude above 12-percent of the maximum amplitude along the probe\n    velocity_above=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the top of the probe\n    velocity_below=null: float  # (s/m) inverse velocity of waveform propagation from the soma toward the bottom of the probe\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.QualityMetrics.make","title":"<code>make(key)</code>","text":"<p>Populates tables with quality metrics data.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates tables with quality metrics data.\"\"\"\n    output_dir = (ClusteringTask &amp; key).fetch1(\"clustering_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    metric_fp = kilosort_dir / \"metrics.csv\"\n\n    if not metric_fp.exists():\n        raise FileNotFoundError(f\"QC metrics file not found: {metric_fp}\")\n\n    metrics_df = pd.read_csv(metric_fp)\n    metrics_df.set_index(\"cluster_id\", inplace=True)\n\n    metrics_list = [\n        dict(metrics_df.loc[unit_key[\"unit\"]], **unit_key)\n        for unit_key in (CuratedClustering.Unit &amp; key).fetch(\"KEY\")\n    ]\n\n    self.insert1(key)\n    self.Cluster.insert(metrics_list, ignore_extra_fields=True)\n    self.Waveform.insert(metrics_list, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet","title":"<code>WaveformSet</code>","text":"<p>         Bases: <code>dj.Imported</code></p> <p>A set of spike waveforms for units out of a given CuratedClustering.</p> <p>Attributes:</p> Name Type Description <code>CuratedClustering</code> <code>foreign key</code> <p>CuratedClustering primary key.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>@schema\nclass WaveformSet(dj.Imported):\n\"\"\"A set of spike waveforms for units out of a given CuratedClustering.\n\n    Attributes:\n        CuratedClustering (foreign key): CuratedClustering primary key.\n    \"\"\"\n\n    definition = \"\"\"\n    # A set of spike waveforms for units out of a given CuratedClustering\n    -&gt; CuratedClustering\n    \"\"\"\n\n    class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n        \"\"\"\n\n        definition = \"\"\"\n        # Mean waveform across spikes for a given unit at its representative electrode\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        ---\n        peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n        \"\"\"\n\n    class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n        Attributes:\n            WaveformSet (foreign key): WaveformSet primary key.\n            CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n            probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n            waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n            waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n        \"\"\"\n\n        definition = \"\"\"\n        # Spike waveforms and their mean across spikes for the given unit\n        -&gt; master\n        -&gt; CuratedClustering.Unit\n        -&gt; probe.ElectrodeConfig.Electrode\n        ---\n        waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n        waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n        output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n        kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n        kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n        acq_software, probe_serial_number = (\n            EphysRecording * ProbeInsertion &amp; key\n        ).fetch1(\"acq_software\", \"probe\")\n\n        # -- Get channel and electrode-site mapping\n        recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n        channel2electrodes = get_neuropixels_channel2electrode_map(\n            recording_key, acq_software\n        )\n\n        is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n        # Get all units\n        units = {\n            u[\"unit\"]: u\n            for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n        }\n\n        if is_qc:\n            unit_waveforms = np.load(\n                kilosort_dir / \"mean_waveforms.npy\"\n            )  # unit x channel x sample\n\n            def yield_unit_waveforms():\n                for unit_no, unit_waveform in zip(\n                    kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n                ):\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n                    if unit_no in units:\n                        for channel, channel_waveform in zip(\n                            kilosort_dataset.data[\"channel_map\"], unit_waveform\n                        ):\n                            unit_electrode_waveforms.append(\n                                {\n                                    **units[unit_no],\n                                    **channel2electrodes[channel],\n                                    \"waveform_mean\": channel_waveform,\n                                }\n                            )\n                            if (\n                                channel2electrodes[channel][\"electrode\"]\n                                == units[unit_no][\"electrode\"]\n                            ):\n                                unit_peak_waveform = {\n                                    **units[unit_no],\n                                    \"peak_electrode_waveform\": channel_waveform,\n                                }\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        else:\n            if acq_software == \"SpikeGLX\":\n                spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n                neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n            elif acq_software == \"Open Ephys\":\n                session_dir = find_full_path(\n                    get_ephys_root_data_dir(), get_session_directory(key)\n                )\n                openephys_dataset = openephys.OpenEphys(session_dir)\n                neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n            def yield_unit_waveforms():\n                for unit_dict in units.values():\n                    unit_peak_waveform = {}\n                    unit_electrode_waveforms = []\n\n                    spikes = unit_dict[\"spike_times\"]\n                    waveforms = neuropixels_recording.extract_spike_waveforms(\n                        spikes, kilosort_dataset.data[\"channel_map\"]\n                    )  # (sample x channel x spike)\n                    waveforms = waveforms.transpose(\n                        (1, 2, 0)\n                    )  # (channel x spike x sample)\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], waveforms\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **unit_dict,\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform.mean(axis=0),\n                                \"waveforms\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == unit_dict[\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **unit_dict,\n                                \"peak_electrode_waveform\": channel_waveform.mean(\n                                    axis=0\n                                ),\n                            }\n\n                    yield unit_peak_waveform, unit_electrode_waveforms\n\n        # insert waveform on a per-unit basis to mitigate potential memory issue\n        self.insert1(key)\n        for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n            self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n            self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.PeakWaveform","title":"<code>PeakWaveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Mean waveform across spikes for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>peak_electrode_waveform</code> <code>longblob</code> <p>Mean waveform for a given unit at its representative electrode.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class PeakWaveform(dj.Part):\n\"\"\"Mean waveform across spikes for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        peak_electrode_waveform (longblob): Mean waveform for a given unit at its representative electrode.\n    \"\"\"\n\n    definition = \"\"\"\n    # Mean waveform across spikes for a given unit at its representative electrode\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    ---\n    peak_electrode_waveform: longblob  # (uV) mean waveform for a given unit at its representative electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.Waveform","title":"<code>Waveform</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Spike waveforms for a given unit.</p> <p>Attributes:</p> Name Type Description <code>WaveformSet</code> <code>foreign key</code> <p>WaveformSet primary key.</p> <code>CuratedClustering.Unit</code> <code>foreign key</code> <p>CuratedClustering.Unit primary key.</p> <code>probe.ElectrodeConfig.Electrode</code> <code>foreign key</code> <p>probe.ElectrodeConfig.Electrode primary key.</p> <code>waveform_mean</code> <code>longblob</code> <p>mean waveform across spikes of the unit in microvolts.</p> <code>waveforms</code> <code>longblob</code> <p>waveforms of a sampling of spikes at the given electrode and unit.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>class Waveform(dj.Part):\n\"\"\"Spike waveforms for a given unit.\n\n    Attributes:\n        WaveformSet (foreign key): WaveformSet primary key.\n        CuratedClustering.Unit (foreign key): CuratedClustering.Unit primary key.\n        probe.ElectrodeConfig.Electrode (foreign key): probe.ElectrodeConfig.Electrode primary key.\n        waveform_mean (longblob): mean waveform across spikes of the unit in microvolts.\n        waveforms (longblob): waveforms of a sampling of spikes at the given electrode and unit.\n    \"\"\"\n\n    definition = \"\"\"\n    # Spike waveforms and their mean across spikes for the given unit\n    -&gt; master\n    -&gt; CuratedClustering.Unit\n    -&gt; probe.ElectrodeConfig.Electrode\n    ---\n    waveform_mean: longblob   # (uV) mean waveform across spikes of the given unit\n    waveforms=null: longblob  # (uV) (spike x sample) waveforms of a sampling of spikes at the given electrode for the given unit\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.WaveformSet.make","title":"<code>make(key)</code>","text":"<p>Populates waveform tables.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def make(self, key):\n\"\"\"Populates waveform tables.\"\"\"\n    output_dir = (Curation &amp; key).fetch1(\"curation_output_dir\")\n    kilosort_dir = find_full_path(get_ephys_root_data_dir(), output_dir)\n\n    kilosort_dataset = kilosort.Kilosort(kilosort_dir)\n\n    acq_software, probe_serial_number = (\n        EphysRecording * ProbeInsertion &amp; key\n    ).fetch1(\"acq_software\", \"probe\")\n\n    # -- Get channel and electrode-site mapping\n    recording_key = (EphysRecording &amp; key).fetch1(\"KEY\")\n    channel2electrodes = get_neuropixels_channel2electrode_map(\n        recording_key, acq_software\n    )\n\n    is_qc = (Curation &amp; key).fetch1(\"quality_control\")\n\n    # Get all units\n    units = {\n        u[\"unit\"]: u\n        for u in (CuratedClustering.Unit &amp; key).fetch(as_dict=True, order_by=\"unit\")\n    }\n\n    if is_qc:\n        unit_waveforms = np.load(\n            kilosort_dir / \"mean_waveforms.npy\"\n        )  # unit x channel x sample\n\n        def yield_unit_waveforms():\n            for unit_no, unit_waveform in zip(\n                kilosort_dataset.data[\"cluster_ids\"], unit_waveforms\n            ):\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n                if unit_no in units:\n                    for channel, channel_waveform in zip(\n                        kilosort_dataset.data[\"channel_map\"], unit_waveform\n                    ):\n                        unit_electrode_waveforms.append(\n                            {\n                                **units[unit_no],\n                                **channel2electrodes[channel],\n                                \"waveform_mean\": channel_waveform,\n                            }\n                        )\n                        if (\n                            channel2electrodes[channel][\"electrode\"]\n                            == units[unit_no][\"electrode\"]\n                        ):\n                            unit_peak_waveform = {\n                                **units[unit_no],\n                                \"peak_electrode_waveform\": channel_waveform,\n                            }\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    else:\n        if acq_software == \"SpikeGLX\":\n            spikeglx_meta_filepath = get_spikeglx_meta_filepath(key)\n            neuropixels_recording = spikeglx.SpikeGLX(spikeglx_meta_filepath.parent)\n        elif acq_software == \"Open Ephys\":\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(key)\n            )\n            openephys_dataset = openephys.OpenEphys(session_dir)\n            neuropixels_recording = openephys_dataset.probes[probe_serial_number]\n\n        def yield_unit_waveforms():\n            for unit_dict in units.values():\n                unit_peak_waveform = {}\n                unit_electrode_waveforms = []\n\n                spikes = unit_dict[\"spike_times\"]\n                waveforms = neuropixels_recording.extract_spike_waveforms(\n                    spikes, kilosort_dataset.data[\"channel_map\"]\n                )  # (sample x channel x spike)\n                waveforms = waveforms.transpose(\n                    (1, 2, 0)\n                )  # (channel x spike x sample)\n                for channel, channel_waveform in zip(\n                    kilosort_dataset.data[\"channel_map\"], waveforms\n                ):\n                    unit_electrode_waveforms.append(\n                        {\n                            **unit_dict,\n                            **channel2electrodes[channel],\n                            \"waveform_mean\": channel_waveform.mean(axis=0),\n                            \"waveforms\": channel_waveform,\n                        }\n                    )\n                    if (\n                        channel2electrodes[channel][\"electrode\"]\n                        == unit_dict[\"electrode\"]\n                    ):\n                        unit_peak_waveform = {\n                            **unit_dict,\n                            \"peak_electrode_waveform\": channel_waveform.mean(\n                                axis=0\n                            ),\n                        }\n\n                yield unit_peak_waveform, unit_electrode_waveforms\n\n    # insert waveform on a per-unit basis to mitigate potential memory issue\n    self.insert1(key)\n    for unit_peak_waveform, unit_electrode_waveforms in yield_unit_waveforms():\n        self.PeakWaveform.insert1(unit_peak_waveform, ignore_extra_fields=True)\n        self.Waveform.insert(unit_electrode_waveforms, ignore_extra_fields=True)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.activate","title":"<code>activate(ephys_schema_name, probe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>","text":"<p>Activates the <code>ephys</code> and <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>ephys_schema_name</code> <code>str</code> <p>A string containing the name of the ephys schema.</p> required <code>probe_schema_name</code> <code>str</code> <p>A string containing the name of the probe scehma.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.</p> Functions <p>get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s). get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def activate(\n    ephys_schema_name: str,\n    probe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activates the `ephys` and `probe` schemas.\n\n    Args:\n        ephys_schema_name (str): A string containing the name of the ephys schema.\n        probe_schema_name (str): A string containing the name of the probe scehma.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n        linking_module (str): A string containing the module name or module containing the required dependencies to activate the schema.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion\n        Probe: A parent table to EphysRecording. Probe information is required before electrophysiology data is imported.\n\n    Functions:\n        get_ephys_root_data_dir(): Returns absolute path for root data director(y/ies) with all electrophysiological recording sessions, as a list of string(s).\n        get_session_direction(session_key: dict): Returns path to electrophysiology data for the a particular session as a list of strings.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    probe.activate(\n        probe_schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n    schema.activate(\n        ephys_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n    ephys_report.activate(f\"{ephys_schema_name}_report\", ephys_schema_name)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.generate_electrode_config","title":"<code>generate_electrode_config(probe_type, electrode_keys)</code>","text":"<p>Generate and insert new ElectrodeConfig</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g. neuropixels 2.0 - SS)</p> required <code>electrode_keys</code> <code>list</code> <p>list of keys of the probe.ProbeType.Electrode table</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>representing a key of the probe.ElectrodeConfig table</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def generate_electrode_config(probe_type: str, electrode_keys: list) -&gt; dict:\n\"\"\"Generate and insert new ElectrodeConfig\n\n    Args:\n        probe_type (str): probe type (e.g. neuropixels 2.0 - SS)\n        electrode_keys (list): list of keys of the probe.ProbeType.Electrode table\n\n    Returns:\n        dict: representing a key of the probe.ElectrodeConfig table\n    \"\"\"\n    # compute hash for the electrode config (hash of dict of all ElectrodeConfig.Electrode)\n    electrode_config_hash = dict_to_uuid({k[\"electrode\"]: k for k in electrode_keys})\n\n    electrode_list = sorted([k[\"electrode\"] for k in electrode_keys])\n    electrode_gaps = (\n        [-1]\n        + np.where(np.diff(electrode_list) &gt; 1)[0].tolist()\n        + [len(electrode_list) - 1]\n    )\n    electrode_config_name = \"; \".join(\n        [\n            f\"{electrode_list[start + 1]}-{electrode_list[end]}\"\n            for start, end in zip(electrode_gaps[:-1], electrode_gaps[1:])\n        ]\n    )\n\n    electrode_config_key = {\"electrode_config_hash\": electrode_config_hash}\n\n    # ---- make new ElectrodeConfig if needed ----\n    if not probe.ElectrodeConfig &amp; electrode_config_key:\n        probe.ElectrodeConfig.insert1(\n            {\n                **electrode_config_key,\n                \"probe_type\": probe_type,\n                \"electrode_config_name\": electrode_config_name,\n            }\n        )\n        probe.ElectrodeConfig.Electrode.insert(\n            {**electrode_config_key, **electrode} for electrode in electrode_keys\n        )\n\n    return electrode_config_key\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Fetches absolute data path to ephys data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to ephys data directories.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_ephys_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to ephys data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to ephys data directories.\n    \"\"\"\n    return _linking_module.get_ephys_root_data_dir()\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_neuropixels_channel2electrode_map","title":"<code>get_neuropixels_channel2electrode_map(ephys_recording_key, acq_software)</code>","text":"<p>Get the channel map for neuropixels probe.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_neuropixels_channel2electrode_map(\n    ephys_recording_key: dict, acq_software: str\n) -&gt; dict:\n\"\"\"Get the channel map for neuropixels probe.\"\"\"\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = get_spikeglx_meta_filepath(ephys_recording_key)\n        spikeglx_meta = spikeglx.SpikeGLXMeta(spikeglx_meta_filepath)\n        electrode_config_key = (\n            EphysRecording * probe.ElectrodeConfig &amp; ephys_recording_key\n        ).fetch1(\"KEY\")\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n            &amp; electrode_config_key\n        )\n\n        probe_electrodes = {\n            (shank, shank_col, shank_row): key\n            for key, shank, shank_col, shank_row in zip(\n                *electrode_query.fetch(\"KEY\", \"shank\", \"shank_col\", \"shank_row\")\n            )\n        }\n\n        channel2electrode_map = {\n            recorded_site: probe_electrodes[(shank, shank_col, shank_row)]\n            for recorded_site, (shank, shank_col, shank_row, _) in enumerate(\n                spikeglx_meta.shankmap[\"data\"]\n            )\n        }\n    elif acq_software == \"Open Ephys\":\n        session_dir = find_full_path(\n            get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n        )\n        openephys_dataset = openephys.OpenEphys(session_dir)\n        probe_serial_number = (ProbeInsertion &amp; ephys_recording_key).fetch1(\"probe\")\n        probe_dataset = openephys_dataset.probes[probe_serial_number]\n\n        electrode_query = (\n            probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode * EphysRecording\n            &amp; ephys_recording_key\n        )\n\n        probe_electrodes = {\n            key[\"electrode\"]: key for key in electrode_query.fetch(\"KEY\")\n        }\n\n        channel2electrode_map = {\n            channel_idx: probe_electrodes[channel_idx]\n            for channel_idx in probe_dataset.ap_meta[\"channels_ids\"]\n        }\n\n    return channel2electrode_map\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Retrieve the session directory with Neuropixels for the given session.</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string for the path to the session directory.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Retrieve the session directory with Neuropixels for the given session.\n\n    Args:\n        session_key (dict): A dictionary mapping subject to an entry in the subject table, and session_datetime corresponding to a session in the database.\n\n    Returns:\n        A string for the path to the session directory.\n    \"\"\"\n    return _linking_module.get_session_directory(session_key)\n</code></pre>"},{"location":"api/element_array_ephys/ephys_precluster/#element_array_ephys.ephys_precluster.get_spikeglx_meta_filepath","title":"<code>get_spikeglx_meta_filepath(ephys_recording_key)</code>","text":"<p>Get spikeGLX data filepath.</p> Source code in <code>element_array_ephys/ephys_precluster.py</code> <pre><code>def get_spikeglx_meta_filepath(ephys_recording_key: dict) -&gt; str:\n\"\"\"Get spikeGLX data filepath.\"\"\"\n    # attempt to retrieve from EphysRecording.EphysFile\n    spikeglx_meta_filepath = (\n        EphysRecording.EphysFile &amp; ephys_recording_key &amp; 'file_path LIKE \"%.ap.meta\"'\n    ).fetch1(\"file_path\")\n\n    try:\n        spikeglx_meta_filepath = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath\n        )\n    except FileNotFoundError:\n        # if not found, search in session_dir again\n        if not spikeglx_meta_filepath.exists():\n            session_dir = find_full_path(\n                get_ephys_root_data_dir(), get_session_directory(ephys_recording_key)\n            )\n            inserted_probe_serial_number = (\n                ProbeInsertion * probe.Probe &amp; ephys_recording_key\n            ).fetch1(\"probe\")\n\n            spikeglx_meta_filepaths = [fp for fp in session_dir.rglob(\"*.ap.meta\")]\n            for meta_filepath in spikeglx_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n                if str(spikeglx_meta.probe_SN) == inserted_probe_serial_number:\n                    spikeglx_meta_filepath = meta_filepath\n                    break\n            else:\n                raise FileNotFoundError(\n                    \"No SpikeGLX data found for probe insertion: {}\".format(\n                        ephys_recording_key\n                    )\n                )\n\n    return spikeglx_meta_filepath\n</code></pre>"},{"location":"api/element_array_ephys/ephys_report/","title":"ephys_report.py","text":""},{"location":"api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.ProbeLevelReport","title":"<code>ProbeLevelReport</code>","text":"<p>         Bases: <code>dj.Computed</code></p> <p>Table for storing probe level figures.</p> <p>Attributes:</p> Name Type Description <code>ephys.CuratedClustering</code> <code>foreign key</code> <p>ephys.CuratedClustering primary key.</p> <code>shank</code> <code>tinyint unsigned</code> <p>Shank of the probe.</p> <code>drift_map_plot</code> <code>attach</code> <p>Figure object for drift map.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass ProbeLevelReport(dj.Computed):\n\"\"\"Table for storing probe level figures.\n\n    Attributes:\n        ephys.CuratedClustering (foreign key): ephys.CuratedClustering primary key.\n        shank (tinyint unsigned): Shank of the probe.\n        drift_map_plot (attach): Figure object for drift map.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.CuratedClustering\n    shank         : tinyint unsigned\n    ---\n    drift_map_plot: attach\n    \"\"\"\n\n    def make(self, key):\n\n        from .plotting.probe_level import plot_driftmap\n\n        save_dir = _make_save_dir()\n\n        units = ephys.CuratedClustering.Unit &amp; key &amp; \"cluster_quality_label='good'\"\n\n        shanks = set((probe.ProbeType.Electrode &amp; units).fetch(\"shank\"))\n\n        for shank_no in shanks:\n\n            table = units * ephys.ProbeInsertion * probe.ProbeType.Electrode &amp; {\n                \"shank\": shank_no\n            }\n\n            spike_times, spike_depths = table.fetch(\n                \"spike_times\", \"spike_depths\", order_by=\"unit\"\n            )\n\n            # Get the figure\n            fig = plot_driftmap(spike_times, spike_depths, colormap=\"gist_heat_r\")\n            fig_prefix = (\n                \"-\".join(\n                    [\n                        v.strftime(\"%Y%m%d%H%M%S\")\n                        if isinstance(v, datetime.datetime)\n                        else str(v)\n                        for v in key.values()\n                    ]\n                )\n                + f\"-{shank_no}\"\n            )\n\n            # Save fig and insert\n            fig_dict = _save_figs(\n                figs=(fig,),\n                fig_names=(\"drift_map_plot\",),\n                save_dir=save_dir,\n                fig_prefix=fig_prefix,\n                extension=\".png\",\n            )\n\n            self.insert1({**key, **fig_dict, \"shank\": shank_no})\n</code></pre>"},{"location":"api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.UnitLevelReport","title":"<code>UnitLevelReport</code>","text":"<p>         Bases: <code>dj.Computed</code></p> <p>Table for storing unit level figures.</p> <p>Attributes:</p> Name Type Description <code>ephys.CuratedClustering.Unit</code> <code>foreign key</code> <p>ephys.CuratedClustering.Unit primary key.</p> <code>ephys.ClusterQualityLabel</code> <code>foreign key</code> <p>ephys.ClusterQualityLabel primary key.</p> <code>waveform_plotly</code> <code>longblob</code> <p>Figure object for unit waveform.</p> <code>autocorrelogram_plotly</code> <code>longblob</code> <p>Figure object for an autocorrelogram.</p> <code>depth_waveform_plotly</code> <code>longblob</code> <p>Figure object for depth waveforms.</p> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>@schema\nclass UnitLevelReport(dj.Computed):\n\"\"\"Table for storing unit level figures.\n\n    Attributes:\n        ephys.CuratedClustering.Unit (foreign key): ephys.CuratedClustering.Unit primary key.\n        ephys.ClusterQualityLabel (foreign key): ephys.ClusterQualityLabel primary key.\n        waveform_plotly (longblob): Figure object for unit waveform.\n        autocorrelogram_plotly (longblob): Figure object for an autocorrelogram.\n        depth_waveform_plotly (longblob): Figure object for depth waveforms.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.CuratedClustering.Unit\n    ---\n    -&gt; ephys.ClusterQualityLabel\n    waveform_plotly                 : longblob\n    autocorrelogram_plotly          : longblob\n    depth_waveform_plotly           : longblob\n    \"\"\"\n\n    def make(self, key):\n\n        from .plotting.unit_level import (\n            plot_auto_correlogram,\n            plot_depth_waveforms,\n            plot_waveform,\n        )\n\n        sampling_rate = (ephys.EphysRecording &amp; key).fetch1(\n            \"sampling_rate\"\n        ) / 1e3  # in kHz\n\n        peak_electrode_waveform, spike_times, cluster_quality_label = (\n            (ephys.CuratedClustering.Unit &amp; key) * ephys.WaveformSet.PeakWaveform\n        ).fetch1(\"peak_electrode_waveform\", \"spike_times\", \"cluster_quality_label\")\n\n        # Get the figure\n        waveform_fig = plot_waveform(\n            waveform=peak_electrode_waveform, sampling_rate=sampling_rate\n        )\n\n        correlogram_fig = plot_auto_correlogram(\n            spike_times=spike_times, bin_size=0.001, window_size=1\n        )\n\n        depth_waveform_fig = plot_depth_waveforms(ephys, unit_key=key, y_range=60)\n\n        self.insert1(\n            {\n                **key,\n                \"cluster_quality_label\": cluster_quality_label,\n                \"waveform_plotly\": waveform_fig.to_plotly_json(),\n                \"autocorrelogram_plotly\": correlogram_fig.to_plotly_json(),\n                \"depth_waveform_plotly\": depth_waveform_fig.to_plotly_json(),\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/ephys_report/#element_array_ephys.ephys_report.activate","title":"<code>activate(schema_name, ephys_schema_name, *, create_schema=True, create_tables=True)</code>","text":"<p>Activate the current schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>schema name on the database server to activate the <code>ephys_report</code> schema.</p> required <code>ephys_schema_name</code> <code>str</code> <p>schema name of the activated ephys element for which     this ephys_report schema will be downstream from.</p> required <code>create_schema</code> <code>bool</code> <p>If True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True (default), create tables in the database if they do not yet exist.</p> <code>True</code> Source code in <code>element_array_ephys/ephys_report.py</code> <pre><code>def activate(schema_name, ephys_schema_name, *, create_schema=True, create_tables=True):\n\"\"\"Activate the current schema.\n\n    Args:\n        schema_name (str): schema name on the database server to activate the `ephys_report` schema.\n        ephys_schema_name (str): schema name of the activated ephys element for which\n                this ephys_report schema will be downstream from.\n        create_schema (bool, optional): If True (default), create schema in the database if it does not yet exist.\n        create_tables (bool, optional): If True (default), create tables in the database if they do not yet exist.\n    \"\"\"\n\n    global ephys\n    ephys = dj.create_virtual_module(\"ephys\", ephys_schema_name)\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=ephys.__dict__,\n    )\n</code></pre>"},{"location":"api/element_array_ephys/probe/","title":"probe.py","text":"<p>Neuropixels Probes</p>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.ElectrodeConfig","title":"<code>ElectrodeConfig</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Electrode configuration setting on a probe.</p> <p>Attributes:</p> Name Type Description <code>electrode_config_hash</code> <code>foreign key, uuid</code> <p>unique index for electrode configuration.</p> <code>ProbeType</code> <code>dict</code> <p>ProbeType entry.</p> <code>electrode_config_name</code> <code> varchar(4000) </code> <p>User-friendly name for this electrode configuration.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@schema\nclass ElectrodeConfig(dj.Lookup):\n\"\"\"Electrode configuration setting on a probe.\n\n    Attributes:\n        electrode_config_hash (foreign key, uuid): unique index for electrode configuration.\n        ProbeType (dict): ProbeType entry.\n        electrode_config_name ( varchar(4000) ): User-friendly name for this electrode configuration.\n    \"\"\"\n\n    definition = \"\"\"\n    # The electrode configuration setting on a given probe\n    electrode_config_hash: uuid\n    ---\n    -&gt; ProbeType\n    electrode_config_name: varchar(4000)  # user friendly name\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Electrode included in the recording.\n\n        Attributes:\n            ElectrodeConfig (foreign key): ElectrodeConfig primary key.\n            ProbeType.Electrode (foreign key): ProbeType.Electrode primary key.\n        \"\"\"\n\n        definition = \"\"\"  # Electrodes selected for recording\n        -&gt; master\n        -&gt; ProbeType.Electrode\n        \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.ElectrodeConfig.Electrode","title":"<code>Electrode</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Electrode included in the recording.</p> <p>Attributes:</p> Name Type Description <code>ElectrodeConfig</code> <code>foreign key</code> <p>ElectrodeConfig primary key.</p> <code>ProbeType.Electrode</code> <code>foreign key</code> <p>ProbeType.Electrode primary key.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Electrode included in the recording.\n\n    Attributes:\n        ElectrodeConfig (foreign key): ElectrodeConfig primary key.\n        ProbeType.Electrode (foreign key): ProbeType.Electrode primary key.\n    \"\"\"\n\n    definition = \"\"\"  # Electrodes selected for recording\n    -&gt; master\n    -&gt; ProbeType.Electrode\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.Probe","title":"<code>Probe</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Represent a physical probe with unique ID</p> <p>Attributes:</p> Name Type Description <code>probe</code> <code>foreign key, varchar(32) </code> <p>Unique ID for this model of the probe.</p> <code>ProbeType</code> <code>dict</code> <p>ProbeType entry.</p> <code>probe_comment</code> <code> varchar(1000) </code> <p>Comment about this model of probe.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@schema\nclass Probe(dj.Lookup):\n\"\"\"Represent a physical probe with unique ID\n\n    Attributes:\n        probe (foreign key, varchar(32) ): Unique ID for this model of the probe.\n        ProbeType (dict): ProbeType entry.\n        probe_comment ( varchar(1000) ): Comment about this model of probe.\n    \"\"\"\n\n    definition = \"\"\"\n    # Represent a physical probe with unique identification\n    probe: varchar(32)  # unique identifier for this model of probe (e.g. serial number)\n    ---\n    -&gt; ProbeType\n    probe_comment='' :  varchar(1000)\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType","title":"<code>ProbeType</code>","text":"<p>         Bases: <code>dj.Lookup</code></p> <p>Type of probe.</p> <p>Attributes:</p> Name Type Description <code>probe_type</code> <code>foreign key, varchar (32) </code> <p>Name of the probe type.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@schema\nclass ProbeType(dj.Lookup):\n\"\"\"Type of probe.\n\n    Attributes:\n        probe_type (foreign key, varchar (32) ): Name of the probe type.\n    \"\"\"\n\n    definition = \"\"\"\n    # Type of probe, with specific electrodes geometry defined\n    probe_type: varchar(32)  # e.g. neuropixels_1.0\n    \"\"\"\n\n    class Electrode(dj.Part):\n\"\"\"Electrode information for a given probe.\n\n        Attributes:\n            ProbeType (foreign key): ProbeType primary key.\n            electrode (foreign key, int): Electrode index, starting at 0.\n            shank (int): shank index, starting at 0.\n            shank_col (int): column index, starting at 0.\n            shank_row (int): row index, starting at 0.\n            x_coord (float): x-coordinate of the electrode within the probe in micrometers.\n            y_coord (float): y-coordinate of the electrode within the probe in micrometers.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        electrode: int       # electrode index, starts at 0\n        ---\n        shank: int           # shank index, starts at 0, advance left to right\n        shank_col: int       # column index, starts at 0, advance left to right\n        shank_row: int       # row index, starts at 0.\n        x_coord=NULL: float  # (um) x coordinate of the electrode within the probe.\n        y_coord=NULL: float  # (um) y coordinate of the electrode within the probe.\n        \"\"\"\n\n    @staticmethod\n    def create_neuropixels_probe(probe_type: str = \"neuropixels 1.0 - 3A\"):\n\"\"\"\n        Create `ProbeType` and `Electrode` for neuropixels probes:\n        + neuropixels 1.0 - 3A\n        + neuropixels 1.0 - 3B\n        + neuropixels UHD\n        + neuropixels 2.0 - SS\n        + neuropixels 2.0 - MS\n\n        For electrode location, the (0, 0) is the\n         bottom left corner of the probe (ignore the tip portion)\n        Electrode numbering is 1-indexing\n        \"\"\"\n\n        neuropixels_probes_config = {\n            \"neuropixels 1.0 - 3A\": dict(\n                site_count_per_shank=960,\n                col_spacing=32,\n                row_spacing=20,\n                white_spacing=16,\n                col_count_per_shank=2,\n                shank_count=1,\n                shank_spacing=0,\n            ),\n            \"neuropixels 1.0 - 3B\": dict(\n                site_count_per_shank=960,\n                col_spacing=32,\n                row_spacing=20,\n                white_spacing=16,\n                col_count_per_shank=2,\n                shank_count=1,\n                shank_spacing=0,\n            ),\n            \"neuropixels UHD\": dict(\n                site_count_per_shank=384,\n                col_spacing=6,\n                row_spacing=6,\n                white_spacing=0,\n                col_count_per_shank=8,\n                shank_count=1,\n                shank_spacing=0,\n            ),\n            \"neuropixels 2.0 - SS\": dict(\n                site_count_per_shank=1280,\n                col_spacing=32,\n                row_spacing=15,\n                white_spacing=0,\n                col_count_per_shank=2,\n                shank_count=1,\n                shank_spacing=250,\n            ),\n            \"neuropixels 2.0 - MS\": dict(\n                site_count_per_shank=1280,\n                col_spacing=32,\n                row_spacing=15,\n                white_spacing=0,\n                col_count_per_shank=2,\n                shank_count=4,\n                shank_spacing=250,\n            ),\n        }\n\n        probe_type = {\"probe_type\": probe_type}\n        electrode_layouts = build_electrode_layouts(\n            **{**neuropixels_probes_config[probe_type[\"probe_type\"]], **probe_type}\n        )\n        with ProbeType.connection.transaction:\n            ProbeType.insert1(probe_type, skip_duplicates=True)\n            ProbeType.Electrode.insert(electrode_layouts, skip_duplicates=True)\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.Electrode","title":"<code>Electrode</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Electrode information for a given probe.</p> <p>Attributes:</p> Name Type Description <code>ProbeType</code> <code>foreign key</code> <p>ProbeType primary key.</p> <code>electrode</code> <code>foreign key, int</code> <p>Electrode index, starting at 0.</p> <code>shank</code> <code>int</code> <p>shank index, starting at 0.</p> <code>shank_col</code> <code>int</code> <p>column index, starting at 0.</p> <code>shank_row</code> <code>int</code> <p>row index, starting at 0.</p> <code>x_coord</code> <code>float</code> <p>x-coordinate of the electrode within the probe in micrometers.</p> <code>y_coord</code> <code>float</code> <p>y-coordinate of the electrode within the probe in micrometers.</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>class Electrode(dj.Part):\n\"\"\"Electrode information for a given probe.\n\n    Attributes:\n        ProbeType (foreign key): ProbeType primary key.\n        electrode (foreign key, int): Electrode index, starting at 0.\n        shank (int): shank index, starting at 0.\n        shank_col (int): column index, starting at 0.\n        shank_row (int): row index, starting at 0.\n        x_coord (float): x-coordinate of the electrode within the probe in micrometers.\n        y_coord (float): y-coordinate of the electrode within the probe in micrometers.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    electrode: int       # electrode index, starts at 0\n    ---\n    shank: int           # shank index, starts at 0, advance left to right\n    shank_col: int       # column index, starts at 0, advance left to right\n    shank_row: int       # row index, starts at 0.\n    x_coord=NULL: float  # (um) x coordinate of the electrode within the probe.\n    y_coord=NULL: float  # (um) y coordinate of the electrode within the probe.\n    \"\"\"\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.ProbeType.create_neuropixels_probe","title":"<code>create_neuropixels_probe(probe_type='neuropixels 1.0 - 3A')</code>  <code>staticmethod</code>","text":"<p>Create <code>ProbeType</code> and <code>Electrode</code> for neuropixels probes: + neuropixels 1.0 - 3A + neuropixels 1.0 - 3B + neuropixels UHD + neuropixels 2.0 - SS + neuropixels 2.0 - MS</p> <p>For electrode location, the (0, 0) is the  bottom left corner of the probe (ignore the tip portion) Electrode numbering is 1-indexing</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>@staticmethod\ndef create_neuropixels_probe(probe_type: str = \"neuropixels 1.0 - 3A\"):\n\"\"\"\n    Create `ProbeType` and `Electrode` for neuropixels probes:\n    + neuropixels 1.0 - 3A\n    + neuropixels 1.0 - 3B\n    + neuropixels UHD\n    + neuropixels 2.0 - SS\n    + neuropixels 2.0 - MS\n\n    For electrode location, the (0, 0) is the\n     bottom left corner of the probe (ignore the tip portion)\n    Electrode numbering is 1-indexing\n    \"\"\"\n\n    neuropixels_probes_config = {\n        \"neuropixels 1.0 - 3A\": dict(\n            site_count_per_shank=960,\n            col_spacing=32,\n            row_spacing=20,\n            white_spacing=16,\n            col_count_per_shank=2,\n            shank_count=1,\n            shank_spacing=0,\n        ),\n        \"neuropixels 1.0 - 3B\": dict(\n            site_count_per_shank=960,\n            col_spacing=32,\n            row_spacing=20,\n            white_spacing=16,\n            col_count_per_shank=2,\n            shank_count=1,\n            shank_spacing=0,\n        ),\n        \"neuropixels UHD\": dict(\n            site_count_per_shank=384,\n            col_spacing=6,\n            row_spacing=6,\n            white_spacing=0,\n            col_count_per_shank=8,\n            shank_count=1,\n            shank_spacing=0,\n        ),\n        \"neuropixels 2.0 - SS\": dict(\n            site_count_per_shank=1280,\n            col_spacing=32,\n            row_spacing=15,\n            white_spacing=0,\n            col_count_per_shank=2,\n            shank_count=1,\n            shank_spacing=250,\n        ),\n        \"neuropixels 2.0 - MS\": dict(\n            site_count_per_shank=1280,\n            col_spacing=32,\n            row_spacing=15,\n            white_spacing=0,\n            col_count_per_shank=2,\n            shank_count=4,\n            shank_spacing=250,\n        ),\n    }\n\n    probe_type = {\"probe_type\": probe_type}\n    electrode_layouts = build_electrode_layouts(\n        **{**neuropixels_probes_config[probe_type[\"probe_type\"]], **probe_type}\n    )\n    with ProbeType.connection.transaction:\n        ProbeType.insert1(probe_type, skip_duplicates=True)\n        ProbeType.Electrode.insert(electrode_layouts, skip_duplicates=True)\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.activate","title":"<code>activate(schema_name, *, create_schema=True, create_tables=True)</code>","text":"<p>Activates the <code>probe</code> schemas.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>A string containing the name of the probe schema.</p> required <code>create_schema</code> <code>bool</code> <p>If True, schema will be created in the database.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If True, tables related to the schema will be created in the database.</p> <code>True</code> <p>Dependencies:</p> Upstream tables <p>Session: A parent table to ProbeInsertion.</p> <p>Functions:</p> Source code in <code>element_array_ephys/probe.py</code> <pre><code>def activate(\n    schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n):\n\"\"\"Activates the `probe` schemas.\n\n    Args:\n        schema_name (str): A string containing the name of the probe schema.\n        create_schema (bool): If True, schema will be created in the database.\n        create_tables (bool): If True, tables related to the schema will be created in the database.\n\n    Dependencies:\n    Upstream tables:\n        Session: A parent table to ProbeInsertion.\n\n    Functions:\n    \"\"\"\n    schema.activate(\n        schema_name, create_schema=create_schema, create_tables=create_tables\n    )\n\n    # Add neuropixels probes\n    for probe_type in (\n        \"neuropixels 1.0 - 3A\",\n        \"neuropixels 1.0 - 3B\",\n        \"neuropixels UHD\",\n        \"neuropixels 2.0 - SS\",\n        \"neuropixels 2.0 - MS\",\n    ):\n        if not (ProbeType &amp; {\"probe_type\": probe_type}):\n            try:\n                ProbeType.create_neuropixels_probe(probe_type)\n            except dj.errors.DataJointError as e:\n                print(f\"Unable to create probe-type: {probe_type}\\n{str(e)}\")\n</code></pre>"},{"location":"api/element_array_ephys/probe/#element_array_ephys.probe.build_electrode_layouts","title":"<code>build_electrode_layouts(probe_type, site_count_per_shank, col_spacing=None, row_spacing=None, white_spacing=None, col_count_per_shank=1, shank_count=1, shank_spacing=None, y_origin='bottom')</code>","text":"<p>Builds electrode layouts.</p> <p>Parameters:</p> Name Type Description Default <code>probe_type</code> <code>str</code> <p>probe type (e.g., \"neuropixels 1.0 - 3A\").</p> required <code>site_count_per_shank</code> <code>int</code> <p>site count per shank.</p> required <code>col_spacing</code> <code>float</code> <p>(um) horizontal spacing between sites. Defaults to None (single column).</p> <code>None</code> <code>row_spacing</code> <code>float</code> <p>(um) vertical spacing between columns. Defaults to None (single row).</p> <code>None</code> <code>white_spacing</code> <code>float</code> <p>(um) offset spacing. Defaults to None.</p> <code>None</code> <code>col_count_per_shank</code> <code>int</code> <p>number of column per shank. Defaults to 1 (single column).</p> <code>1</code> <code>shank_count</code> <code>int</code> <p>number of shank. Defaults to 1 (single shank).</p> <code>1</code> <code>shank_spacing</code> <code>float</code> <p>(um) spacing between shanks. Defaults to None (single shank).</p> <code>None</code> <code>y_origin</code> <code>str</code> <p>{\"bottom\", \"top\"}. y value decrements if \"top\". Defaults to \"bottom\".</p> <code>'bottom'</code> Source code in <code>element_array_ephys/probe.py</code> <pre><code>def build_electrode_layouts(\n    probe_type: str,\n    site_count_per_shank: int,\n    col_spacing: float = None,\n    row_spacing: float = None,\n    white_spacing: float = None,\n    col_count_per_shank: int = 1,\n    shank_count: int = 1,\n    shank_spacing: float = None,\n    y_origin=\"bottom\",\n) -&gt; list[dict]:\n\n\"\"\"Builds electrode layouts.\n\n    Args:\n        probe_type (str): probe type (e.g., \"neuropixels 1.0 - 3A\").\n        site_count_per_shank (int): site count per shank.\n        col_spacing (float): (um) horizontal spacing between sites. Defaults to None (single column).\n        row_spacing (float): (um) vertical spacing between columns. Defaults to None (single row).\n        white_spacing (float): (um) offset spacing. Defaults to None.\n        col_count_per_shank (int): number of column per shank. Defaults to 1 (single column).\n        shank_count (int): number of shank. Defaults to 1 (single shank).\n        shank_spacing (float): (um) spacing between shanks. Defaults to None (single shank).\n        y_origin (str): {\"bottom\", \"top\"}. y value decrements if \"top\". Defaults to \"bottom\".\n    \"\"\"\n    row_count = int(site_count_per_shank / col_count_per_shank)\n    x_coords = np.tile(\n        np.arange(0, (col_spacing or 1) * col_count_per_shank, (col_spacing or 1)),\n        row_count,\n    )\n    y_coords = np.repeat(np.arange(row_count) * (row_spacing or 1), col_count_per_shank)\n\n    if white_spacing:\n        x_white_spaces = np.tile(\n            [white_spacing, white_spacing, 0, 0], int(row_count / 2)\n        )\n        x_coords = x_coords + x_white_spaces\n\n    shank_cols = np.tile(range(col_count_per_shank), row_count)\n    shank_rows = np.repeat(range(row_count), col_count_per_shank)\n\n    return [\n        {\n            \"probe_type\": probe_type,\n            \"electrode\": (site_count_per_shank * shank_no) + e_id,\n            \"shank\": shank_no,\n            \"shank_col\": c_id,\n            \"shank_row\": r_id,\n            \"x_coord\": x + (shank_no * (shank_spacing or 1)),\n            \"y_coord\": {\"top\": -y, \"bottom\": y}[y_origin],\n        }\n        for shank_no in range(shank_count)\n        for e_id, (c_id, r_id, x, y) in enumerate(\n            zip(shank_cols, shank_rows, x_coords, y_coords)\n        )\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/version/","title":"version.py","text":"<p>Package metadata.</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/","title":"nwb.py","text":""},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator","title":"<code>LFPDataChunkIterator</code>","text":"<p>         Bases: <code>GenericDataChunkIterator</code></p> <p>DataChunkIterator for LFP data that pulls data one channel at a time. Used when reading LFP data from the database (as opposed to directly from source files)</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>class LFPDataChunkIterator(GenericDataChunkIterator):\n\"\"\"\n    DataChunkIterator for LFP data that pulls data one channel at a time. Used when\n    reading LFP data from the database (as opposed to directly from source files)\n    \"\"\"\n\n    def __init__(self, lfp_electrodes_query, chunk_length: int = 10000):\n\"\"\"\n        Parameters\n        ----------\n        lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode\n        chunk_length: int, optional\n            Chunks are blocks of disk space where data are stored contiguously\n            and compressed\n        \"\"\"\n        self.lfp_electrodes_query = lfp_electrodes_query\n        self.electrodes = self.lfp_electrodes_query.fetch(\"electrode\")\n\n        first_record = (\n            self.lfp_electrodes_query &amp; dict(electrode=self.electrodes[0])\n        ).fetch1()\n\n        self.n_channels = len(self.electrodes)\n        self.n_tt = len(first_record[\"lfp\"])\n        self._dtype = first_record[\"lfp\"].dtype\n\n        super().__init__(buffer_shape=(self.n_tt, 1), chunk_shape=(chunk_length, 1))\n\n    def _get_data(self, selection):\n\n        electrode = self.electrodes[selection[1]][0]\n        return (self.lfp_electrodes_query &amp; dict(electrode=electrode)).fetch1(\"lfp\")[\n            selection[0], np.newaxis\n        ]\n\n    def _get_dtype(self):\n        return self._dtype\n\n    def _get_maxshape(self):\n        return self.n_tt, self.n_channels\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__","title":"<code>__init__(lfp_electrodes_query, chunk_length=10000)</code>","text":""},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.LFPDataChunkIterator.__init__--parameters","title":"Parameters","text":"<p>lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode</p> int, optional <p>Chunks are blocks of disk space where data are stored contiguously and compressed</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def __init__(self, lfp_electrodes_query, chunk_length: int = 10000):\n\"\"\"\n    Parameters\n    ----------\n    lfp_electrodes_query: element_array_ephys.ephys.LFP.Electrode\n    chunk_length: int, optional\n        Chunks are blocks of disk space where data are stored contiguously\n        and compressed\n    \"\"\"\n    self.lfp_electrodes_query = lfp_electrodes_query\n    self.electrodes = self.lfp_electrodes_query.fetch(\"electrode\")\n\n    first_record = (\n        self.lfp_electrodes_query &amp; dict(electrode=self.electrodes[0])\n    ).fetch1()\n\n    self.n_channels = len(self.electrodes)\n    self.n_tt = len(first_record[\"lfp\"])\n    self._dtype = first_record[\"lfp\"].dtype\n\n    super().__init__(buffer_shape=(self.n_tt, 1), chunk_shape=(chunk_length, 1))\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb","title":"<code>add_electrodes_to_nwb(session_key, nwbfile)</code>","text":"<p>Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including raw source data and LFP.</p> <p>ephys.InsertionLocation -&gt; ElectrodeGroup.location</p> <p>probe.Probe::probe -&gt; device.name probe.Probe::probe_comment -&gt; device.description probe.Probe::probe_type -&gt; device.manufacturer</p> <p>probe.ProbeType.Electrode::electrode -&gt; electrodes[\"id_in_probe\"] probe.ProbeType.Electrode::y_coord -&gt; electrodes[\"rel_y\"] probe.ProbeType.Electrode::x_coord -&gt; electrodes[\"rel_x\"] probe.ProbeType.Electrode::shank -&gt; electrodes[\"shank\"] probe.ProbeType.Electrode::shank_col -&gt; electrodes[\"shank_col\"] probe.ProbeType.Electrode::shank_row -&gt; electrodes[\"shank_row\"]</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_electrodes_to_nwb--parameters","title":"Parameters","text":"<p>session_key: dict nwbfile: pynwb.NWBFile</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_electrodes_to_nwb(session_key: dict, nwbfile: pynwb.NWBFile):\n\"\"\"\n    Add electrodes table to NWBFile. This is needed for any ElectricalSeries, including\n    raw source data and LFP.\n\n    ephys.InsertionLocation -&gt; ElectrodeGroup.location\n\n    probe.Probe::probe -&gt; device.name\n    probe.Probe::probe_comment -&gt; device.description\n    probe.Probe::probe_type -&gt; device.manufacturer\n\n    probe.ProbeType.Electrode::electrode -&gt; electrodes[\"id_in_probe\"]\n    probe.ProbeType.Electrode::y_coord -&gt; electrodes[\"rel_y\"]\n    probe.ProbeType.Electrode::x_coord -&gt; electrodes[\"rel_x\"]\n    probe.ProbeType.Electrode::shank -&gt; electrodes[\"shank\"]\n    probe.ProbeType.Electrode::shank_col -&gt; electrodes[\"shank_col\"]\n    probe.ProbeType.Electrode::shank_row -&gt; electrodes[\"shank_row\"]\n\n    Parameters\n    ----------\n    session_key: dict\n    nwbfile: pynwb.NWBFile\n    \"\"\"\n    electrodes_query = probe.ProbeType.Electrode * probe.ElectrodeConfig.Electrode\n\n    for additional_attribute in [\"shank_col\", \"shank_row\", \"shank\"]:\n        nwbfile.add_electrode_column(\n            name=electrodes_query.heading.attributes[additional_attribute].name,\n            description=electrodes_query.heading.attributes[\n                additional_attribute\n            ].comment,\n        )\n\n    nwbfile.add_electrode_column(\n        name=\"id_in_probe\",\n        description=\"electrode id within the probe\",\n    )\n\n    for this_probe in (ephys.ProbeInsertion * probe.Probe &amp; session_key).fetch(\n        as_dict=True\n    ):\n        insertion_record = (ephys.InsertionLocation &amp; this_probe).fetch(as_dict=True)\n        if len(insertion_record) == 1:\n            insert_location = json.dumps(\n                {\n                    k: v\n                    for k, v in insertion_record[0].items()\n                    if k not in ephys.InsertionLocation.primary_key\n                },\n                cls=DecimalEncoder,\n            )\n        elif len(insertion_record) == 0:\n            insert_location = \"unknown\"\n        else:\n            raise dj.DataJointError(\n                f\"Found multiple insertion locations for {this_probe}\"\n            )\n\n        device = nwbfile.create_device(\n            name=this_probe[\"probe\"],\n            description=this_probe.get(\"probe_comment\", None),\n            manufacturer=this_probe.get(\"probe_type\", None),\n        )\n        shank_ids = set((probe.ProbeType.Electrode &amp; this_probe).fetch(\"shank\"))\n        for shank_id in shank_ids:\n            electrode_group = nwbfile.create_electrode_group(\n                name=f\"probe{this_probe['probe']}_shank{shank_id}\",\n                description=f\"probe{this_probe['probe']}_shank{shank_id}\",\n                location=insert_location,\n                device=device,\n            )\n\n            electrodes_query = (\n                probe.ProbeType.Electrode &amp; this_probe &amp; dict(shank=shank_id)\n            ).fetch(as_dict=True)\n            for electrode in electrodes_query:\n                nwbfile.add_electrode(\n                    group=electrode_group,\n                    filtering=\"unknown\",\n                    imp=-1.0,\n                    x=np.nan,\n                    y=np.nan,\n                    z=np.nan,\n                    rel_x=electrode[\"x_coord\"],\n                    rel_y=electrode[\"y_coord\"],\n                    rel_z=np.nan,\n                    shank_col=electrode[\"shank_col\"],\n                    shank_row=electrode[\"shank_row\"],\n                    location=\"unknown\",\n                    id_in_probe=electrode[\"electrode\"],\n                    shank=electrode[\"shank\"],\n                )\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb","title":"<code>add_ephys_lfp_from_dj_to_nwb(session_key, nwbfile)</code>","text":"<p>Read LFP data from the data in element-aray-ephys</p> <p>ephys.LFP.Electrode::lfp -&gt;     processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"                                                ].data ephys.LFP::lfp_time_stamps -&gt;     processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"                                                ].timestamps</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_dj_to_nwb--parameters","title":"Parameters","text":"<p>session_key: dict nwbfile: NWBFile</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_lfp_from_dj_to_nwb(session_key: dict, nwbfile: pynwb.NWBFile):\n\"\"\"\n    Read LFP data from the data in element-aray-ephys\n\n    ephys.LFP.Electrode::lfp -&gt;\n        processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"\n                                                   ].data\n    ephys.LFP::lfp_time_stamps -&gt;\n        processing[\"ecephys\"].lfp.electrical_series[\"ElectricalSeries{insertion_number}\"\n                                                   ].timestamps\n\n    Parameters\n    ----------\n    session_key: dict\n    nwbfile: NWBFile\n    \"\"\"\n\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    ecephys_module = get_module(\n        nwbfile, name=\"ecephys\", description=\"preprocessed ephys data\"\n    )\n\n    nwb_lfp = pynwb.ecephys.LFP(name=\"LFP\")\n    ecephys_module.add(nwb_lfp)\n\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    for lfp_record in (ephys.LFP &amp; session_key).fetch(as_dict=True):\n        probe_id = (ephys.ProbeInsertion &amp; lfp_record).fetch1(\"probe\")\n\n        lfp_electrodes_query = ephys.LFP.Electrode &amp; lfp_record\n        lfp_data = LFPDataChunkIterator(lfp_electrodes_query)\n\n        nwb_lfp.create_electrical_series(\n            name=f\"ElectricalSeries{lfp_record['insertion_number']}\",\n            description=f\"LFP from probe {probe_id}\",\n            data=H5DataIO(lfp_data, compression=True),\n            timestamps=lfp_record[\"lfp_time_stamps\"],\n            electrodes=nwbfile.create_electrode_table_region(\n                name=\"electrodes\",\n                description=\"electrodes used for LFP\",\n                region=[\n                    mapping[(probe_id, x)]\n                    for x in lfp_electrodes_query.fetch(\"electrode\")\n                ],\n            ),\n        )\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb","title":"<code>add_ephys_lfp_from_source_to_nwb(session_key, ephys_root_data_dir, nwbfile, end_frame=None)</code>","text":"<p>Read the LFP data from the source file. Currently, only works for SpikeGLX data.</p> <p>ephys.EphysRecording::recording_datetime -&gt; acquisition</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_lfp_from_source_to_nwb--parameters","title":"Parameters","text":"<p>session_key: dict nwbfile: pynwb.NWBFile</p> int, optional <p>use for small test conversions</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_lfp_from_source_to_nwb(\n    session_key: dict, ephys_root_data_dir, nwbfile: pynwb.NWBFile, end_frame=None\n):\n\"\"\"\n    Read the LFP data from the source file. Currently, only works for SpikeGLX data.\n\n    ephys.EphysRecording::recording_datetime -&gt; acquisition\n\n    Parameters\n    ----------\n    session_key: dict\n    nwbfile: pynwb.NWBFile\n    end_frame: int, optional\n        use for small test conversions\n\n    \"\"\"\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    ecephys_module = get_module(\n        nwbfile, name=\"ecephys\", description=\"preprocessed ephys data\"\n    )\n\n    lfp = pynwb.ecephys.LFP()\n    ecephys_module.add(lfp)\n\n    for ephys_recording_record in (ephys.EphysRecording &amp; session_key).fetch(\n        as_dict=True\n    ):\n        probe_id = (ephys.ProbeInsertion() &amp; ephys_recording_record).fetch1(\"probe\")\n\n        relative_path = (\n            ephys.EphysRecording.EphysFile &amp; ephys_recording_record\n        ).fetch1(\"file_path\")\n        relative_path = relative_path.replace(\"\\\\\", \"/\")\n        file_path = find_full_path(ephys_root_data_dir, relative_path)\n\n        if ephys_recording_record[\"acq_software\"] == \"SpikeGLX\":\n            extractor = extractors.read_spikeglx(os.path.split(file_path)[0], \"imec.lf\")\n        else:\n            raise ValueError(\n                \"unsupported acq_software type:\"\n                + f\"{ephys_recording_record['acq_software']}\"\n            )\n\n        if end_frame is not None:\n            extractor = extractor.frame_slice(0, end_frame)\n\n        recording_channels_by_id = (\n            probe.ElectrodeConfig.Electrode() &amp; ephys_recording_record\n        ).fetch(\"electrode\")\n\n        conversion_kwargs = gains_helper(extractor.get_channel_gains())\n\n        lfp.add_electrical_series(\n            pynwb.ecephys.ElectricalSeries(\n                name=f\"ElectricalSeries{ephys_recording_record['insertion_number']}\",\n                description=f\"LFP from probe {probe_id}\",\n                data=SpikeInterfaceRecordingDataChunkIterator(extractor),\n                rate=extractor.get_sampling_frequency(),\n                starting_time=(\n                    ephys_recording_record[\"recording_datetime\"]\n                    - ephys_recording_record[\"session_datetime\"]\n                ).total_seconds(),\n                electrodes=nwbfile.create_electrode_table_region(\n                    region=[mapping[(probe_id, x)] for x in recording_channels_by_id],\n                    name=\"electrodes\",\n                    description=\"recorded electrodes\",\n                ),\n                **conversion_kwargs,\n            )\n        )\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb","title":"<code>add_ephys_recording_to_nwb(session_key, ephys_root_data_dir, nwbfile, end_frame=None)</code>","text":"<p>Read voltage data directly from source files and iteratively transfer them to the NWB file. Automatically applies lossless compression to the data, so the final file might be smaller than the original, without data loss. Currently supports Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface to read the data.</p> <p>source data -&gt; acquisition[\"ElectricalSeries\"]</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_recording_to_nwb--parameters","title":"Parameters","text":"dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional <p>Used for small test conversions</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_recording_to_nwb(\n    session_key: dict,\n    ephys_root_data_dir: str,\n    nwbfile: pynwb.NWBFile,\n    end_frame: int = None,\n):\n\"\"\"\n    Read voltage data directly from source files and iteratively transfer them to the\n    NWB file. Automatically applies lossless compression to the data, so the final file\n    might be smaller than the original, without data loss. Currently supports\n    Neuropixels data acquired with SpikeGLX or Open Ephys, and relies on SpikeInterface\n    to read the data.\n\n    source data -&gt; acquisition[\"ElectricalSeries\"]\n\n    Parameters\n    ----------\n    session_key: dict ephys_root_data_dir: str nwbfile: NWBFile end_frame: int, optional\n        Used for small test conversions\n    \"\"\"\n\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    for ephys_recording_record in (ephys.EphysRecording &amp; session_key).fetch(\n        as_dict=True\n    ):\n        probe_id = (ephys.ProbeInsertion() &amp; ephys_recording_record).fetch1(\"probe\")\n\n        relative_path = (\n            ephys.EphysRecording.EphysFile &amp; ephys_recording_record\n        ).fetch1(\"file_path\")\n        relative_path = relative_path.replace(\"\\\\\", \"/\")\n        file_path = find_full_path(ephys_root_data_dir, relative_path)\n\n        if ephys_recording_record[\"acq_software\"] == \"SpikeGLX\":\n            extractor = extractors.read_spikeglx(os.path.split(file_path)[0], \"imec.ap\")\n        elif ephys_recording_record[\"acq_software\"] == \"OpenEphys\":\n            extractor = extractors.read_openephys(file_path, stream_id=\"0\")\n        else:\n            raise ValueError(\n                f\"unsupported acq_software type: {ephys_recording_record['acq_software']}\"\n            )\n\n        conversion_kwargs = gains_helper(extractor.get_channel_gains())\n\n        if end_frame is not None:\n            extractor = extractor.frame_slice(0, end_frame)\n\n        recording_channels_by_id = (\n            probe.ElectrodeConfig.Electrode() &amp; ephys_recording_record\n        ).fetch(\"electrode\")\n\n        nwbfile.add_acquisition(\n            pynwb.ecephys.ElectricalSeries(\n                name=f\"ElectricalSeries{ephys_recording_record['insertion_number']}\",\n                description=str(ephys_recording_record),\n                data=SpikeInterfaceRecordingDataChunkIterator(extractor),\n                rate=ephys_recording_record[\"sampling_rate\"],\n                starting_time=(\n                    ephys_recording_record[\"recording_datetime\"]\n                    - ephys_recording_record[\"session_datetime\"]\n                ).total_seconds(),\n                electrodes=nwbfile.create_electrode_table_region(\n                    region=[mapping[(probe_id, x)] for x in recording_channels_by_id],\n                    name=\"electrodes\",\n                    description=\"recorded electrodes\",\n                ),\n                **conversion_kwargs,\n            )\n        )\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb","title":"<code>add_ephys_units_to_nwb(session_key, nwbfile, primary_clustering_paramset_idx=0)</code>","text":"<p>Add spiking data to NWBFile.</p> <p>In NWB, spiking data is stored in a Units table. The primary Units table is stored at /units. The spiking data in /units is generally the data used in downstream analysis. Only a single Units table can be stored at /units. Other Units tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of Units tables can be stored in this ProcessingModule as long as they have different names, and these Units tables can store intermediate processing steps or alternative curations.</p> <p>Use <code>primary_clustering_paramset_idx</code> to indicate which clustering is primary. All others will be stored in /processing/ecephys/.</p> <p>ephys.CuratedClustering.Unit::unit -&gt; units.id ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]</p> <p>ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.add_ephys_units_to_nwb--parameters","title":"Parameters","text":"<p>session_key: dict nwbfile: pynwb.NWBFile primary_clustering_paramset_idx: int, optional</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def add_ephys_units_to_nwb(\n    session_key: dict, nwbfile: pynwb.NWBFile, primary_clustering_paramset_idx: int = 0\n):\n\"\"\"\n    Add spiking data to NWBFile.\n\n    In NWB, spiking data is stored in a Units table. The primary Units table is\n    stored at /units. The spiking data in /units is generally the data used in\n    downstream analysis. Only a single Units table can be stored at /units. Other Units\n    tables can be stored in a ProcessingModule at /processing/ecephys/. Any number of\n    Units tables can be stored in this ProcessingModule as long as they have different\n    names, and these Units tables can store intermediate processing steps or\n    alternative curations.\n\n    Use `primary_clustering_paramset_idx` to indicate which clustering is primary. All\n    others will be stored in /processing/ecephys/.\n\n    ephys.CuratedClustering.Unit::unit -&gt; units.id\n    ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"]\n    ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"]\n    ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]\n\n    ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]\n\n    Parameters\n    ----------\n    session_key: dict\n    nwbfile: pynwb.NWBFile\n    primary_clustering_paramset_idx: int, optional\n    \"\"\"\n\n    if not ephys.ClusteringTask &amp; session_key:\n        warnings.warn(f\"No unit data exists for session:{session_key}\")\n        return\n\n    if nwbfile.electrodes is None:\n        add_electrodes_to_nwb(session_key, nwbfile)\n\n    for paramset_record in (\n        ephys.ClusteringParamSet &amp; ephys.CuratedClustering &amp; session_key\n    ).fetch(\"paramset_idx\", \"clustering_method\", \"paramset_desc\", as_dict=True):\n        if paramset_record[\"paramset_idx\"] == primary_clustering_paramset_idx:\n            units_table = create_units_table(\n                session_key,\n                nwbfile,\n                paramset_record,\n                desc=paramset_record[\"paramset_desc\"],\n            )\n            nwbfile.units = units_table\n        else:\n            name = f\"units_{paramset_record['clustering_method']}\"\n            units_table = create_units_table(\n                session_key,\n                nwbfile,\n                paramset_record,\n                name=name,\n                desc=paramset_record[\"paramset_desc\"],\n            )\n            ecephys_module = get_module(nwbfile, \"ecephys\")\n            ecephys_module.add(units_table)\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table","title":"<code>create_units_table(session_key, nwbfile, paramset_record, name='units', desc='data on spiking units')</code>","text":"<p>ephys.CuratedClustering.Unit::unit -&gt; units.id ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"] ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"] ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]</p> <p>ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.create_units_table--parameters","title":"Parameters","text":"<p>session_key: dict nwbfile: pynwb.NWBFile paramset_record: int</p> str, optional <p>default=\"units\"</p> str, optional <p>default=\"data on spiking units\"</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def create_units_table(\n    session_key: dict,\n    nwbfile: pynwb.NWBFile,\n    paramset_record,\n    name=\"units\",\n    desc=\"data on spiking units\",\n):\n\"\"\"\n\n    ephys.CuratedClustering.Unit::unit -&gt; units.id\n    ephys.CuratedClustering.Unit::spike_times -&gt; units[\"spike_times\"]\n    ephys.CuratedClustering.Unit::spike_depths -&gt; units[\"spike_depths\"]\n    ephys.CuratedClustering.Unit::cluster_quality_label -&gt; units[\"cluster_quality_label\"]\n\n    ephys.WaveformSet.PeakWaveform::peak_electrode_waveform -&gt; units[\"waveform_mean\"]\n\n    Parameters\n    ----------\n    session_key: dict\n    nwbfile: pynwb.NWBFile\n    paramset_record: int\n    name: str, optional\n        default=\"units\"\n    desc: str, optional\n        default=\"data on spiking units\"\n    \"\"\"\n\n    # electrode id mapping\n    mapping = get_electrodes_mapping(nwbfile.electrodes)\n\n    units_table = pynwb.misc.Units(name=name, description=desc)\n    # add additional columns to the units table\n    for additional_attribute in [\"cluster_quality_label\", \"spike_depths\"]:\n        # The `index` parameter indicates whether the column is a \"ragged array,\" i.e.\n        # whether each row of this column is a vector with potentially different lengths\n        # for each row.\n        units_table.add_column(\n            name=additional_attribute,\n            description=ephys.CuratedClustering.Unit.heading.attributes[\n                additional_attribute\n            ].comment,\n            index=additional_attribute == \"spike_depths\",\n        )\n\n    clustering_query = (\n        ephys.EphysRecording * ephys.ClusteringTask &amp; session_key &amp; paramset_record\n    )\n\n    for unit in tqdm(\n        (ephys.CuratedClustering.Unit &amp; clustering_query.proj()).fetch(as_dict=True),\n        desc=f\"creating units table for paramset {paramset_record['paramset_idx']}\",\n    ):\n\n        probe_id, shank_num = (\n            ephys.ProbeInsertion\n            * ephys.CuratedClustering.Unit\n            * probe.ProbeType.Electrode\n            &amp; dict(\n                (k, unit[k])\n                for k in unit.keys()  # excess keys caused errs\n                if k not in [\"spike_times\", \"spike_sites\", \"spike_depths\"]\n            )\n        ).fetch1(\"probe\", \"shank\")\n\n        waveform_mean = (\n            ephys.WaveformSet.PeakWaveform() &amp; clustering_query &amp; unit\n        ).fetch1(\"peak_electrode_waveform\")\n\n        units_table.add_row(\n            id=unit[\"unit\"],\n            electrodes=[mapping[(probe_id, unit[\"electrode\"])]],\n            electrode_group=nwbfile.electrode_groups[\n                f\"probe{probe_id}_shank{shank_num}\"\n            ],\n            cluster_quality_label=unit[\"cluster_quality_label\"],\n            spike_times=unit[\"spike_times\"],\n            spike_depths=unit[\"spike_depths\"],\n            waveform_mean=waveform_mean,\n        )\n\n    return units_table\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb","title":"<code>ecephys_session_to_nwb(session_key, raw=True, spikes=True, lfp='source', end_frame=None, lab_key=None, project_key=None, protocol_key=None, nwbfile_kwargs=None)</code>","text":"<p>Main function for converting ephys data to NWB</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.ecephys_session_to_nwb--parameters","title":"Parameters","text":"<p>session_key: dict</p> bool <p>Whether to include the raw data from source. SpikeGLX &amp; OpenEphys are supported</p> bool <p>Whether to include CuratedClustering</p> lfp <p>\"dj\" - read LFP data from ephys.LFP \"source\" - read LFP data from source (SpikeGLX supported) False - do not convert LFP</p> int, optional <p>Used to create small test conversions where large datasets are truncated.</p> <p>lab_key, project_key, and protocol_key: dictionaries to look up optional metadata</p> dict, optional <ul> <li>If element-session is not being used, this argument is required and must be a   dictionary containing 'session_description' (str), 'identifier' (str), and   'session_start_time' (datetime), the required minimal data for instantiating   an NWBFile object.</li> <li>If element-session is being used, this argument can optionally be used to   overwrite NWBFile fields.</li> </ul> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def ecephys_session_to_nwb(\n    session_key,\n    raw=True,\n    spikes=True,\n    lfp=\"source\",\n    end_frame=None,\n    lab_key=None,\n    project_key=None,\n    protocol_key=None,\n    nwbfile_kwargs=None,\n):\n\"\"\"\n    Main function for converting ephys data to NWB\n\n    Parameters\n    ----------\n    session_key: dict\n    raw: bool\n        Whether to include the raw data from source. SpikeGLX &amp; OpenEphys are supported\n    spikes: bool\n        Whether to include CuratedClustering\n    lfp:\n        \"dj\" - read LFP data from ephys.LFP\n        \"source\" - read LFP data from source (SpikeGLX supported)\n        False - do not convert LFP\n    end_frame: int, optional\n        Used to create small test conversions where large datasets are truncated.\n    lab_key, project_key, and protocol_key: dictionaries to look up optional metadata\n    nwbfile_kwargs: dict, optional\n        - If element-session is not being used, this argument is required and must be a\n          dictionary containing 'session_description' (str), 'identifier' (str), and\n          'session_start_time' (datetime), the required minimal data for instantiating\n          an NWBFile object.\n        - If element-session is being used, this argument can optionally be used to\n          overwrite NWBFile fields.\n    \"\"\"\n\n    session_to_nwb = getattr(ephys._linking_module, \"session_to_nwb\", False)\n\n    if session_to_nwb:\n        nwbfile = session_to_nwb(\n            session_key,\n            lab_key=lab_key,\n            project_key=project_key,\n            protocol_key=protocol_key,\n            additional_nwbfile_kwargs=nwbfile_kwargs,\n        )\n    else:\n        nwbfile = pynwb.NWBFile(**nwbfile_kwargs)\n\n    ephys_root_data_dir = ephys.get_ephys_root_data_dir()\n\n    if raw:\n        add_ephys_recording_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    if spikes:\n        add_ephys_units_to_nwb(session_key, nwbfile)\n\n    if lfp == \"dj\":\n        add_ephys_lfp_from_dj_to_nwb(session_key, nwbfile)\n\n    if lfp == \"source\":\n        add_ephys_lfp_from_source_to_nwb(\n            session_key,\n            ephys_root_data_dir=ephys_root_data_dir,\n            nwbfile=nwbfile,\n            end_frame=end_frame,\n        )\n\n    return nwbfile\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper","title":"<code>gains_helper(gains)</code>","text":"<p>This handles three different cases for gains: 1. gains are all 1. In this case, return conversion=1e-6, which applies to all channels and converts from microvolts to volts. 2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this gain to all channels and convert units to volts. 3. Gains are different for different channels. In this case use the <code>channel_conversion</code> field in addition to the <code>conversion</code> field so that each channel can be converted to volts using its own individual gain.</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper--parameters","title":"Parameters","text":"<p>gains: np.ndarray</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.gains_helper--returns","title":"Returns","text":"<p>dict     conversion : float     channel_conversion : np.ndarray</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def gains_helper(gains):\n\"\"\"\n    This handles three different cases for gains:\n    1. gains are all 1. In this case, return conversion=1e-6, which applies to all\n    channels and converts from microvolts to volts.\n    2. Gains are all equal, but not 1. In this case, multiply this by 1e-6 to apply this\n    gain to all channels and convert units to volts.\n    3. Gains are different for different channels. In this case use the\n    `channel_conversion` field in addition to the `conversion` field so that each\n    channel can be converted to volts using its own individual gain.\n\n    Parameters\n    ----------\n    gains: np.ndarray\n\n    Returns\n    -------\n    dict\n        conversion : float\n        channel_conversion : np.ndarray\n\n    \"\"\"\n    if all(x == 1 for x in gains):\n        return dict(conversion=1e-6, channel_conversion=None)\n    if all(x == gains[0] for x in gains):\n        return dict(conversion=1e-6 * gains[0], channel_conversion=None)\n    return dict(conversion=1e-6, channel_conversion=gains)\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping","title":"<code>get_electrodes_mapping(electrodes)</code>","text":"<p>Create a mapping from the probe and electrode id to the row number of the electrodes table. This is used in the construction of the DynamicTableRegion that indicates what rows of the electrodes table correspond to the data in an ElectricalSeries.</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping--parameters","title":"Parameters","text":"<p>electrodes: hdmf.common.table.DynamicTable</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.get_electrodes_mapping--returns","title":"Returns","text":"<p>dict</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def get_electrodes_mapping(electrodes):\n\"\"\"\n    Create a mapping from the probe and electrode id to the row number of the electrodes\n    table. This is used in the construction of the DynamicTableRegion that indicates\n    what rows of the electrodes table correspond to the data in an ElectricalSeries.\n\n    Parameters\n    ----------\n    electrodes: hdmf.common.table.DynamicTable\n\n    Returns\n    -------\n    dict\n\n    \"\"\"\n    return {\n        (\n            electrodes[\"group\"][idx].device.name,\n            electrodes[\"id_in_probe\"][idx],\n        ): idx\n        for idx in range(len(electrodes))\n    }\n</code></pre>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb","title":"<code>write_nwb(nwbfile, fname, check_read=True)</code>","text":"<p>Export NWBFile</p>"},{"location":"api/element_array_ephys/export/nwb/nwb/#element_array_ephys.export.nwb.nwb.write_nwb--parameters","title":"Parameters","text":"<p>nwbfile: pynwb.NWBFile fname: str Absolute path including <code>*.nwb</code> extension.</p> bool <p>If True, PyNWB will try to read the produced NWB file and ensure that it can be read.</p> Source code in <code>element_array_ephys/export/nwb/nwb.py</code> <pre><code>def write_nwb(nwbfile, fname, check_read=True):\n\"\"\"\n    Export NWBFile\n\n    Parameters\n    ----------\n    nwbfile: pynwb.NWBFile\n    fname: str Absolute path including `*.nwb` extension.\n    check_read: bool\n        If True, PyNWB will try to read the produced NWB file and ensure that it can be\n        read.\n    \"\"\"\n    with pynwb.NWBHDF5IO(fname, \"w\") as io:\n        io.write(nwbfile)\n\n    if check_read:\n        with pynwb.NWBHDF5IO(fname, \"r\") as io:\n            io.read()\n</code></pre>"},{"location":"api/element_array_ephys/plotting/corr/","title":"corr.py","text":"<p>Code adapted from International Brain Laboratory, T. (2021). ibllib [Computer software]. https://github.com/int-brain-lab/ibllib</p>"},{"location":"api/element_array_ephys/plotting/corr/#element_array_ephys.plotting.corr.acorr","title":"<code>acorr(spike_times, bin_size, window_size)</code>","text":"<p>Compute the auto-correlogram of a unit.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike times in seconds.</p> required <code>bin_size</code> <code>float</code> <p>Size of the time bin in seconds.</p> required <code>window_size</code> <code>int</code> <p>Size of the correlogram window in seconds.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: auto-correlogram array (winsize_samples,)</p> Source code in <code>element_array_ephys/plotting/corr.py</code> <pre><code>def acorr(spike_times: np.ndarray, bin_size: float, window_size: int) -&gt; np.ndarray:\n\"\"\"Compute the auto-correlogram of a unit.\n\n    Args:\n        spike_times (np.ndarray): Spike times in seconds.\n        bin_size (float, optional): Size of the time bin in seconds.\n        window_size (int, optional): Size of the correlogram window in seconds.\n\n    Returns:\n        np.ndarray: auto-correlogram array (winsize_samples,)\n    \"\"\"\n    xc = xcorr(\n        spike_times,\n        np.zeros_like(spike_times, dtype=np.int32),\n        bin_size=bin_size,\n        window_size=window_size,\n    )\n    return xc[0, 0, :]\n</code></pre>"},{"location":"api/element_array_ephys/plotting/corr/#element_array_ephys.plotting.corr.xcorr","title":"<code>xcorr(spike_times, spike_clusters, bin_size, window_size)</code>","text":"<p>Compute all pairwise cross-correlograms among the clusters appearing in <code>spike_clusters</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike times in seconds.</p> required <code>spike_clusters</code> <code>np.ndarray</code> <p>Spike-cluster mapping.</p> required <code>bin_size</code> <code>float</code> <p>Size of the time bin in seconds.</p> required <code>window_size</code> <code>int</code> <p>Size of the correlogram window in seconds.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: cross-correlogram array</p> Source code in <code>element_array_ephys/plotting/corr.py</code> <pre><code>def xcorr(\n    spike_times: np.ndarray,\n    spike_clusters: np.ndarray,\n    bin_size: float,\n    window_size: int,\n) -&gt; np.ndarray:\n\"\"\"Compute all pairwise cross-correlograms among the clusters appearing in `spike_clusters`.\n\n    Args:\n        spike_times (np.ndarray): Spike times in seconds.\n        spike_clusters (np.ndarray): Spike-cluster mapping.\n        bin_size (float): Size of the time bin in seconds.\n        window_size (int): Size of the correlogram window in seconds.\n\n    Returns:\n         np.ndarray: cross-correlogram array\n    \"\"\"\n    assert np.all(np.diff(spike_times) &gt;= 0), \"The spike times must be increasing.\"\n    assert spike_times.ndim == 1\n    assert spike_times.shape == spike_clusters.shape\n\n    # Find `binsize`.\n    bin_size = np.clip(bin_size, 1e-5, 1e5)  # in seconds\n\n    # Find `winsize_bins`.\n    window_size = np.clip(window_size, 1e-5, 1e5)  # in seconds\n    winsize_bins = 2 * int(0.5 * window_size / bin_size) + 1\n\n    # Take the cluster order into account.\n    clusters = np.unique(spike_clusters)\n    n_clusters = len(clusters)\n\n    # Like spike_clusters, but with 0..n_clusters-1 indices.\n    spike_clusters_i = _index_of(spike_clusters, clusters)\n\n    # Shift between the two copies of the spike trains.\n    shift = 1\n\n    # At a given shift, the mask precises which spikes have matching spikes\n    # within the correlogram time window.\n    mask = np.ones_like(spike_times, dtype=bool)\n\n    correlograms = _create_correlograms_array(n_clusters, winsize_bins)\n\n    # The loop continues as long as there is at least one spike with\n    # a matching spike.\n    while mask[:-shift].any():\n        # Interval between spike i and spike i+shift.\n        spike_diff = _diff_shifted(spike_times, shift)\n\n        # Binarize the delays between spike i and spike i+shift.\n        spike_diff_b = np.round(spike_diff / bin_size).astype(np.int64)\n\n        # Spikes with no matching spikes are masked.\n        mask[:-shift][spike_diff_b &gt; (winsize_bins / 2)] = False\n\n        # Cache the masked spike delays.\n        m = mask[:-shift].copy()\n        d = spike_diff_b[m]\n\n        # Find the indices in the raveled correlograms array that need\n        # to be incremented, taking into account the spike clusters.\n        indices = np.ravel_multi_index(\n            (spike_clusters_i[:-shift][m], spike_clusters_i[+shift:][m], d),\n            correlograms.shape,\n        )\n\n        # Increment the matching spikes in the correlograms array.\n        _increment(correlograms.ravel(), indices)\n\n        shift += 1\n\n    return _symmetrize_correlograms(correlograms)\n</code></pre>"},{"location":"api/element_array_ephys/plotting/probe_level/","title":"probe_level.py","text":""},{"location":"api/element_array_ephys/plotting/probe_level/#element_array_ephys.plotting.probe_level.plot_driftmap","title":"<code>plot_driftmap(spike_times, spike_depths, colormap='gist_heat_r')</code>","text":"<p>Plot drift map of unit activity for all units recorded in a given shank of a probe.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike timestamps in seconds.</p> required <code>spike_depths</code> <code>np.ndarray</code> <p>The depth of the electrode where the spike was found in \u03bcm.</p> required <code>colormap</code> <code>str</code> <p>Colormap. Defaults to \"gist_heat_r\".</p> <code>'gist_heat_r'</code> <p>Returns:</p> Type Description <code>matplotlib.figure.Figure</code> <p>matplotlib.figure.Figure: matplotlib figure object for showing population activity for all units over time (x-axis in seconds) according to the spatial depths of the spikes (y-axis in \u03bcm).</p> Source code in <code>element_array_ephys/plotting/probe_level.py</code> <pre><code>def plot_driftmap(\n    spike_times: np.ndarray, spike_depths: np.ndarray, colormap=\"gist_heat_r\"\n) -&gt; matplotlib.figure.Figure:\n\"\"\"Plot drift map of unit activity for all units recorded in a given shank of a probe.\n\n    Args:\n        spike_times (np.ndarray): Spike timestamps in seconds.\n        spike_depths (np.ndarray): The depth of the electrode where the spike was found in \u03bcm.\n        colormap (str, optional): Colormap. Defaults to \"gist_heat_r\".\n\n    Returns:\n        matplotlib.figure.Figure: matplotlib figure object for showing population activity for all units over time (x-axis in seconds) according to the spatial depths of the spikes (y-axis in \u03bcm).\n    \"\"\"\n\n    spike_times = np.hstack(spike_times)\n    spike_depths = np.hstack(spike_depths)\n\n    # Time-depth 2D histogram\n    time_bin_count = 1000\n    depth_bin_count = 200\n\n    spike_bins = np.linspace(0, spike_times.max(), time_bin_count)\n    depth_bins = np.linspace(0, np.nanmax(spike_depths), depth_bin_count)\n\n    spk_count, spk_edges, depth_edges = np.histogram2d(\n        spike_times, spike_depths, bins=[spike_bins, depth_bins]\n    )\n    spk_rates = spk_count / np.mean(np.diff(spike_bins))\n    spk_edges = spk_edges[:-1]\n    depth_edges = depth_edges[:-1]\n\n    # Canvas setup\n    fig = plt.figure(figsize=(12, 5), dpi=200)\n    grid = plt.GridSpec(15, 12)\n\n    ax_cbar = plt.subplot(grid[0, 0:10])\n    ax_driftmap = plt.subplot(grid[2:, 0:10])\n    ax_spkcount = plt.subplot(grid[2:, 10:])\n\n    # Plot main\n    im = ax_driftmap.imshow(\n        spk_rates.T,\n        aspect=\"auto\",\n        cmap=colormap,\n        extent=[spike_bins[0], spike_bins[-1], depth_bins[-1], depth_bins[0]],\n    )\n    # Cosmetic\n    ax_driftmap.invert_yaxis()\n    ax_driftmap.set(\n        xlabel=\"Time (s)\",\n        ylabel=\"Distance from the probe tip ($\\mu$m)\",\n        ylim=[depth_edges[0], depth_edges[-1]],\n    )\n\n    # Colorbar for firing rates\n    cb = fig.colorbar(im, cax=ax_cbar, orientation=\"horizontal\")\n    cb.outline.set_visible(False)\n    cb.ax.xaxis.tick_top()\n    cb.set_label(\"Firing rate (Hz)\")\n    cb.ax.xaxis.set_label_position(\"top\")\n\n    # Plot spike count\n    ax_spkcount.plot(spk_count.sum(axis=0) / 10e3, depth_edges, \"k\")\n    ax_spkcount.set_xlabel(\"Spike count (x$10^3$)\")\n    ax_spkcount.set_yticks([])\n    ax_spkcount.set_ylim(depth_edges[0], depth_edges[-1])\n    sns.despine()\n\n    return fig\n</code></pre>"},{"location":"api/element_array_ephys/plotting/probe_level/#element_array_ephys.plotting.probe_level.plot_raster","title":"<code>plot_raster(units, spike_times)</code>","text":"<p>Population raster plots.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>np.ndarray</code> <p>Recorded units.</p> required <code>spike_times</code> <code>np.ndarray</code> <p>Spike timestamps in seconds.</p> required <p>Returns:</p> Type Description <code>matplotlib.figure.Figure</code> <p>matplotlib.figure.Figure: matplotlib figure object showing spikes rasters over time (x-axis in seconds). Each row (y-axis) indicates a single unit.</p> Source code in <code>element_array_ephys/plotting/probe_level.py</code> <pre><code>def plot_raster(units: np.ndarray, spike_times: np.ndarray) -&gt; matplotlib.figure.Figure:\n\"\"\"Population raster plots.\n\n    Args:\n        units (np.ndarray): Recorded units.\n        spike_times (np.ndarray): Spike timestamps in seconds.\n\n    Returns:\n        matplotlib.figure.Figure: matplotlib figure object showing spikes rasters over time (x-axis in seconds). Each row (y-axis) indicates a single unit.\n    \"\"\"\n    units = np.arange(1, len(units) + 1)\n    x = np.hstack(spike_times)\n    y = np.hstack([np.full_like(s, u) for u, s in zip(units, spike_times)])\n    fig, ax = plt.subplots(1, 1, figsize=(32, 8), dpi=100)\n    ax.plot(x, y, \"|\")\n    ax.set(\n        xlabel=\"Time (s)\",\n        ylabel=\"Unit\",\n        xlim=[0 - 0.5, x[-1] + 0.5],\n        ylim=(1, len(units)),\n    )\n    sns.despine()\n    fig.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"api/element_array_ephys/plotting/qc/","title":"qc.py","text":""},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs","title":"<code>QualityMetricFigs</code>","text":"<p>         Bases: <code>object</code></p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>class QualityMetricFigs(object):\n    def __init__(\n        self,\n        ephys: types.ModuleType,\n        key: dict = None,\n        scale: float = 1,\n        fig_width=800,\n        amplitude_cutoff_maximum: float = None,\n        presence_ratio_minimum: float = None,\n        isi_violations_maximum: float = None,\n        dark_mode: bool = False,\n    ):\n\"\"\"Initialize QC metric class\n\n        Args:\n            ephys (module): datajoint module with a QualityMetric table\n            key (dict, optional): key from ephys.QualityMetric table. Defaults to None.\n            scale (float, optional): Scale at which to render figure. Defaults to 1.4.\n            fig_width (int, optional): Figure width in pixels. Defaults to 800.\n            amplitude_cutoff_maximum (float, optional): Cutoff for unit ampliude in\n                visualizations. Defaults to None.\n            presence_ratio_minimum (float, optional): Cutoff for presence ratio in\n                visualizations. Defaults to None.\n            isi_violations_maximum (float, optional): Cutoff for isi violations in\n                visualizations. Defaults to None.\n            dark_mode (bool, optional): Set background to black, foreground white.\n                Default False, black on white.\n        \"\"\"\n        self._ephys = ephys\n        self._key = key\n        self._scale = scale\n        self._plots = {}  # Empty default to defer set to dict property below\n        self._fig_width = fig_width\n        self._amplitude_cutoff_max = amplitude_cutoff_maximum\n        self._presence_ratio_min = presence_ratio_minimum\n        self._isi_violations_max = isi_violations_maximum\n        self._dark_mode = dark_mode\n        self._units = pd.DataFrame()  # Empty default\n        self._x_fmt = dict(showgrid=False, zeroline=False, linewidth=2, ticks=\"outside\")\n        self._y_fmt = dict(showgrid=False, linewidth=0, zeroline=True, visible=False)\n        self._no_data_text = \"No data available\"  # What to show when no data in table\n        self._null_series = pd.Series(np.nan)  # What to substitute when no data\n\n    @property\n    def key(self) -&gt; dict:\n\"\"\"Key in ephys.QualityMetrics table\"\"\"\n        return self._key\n\n    @key.setter  # Allows `cls.property = new_item` notation\n    def key(self, key: dict):\n\"\"\"Use class_instance.key = your_key to reset key\"\"\"\n        if key not in self._ephys.QualityMetrics.fetch(\"KEY\"):\n            # If not already full key, check if unquely identifies entry\n            key = (self._ephys.QualityMetrics &amp; key).fetch1(\"KEY\")\n        self._key = key\n\n    @key.deleter  # Allows `del cls.property` to clear key\n    def key(self):\n\"\"\"Use del class_instance.key to clear key\"\"\"\n        logger.info(\"Cleared key\")\n        self._key = None\n\n    @property\n    def cutoffs(self) -&gt; dict:\n\"\"\"Amplitude, presence ratio, isi violation cutoffs\"\"\"\n        return dict(\n            amplitude_cutoff_maximum=self._amplitude_cutoff_max,\n            presence_ratio_minimum=self._presence_ratio_min,\n            isi_violations_maximum=self._isi_violations_max,\n        )\n\n    @cutoffs.setter\n    def cutoffs(self, cutoff_dict):\n\"\"\"Use class_instance.cutoffs = dict(var=cutoff) to adjust cutoffs\n\n        Args:\n            cutoff_dict (kwargs): Cutoffs to adjust: amplitude_cutoff_maximum,\n                presence_ratio_minimum, and/or isi_violations_maximum\n        \"\"\"\n        self._amplitude_cutoff_max = cutoff_dict.get(\n            \"amplitude_cutoff_maximum\", self._amplitude_cutoff_max\n        )\n        self._presence_ratio_min = cutoff_dict.get(\n            \"presence_ratio_minimum\", self._presence_ratio_min\n        )\n        self._isi_violations_max = cutoff_dict.get(\n            \"isi_violations_maximum\", self._isi_violations_max\n        )\n        _ = self.units\n\n    @property\n    def units(self) -&gt; pd.DataFrame:\n\"\"\"Pandas dataframe of QC metrics\"\"\"\n        if not self._key:\n            return self._null_series\n\n        if self._units.empty:\n            restrictions = [\"TRUE\"]\n            if self._amplitude_cutoff_max:\n                restrictions.append(f\"amplitude_cutoff &lt; {self._amplitude_cutoff_max}\")\n            if self._presence_ratio_min:\n                restrictions.append(f\"presence_ratio &gt; {self._presence_ratio_min}\")\n            if self._isi_violations_max:\n                restrictions.append(f\"isi_violation &lt; {self._isi_violations_max}\")\n            \" AND \".join(restrictions)  # Build restriction from cutoffs\n            return (\n                self._ephys.QualityMetrics\n                * self._ephys.QualityMetrics.Cluster\n                * self._ephys.QualityMetrics.Waveform\n                &amp; self._key\n                &amp; restrictions\n            ).fetch(format=\"frame\")\n\n        return self._units\n\n    def _format_fig(\n        self, fig: go.Figure = None, scale: float = None, ratio: float = 1.0\n    ) -&gt; go.Figure:\n\"\"\"Return formatted figure or apply prmatting to existing figure\n\n        Args:\n            fig (go.Figure, optional): Apply formatting to this plotly graph object\n                Figure to apply formatting. Defaults to empty.\n            scale (float, optional): Scale to render figure. Defaults to scale from\n                class init, 1.\n            ratio (float, optional): Figure aspect ratio width/height . Defaults to 1.\n\n        Returns:\n            go.Figure: Formatted figure\n        \"\"\"\n        if not fig:\n            fig = go.Figure()\n        if not scale:\n            scale = self._scale\n\n        width = self._fig_width * scale\n\n        return fig.update_layout(\n            template=\"plotly_dark\" if self._dark_mode else \"simple_white\",\n            width=width,\n            height=width / ratio,\n            margin=dict(l=20 * scale, r=20 * scale, t=40 * scale, b=40 * scale),\n            showlegend=False,\n        )\n\n    def _empty_fig(\n        self, text=\"Select a key to visualize QC metrics\", scale=None\n    ) -&gt; go.Figure:\n\"\"\"Return figure object for when no key is provided\"\"\"\n        if not scale:\n            scale = self._scale\n\n        return (\n            self._format_fig(scale=scale)\n            .add_annotation(text=text, showarrow=False)\n            .update_layout(xaxis=self._y_fmt, yaxis=self._y_fmt)\n        )\n\n    def _plot_metric(\n        self,\n        data: pd.DataFrame,\n        bins: np.ndarray,\n        scale: float = None,\n        fig: go.Figure = None,\n        **trace_kwargs,\n    ) -&gt; go.Figure:\n\"\"\"Plot histogram using bins provided\n\n        Args:\n            data (pd.DataFrame): Data to be plotted, from QC metric\n            bins (np.ndarray): Array of bins to use for histogram\n            scale (float, optional): Scale to render figure. Defaults to scale from\n                class initialization.\n            fig (go.Figure, optional): Add trace to this figure. Defaults to empty\n                formatted figure.\n\n        Returns:\n            go.Figure: Histogram plot\n        \"\"\"\n        if not scale:\n            scale = self._scale\n        if not fig:\n            fig = self._format_fig(scale=scale)\n\n        if not data.isnull().all():\n            histogram, histogram_bins = np.histogram(data, bins=bins, density=True)\n        else:\n            # To quiet divide by zero error when no data\n            histogram, histogram_bins = np.ndarray(0), np.ndarray(0)\n\n        return fig.add_trace(\n            go.Scatter(\n                x=histogram_bins[:-1],\n                y=gaussian_filter1d(histogram, 1),  # TODO: remove smoothing\n                mode=\"lines\",\n                line=dict(color=\"rgb(0, 160, 223)\", width=2 * scale),  # DataJoint Blue\n                hovertemplate=\"%{x:.2f}&lt;br&gt;%{y:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n            ),\n            **trace_kwargs,\n        )\n\n    def get_single_fig(self, fig_name: str, scale: float = None) -&gt; go.Figure:\n\"\"\"Return a single figure of the plots listed in the plot_list property\n\n        Args:\n            fig_name (str): Name of figure to be rendered\n            scale (float, optional): Scale to render fig. Defaults to scale at class\n                init, 1.\n\n        Returns:\n            go.Figure: Histogram plot\n        \"\"\"\n        if not self._key:\n            return self._empty_fig()\n        if not scale:\n            scale = self._scale\n\n        fig_dict = self.plots.get(fig_name, dict()) if self._key else dict()\n        data = fig_dict.get(\"data\", self._null_series)\n        bins = fig_dict.get(\"bins\", np.linspace(0, 0, 0))\n        vline = fig_dict.get(\"vline\", None)\n\n        if data.isnull().all():\n            return self._empty_fig(text=self._no_data_text)\n\n        fig = (\n            self._plot_metric(data=data, bins=bins, scale=scale)\n            .update_layout(xaxis=self._x_fmt, yaxis=self._y_fmt)\n            .update_layout(  # Add title\n                title=dict(text=fig_dict.get(\"xaxis\", \" \"), xanchor=\"center\", x=0.5),\n                font=dict(size=12 * scale),\n            )\n        )\n\n        if vline:\n            fig.add_vline(x=vline, line_width=2 * scale, line_dash=\"dash\")\n\n        return fig\n\n    def get_grid(self, n_columns: int = 4, scale: float = 1.0) -&gt; go.Figure:\n\"\"\"Plot grid of histograms as subplots in go.Figure using n_columns\n\n        Args:\n            n_columns (int, optional): Number of colums in grid. Defaults to 4.\n            scale (float, optional): Scale to render fig. Defaults to scale at class\n                init, 1.\n\n        Returns:\n            go.Figure: grid of available plots\n        \"\"\"\n        from plotly.subplots import make_subplots\n\n        if not self._key:\n            return self._empty_fig()\n        if not scale:\n            scale = self._scale\n\n        n_rows = int(np.ceil(len(self.plots) / n_columns))\n\n        fig = self._format_fig(\n            fig=make_subplots(\n                rows=n_rows,\n                cols=n_columns,\n                shared_xaxes=False,\n                shared_yaxes=False,\n                vertical_spacing=(0.5 / n_rows),\n            ),\n            scale=scale,\n            ratio=(n_columns / n_rows),\n        ).update_layout(  # Global title\n            title=dict(text=\"Histograms of Quality Metrics\", xanchor=\"center\", x=0.5),\n            font=dict(size=12 * scale),\n        )\n\n        for idx, plot in enumerate(self._plots.values()):  # Each subplot\n            this_row = int(np.floor(idx / n_columns) + 1)\n            this_col = idx % n_columns + 1\n            data = plot.get(\"data\", self._null_series)\n            vline = plot.get(\"vline\", None)\n            if data.isnull().all():\n                vline = None  # If no data, don't want vline either\n                fig[\"layout\"].update(\n                    annotations=[\n                        dict(\n                            xref=f\"x{idx+1}\",\n                            yref=f\"y{idx+1}\",\n                            text=self._no_data_text,\n                            showarrow=False,\n                        ),\n                    ]\n                )\n            fig = self._plot_metric(  # still need to plot empty to cal y_vals min/max\n                data=data,\n                bins=plot[\"bins\"],\n                fig=fig,\n                row=this_row,\n                col=this_col,\n                scale=scale,\n            )\n            fig.update_xaxes(\n                title=dict(text=plot[\"xaxis\"], font_size=11 * scale),\n                row=this_row,\n                col=this_col,\n            )\n            if vline:\n                y_vals = fig.to_dict()[\"data\"][idx][\"y\"]\n                fig.add_shape(  # Add overlay WRT whole fig\n                    go.layout.Shape(\n                        type=\"line\",\n                        yref=\"paper\",\n                        xref=\"x\",  # relative to subplot x\n                        x0=vline,\n                        y0=min(y_vals),\n                        x1=vline,\n                        y1=max(y_vals),\n                        line=dict(width=2 * scale),\n                    ),\n                    row=this_row,\n                    col=this_col,\n                )\n\n        return fig.update_xaxes(**self._x_fmt).update_yaxes(**self._y_fmt)\n\n    @property\n    def plot_list(self):\n\"\"\"List of plots that can be rendered inidividually by name or as grid\"\"\"\n        if not self._plots:\n            _ = self.plots\n        return [plot for plot in self._plots]\n\n    @property\n    def plots(self):\n        if not self._plots:\n            self._plots = {\n                \"firing_rate\": {  # If linear, use np.linspace(0, 50, 100)\n                    \"xaxis\": \"Firing rate (log&lt;sub&gt;10&lt;/sub&gt; Hz)\",\n                    \"data\": np.log10(self.units.get(\"firing_rate\", self._null_series)),\n                    \"bins\": np.linspace(-3, 2, 100),\n                },\n                \"presence_ratio\": {\n                    \"xaxis\": \"Presence ratio\",\n                    \"data\": self.units.get(\"presence_ratio\", self._null_series),\n                    \"bins\": np.linspace(0, 1, 100),\n                    \"vline\": 0.9,\n                },\n                \"amp_cutoff\": {\n                    \"xaxis\": \"Amplitude cutoff\",\n                    \"data\": self.units.get(\"amplitude_cutoff\", self._null_series),\n                    \"bins\": np.linspace(0, 0.5, 200),\n                    \"vline\": 0.1,\n                },\n                \"isi_violation\": {  # If linear bins(0, 10, 200). Offset b/c log(0) null\n                    \"xaxis\": \"ISI violations (log&lt;sub&gt;10&lt;/sub&gt;)\",\n                    \"data\": np.log10(\n                        self.units.get(\"isi_violation\", self._null_series) + 1e-5\n                    ),\n                    \"bins\": np.linspace(-6, 2.5, 100),\n                    \"vline\": np.log10(0.5),\n                },\n                \"snr\": {\n                    \"xaxis\": \"SNR\",\n                    \"data\": self.units.get(\"snr\", self._null_series),\n                    \"bins\": np.linspace(0, 10, 100),\n                },\n                \"iso_dist\": {\n                    \"xaxis\": \"Isolation distance\",\n                    \"data\": self.units.get(\"isolation_distance\", self._null_series),\n                    \"bins\": np.linspace(0, 170, 50),\n                },\n                \"d_prime\": {\n                    \"xaxis\": \"d-prime\",\n                    \"data\": self.units.get(\"d_prime\", self._null_series),\n                    \"bins\": np.linspace(0, 15, 50),\n                },\n                \"nn_hit\": {\n                    \"xaxis\": \"Nearest-neighbors hit rate\",\n                    \"data\": self.units.get(\"nn_hit_rate\", self._null_series),\n                    \"bins\": np.linspace(0, 1, 100),\n                },\n            }\n        return self._plots\n\n    @plots.setter\n    def plots(self, new_plot_dict: dict):\n\"\"\"Adds or updates plot item in the set to be rendered.\n\n        plot items are structured as followed: dict with name key, embedded dict with\n            xaxis: string x-axis label\n            data: pandas dataframe to be plotted\n            bins: numpy ndarray of bin cutoffs for histogram\n        \"\"\"\n        _ = self.plots\n        [self._plots.update({k: v}) for k, v in new_plot_dict.items()]\n\n    def remove_plot(self, plot_name):\n\"\"\"Removes an item from the set of plots\"\"\"\n        _ = self._plots.pop(plot_name)\n</code></pre>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.cutoffs","title":"<code>cutoffs: dict</code>  <code>property</code> <code>writable</code>","text":"<p>Amplitude, presence ratio, isi violation cutoffs</p>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.key","title":"<code>key: dict</code>  <code>deletable</code> <code>property</code> <code>writable</code>","text":"<p>Key in ephys.QualityMetrics table</p>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.plot_list","title":"<code>plot_list</code>  <code>property</code>","text":"<p>List of plots that can be rendered inidividually by name or as grid</p>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.units","title":"<code>units: pd.DataFrame</code>  <code>property</code>","text":"<p>Pandas dataframe of QC metrics</p>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.__init__","title":"<code>__init__(ephys, key=None, scale=1, fig_width=800, amplitude_cutoff_maximum=None, presence_ratio_minimum=None, isi_violations_maximum=None, dark_mode=False)</code>","text":"<p>Initialize QC metric class</p> <p>Parameters:</p> Name Type Description Default <code>ephys</code> <code>module</code> <p>datajoint module with a QualityMetric table</p> required <code>key</code> <code>dict</code> <p>key from ephys.QualityMetric table. Defaults to None.</p> <code>None</code> <code>scale</code> <code>float</code> <p>Scale at which to render figure. Defaults to 1.4.</p> <code>1</code> <code>fig_width</code> <code>int</code> <p>Figure width in pixels. Defaults to 800.</p> <code>800</code> <code>amplitude_cutoff_maximum</code> <code>float</code> <p>Cutoff for unit ampliude in visualizations. Defaults to None.</p> <code>None</code> <code>presence_ratio_minimum</code> <code>float</code> <p>Cutoff for presence ratio in visualizations. Defaults to None.</p> <code>None</code> <code>isi_violations_maximum</code> <code>float</code> <p>Cutoff for isi violations in visualizations. Defaults to None.</p> <code>None</code> <code>dark_mode</code> <code>bool</code> <p>Set background to black, foreground white. Default False, black on white.</p> <code>False</code> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def __init__(\n    self,\n    ephys: types.ModuleType,\n    key: dict = None,\n    scale: float = 1,\n    fig_width=800,\n    amplitude_cutoff_maximum: float = None,\n    presence_ratio_minimum: float = None,\n    isi_violations_maximum: float = None,\n    dark_mode: bool = False,\n):\n\"\"\"Initialize QC metric class\n\n    Args:\n        ephys (module): datajoint module with a QualityMetric table\n        key (dict, optional): key from ephys.QualityMetric table. Defaults to None.\n        scale (float, optional): Scale at which to render figure. Defaults to 1.4.\n        fig_width (int, optional): Figure width in pixels. Defaults to 800.\n        amplitude_cutoff_maximum (float, optional): Cutoff for unit ampliude in\n            visualizations. Defaults to None.\n        presence_ratio_minimum (float, optional): Cutoff for presence ratio in\n            visualizations. Defaults to None.\n        isi_violations_maximum (float, optional): Cutoff for isi violations in\n            visualizations. Defaults to None.\n        dark_mode (bool, optional): Set background to black, foreground white.\n            Default False, black on white.\n    \"\"\"\n    self._ephys = ephys\n    self._key = key\n    self._scale = scale\n    self._plots = {}  # Empty default to defer set to dict property below\n    self._fig_width = fig_width\n    self._amplitude_cutoff_max = amplitude_cutoff_maximum\n    self._presence_ratio_min = presence_ratio_minimum\n    self._isi_violations_max = isi_violations_maximum\n    self._dark_mode = dark_mode\n    self._units = pd.DataFrame()  # Empty default\n    self._x_fmt = dict(showgrid=False, zeroline=False, linewidth=2, ticks=\"outside\")\n    self._y_fmt = dict(showgrid=False, linewidth=0, zeroline=True, visible=False)\n    self._no_data_text = \"No data available\"  # What to show when no data in table\n    self._null_series = pd.Series(np.nan)  # What to substitute when no data\n</code></pre>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.get_grid","title":"<code>get_grid(n_columns=4, scale=1.0)</code>","text":"<p>Plot grid of histograms as subplots in go.Figure using n_columns</p> <p>Parameters:</p> Name Type Description Default <code>n_columns</code> <code>int</code> <p>Number of colums in grid. Defaults to 4.</p> <code>4</code> <code>scale</code> <code>float</code> <p>Scale to render fig. Defaults to scale at class init, 1.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: grid of available plots</p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def get_grid(self, n_columns: int = 4, scale: float = 1.0) -&gt; go.Figure:\n\"\"\"Plot grid of histograms as subplots in go.Figure using n_columns\n\n    Args:\n        n_columns (int, optional): Number of colums in grid. Defaults to 4.\n        scale (float, optional): Scale to render fig. Defaults to scale at class\n            init, 1.\n\n    Returns:\n        go.Figure: grid of available plots\n    \"\"\"\n    from plotly.subplots import make_subplots\n\n    if not self._key:\n        return self._empty_fig()\n    if not scale:\n        scale = self._scale\n\n    n_rows = int(np.ceil(len(self.plots) / n_columns))\n\n    fig = self._format_fig(\n        fig=make_subplots(\n            rows=n_rows,\n            cols=n_columns,\n            shared_xaxes=False,\n            shared_yaxes=False,\n            vertical_spacing=(0.5 / n_rows),\n        ),\n        scale=scale,\n        ratio=(n_columns / n_rows),\n    ).update_layout(  # Global title\n        title=dict(text=\"Histograms of Quality Metrics\", xanchor=\"center\", x=0.5),\n        font=dict(size=12 * scale),\n    )\n\n    for idx, plot in enumerate(self._plots.values()):  # Each subplot\n        this_row = int(np.floor(idx / n_columns) + 1)\n        this_col = idx % n_columns + 1\n        data = plot.get(\"data\", self._null_series)\n        vline = plot.get(\"vline\", None)\n        if data.isnull().all():\n            vline = None  # If no data, don't want vline either\n            fig[\"layout\"].update(\n                annotations=[\n                    dict(\n                        xref=f\"x{idx+1}\",\n                        yref=f\"y{idx+1}\",\n                        text=self._no_data_text,\n                        showarrow=False,\n                    ),\n                ]\n            )\n        fig = self._plot_metric(  # still need to plot empty to cal y_vals min/max\n            data=data,\n            bins=plot[\"bins\"],\n            fig=fig,\n            row=this_row,\n            col=this_col,\n            scale=scale,\n        )\n        fig.update_xaxes(\n            title=dict(text=plot[\"xaxis\"], font_size=11 * scale),\n            row=this_row,\n            col=this_col,\n        )\n        if vline:\n            y_vals = fig.to_dict()[\"data\"][idx][\"y\"]\n            fig.add_shape(  # Add overlay WRT whole fig\n                go.layout.Shape(\n                    type=\"line\",\n                    yref=\"paper\",\n                    xref=\"x\",  # relative to subplot x\n                    x0=vline,\n                    y0=min(y_vals),\n                    x1=vline,\n                    y1=max(y_vals),\n                    line=dict(width=2 * scale),\n                ),\n                row=this_row,\n                col=this_col,\n            )\n\n    return fig.update_xaxes(**self._x_fmt).update_yaxes(**self._y_fmt)\n</code></pre>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.get_single_fig","title":"<code>get_single_fig(fig_name, scale=None)</code>","text":"<p>Return a single figure of the plots listed in the plot_list property</p> <p>Parameters:</p> Name Type Description Default <code>fig_name</code> <code>str</code> <p>Name of figure to be rendered</p> required <code>scale</code> <code>float</code> <p>Scale to render fig. Defaults to scale at class init, 1.</p> <code>None</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Histogram plot</p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def get_single_fig(self, fig_name: str, scale: float = None) -&gt; go.Figure:\n\"\"\"Return a single figure of the plots listed in the plot_list property\n\n    Args:\n        fig_name (str): Name of figure to be rendered\n        scale (float, optional): Scale to render fig. Defaults to scale at class\n            init, 1.\n\n    Returns:\n        go.Figure: Histogram plot\n    \"\"\"\n    if not self._key:\n        return self._empty_fig()\n    if not scale:\n        scale = self._scale\n\n    fig_dict = self.plots.get(fig_name, dict()) if self._key else dict()\n    data = fig_dict.get(\"data\", self._null_series)\n    bins = fig_dict.get(\"bins\", np.linspace(0, 0, 0))\n    vline = fig_dict.get(\"vline\", None)\n\n    if data.isnull().all():\n        return self._empty_fig(text=self._no_data_text)\n\n    fig = (\n        self._plot_metric(data=data, bins=bins, scale=scale)\n        .update_layout(xaxis=self._x_fmt, yaxis=self._y_fmt)\n        .update_layout(  # Add title\n            title=dict(text=fig_dict.get(\"xaxis\", \" \"), xanchor=\"center\", x=0.5),\n            font=dict(size=12 * scale),\n        )\n    )\n\n    if vline:\n        fig.add_vline(x=vline, line_width=2 * scale, line_dash=\"dash\")\n\n    return fig\n</code></pre>"},{"location":"api/element_array_ephys/plotting/qc/#element_array_ephys.plotting.qc.QualityMetricFigs.remove_plot","title":"<code>remove_plot(plot_name)</code>","text":"<p>Removes an item from the set of plots</p> Source code in <code>element_array_ephys/plotting/qc.py</code> <pre><code>def remove_plot(self, plot_name):\n\"\"\"Removes an item from the set of plots\"\"\"\n    _ = self._plots.pop(plot_name)\n</code></pre>"},{"location":"api/element_array_ephys/plotting/unit_level/","title":"unit_level.py","text":""},{"location":"api/element_array_ephys/plotting/unit_level/#element_array_ephys.plotting.unit_level.plot_auto_correlogram","title":"<code>plot_auto_correlogram(spike_times, bin_size=0.001, window_size=1)</code>","text":"<p>Plot the auto-correlogram of a unit.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>np.ndarray</code> <p>Spike timestamps in seconds</p> required <code>bin_size</code> <code>float</code> <p>Size of the time bin (lag) in seconds. Defaults to 0.001.</p> <code>0.001</code> <code>window_size</code> <code>int</code> <p>Size of the correlogram window in seconds. Defaults to 1 (\u00b1 500ms)</p> <code>1</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Plotly figure object for showing</p> <code>go.Figure</code> <p>counts (y-axis) over time lags (x-axis).</p> Source code in <code>element_array_ephys/plotting/unit_level.py</code> <pre><code>def plot_auto_correlogram(\n    spike_times: np.ndarray, bin_size: float = 0.001, window_size: int = 1\n) -&gt; go.Figure:\n\"\"\"Plot the auto-correlogram of a unit.\n\n    Args:\n        spike_times (np.ndarray): Spike timestamps in seconds\n        bin_size (float, optional): Size of the time bin (lag) in seconds. Defaults to 0.001.\n        window_size (int, optional): Size of the correlogram window in seconds. Defaults to 1 (\u00b1 500ms)\n\n    Returns:\n        go.Figure: Plotly figure object for showing\n        counts (y-axis) over time lags (x-axis).\n    \"\"\"\n    from .corr import acorr\n\n    correlogram = acorr(\n        spike_times=spike_times, bin_size=bin_size, window_size=window_size\n    )\n    df = pd.DataFrame(\n        data={\"correlogram\": correlogram},\n        index=pd.RangeIndex(\n            start=-(window_size * 1e3) / 2,\n            stop=(window_size * 1e3) / 2 + bin_size * 1e3,\n            step=bin_size * 1e3,\n        ),\n    )\n    df[\"lags\"] = df.index  # in ms\n\n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=df[\"lags\"],\n            y=df[\"correlogram\"],\n            mode=\"lines\",\n            line=dict(color=\"black\", width=1),\n            hovertemplate=\"%{y}&lt;br&gt;\" + \"%{x:.2f} ms&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    fig.update_layout(\n        title=\"Auto Correlogram\",\n        xaxis_title=\"Lags (ms)\",\n        yaxis_title=\"Count\",\n        template=\"simple_white\",\n        width=350,\n        height=350,\n        yaxis_range=[0, None],\n    )\n    return fig\n</code></pre>"},{"location":"api/element_array_ephys/plotting/unit_level/#element_array_ephys.plotting.unit_level.plot_depth_waveforms","title":"<code>plot_depth_waveforms(ephys, unit_key, y_range=60)</code>","text":"<p>Plot the peak waveform (in red) and waveforms from its neighboring sites on a spatial coordinate.</p> <p>Parameters:</p> Name Type Description Default <code>ephys</code> <code>Module</code> <p>Imported ephys module object.</p> required <code>unit_key</code> <code>dict[str, Any]</code> <p>Key dictionary from ephys.CuratedClustering.Unit table.</p> required <code>y_range</code> <code>float</code> <p>Vertical range to show waveforms relative to the peak waveform in \u03bcm. Defaults to 60.</p> <code>60</code> <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Plotly figure object.</p> Source code in <code>element_array_ephys/plotting/unit_level.py</code> <pre><code>def plot_depth_waveforms(\n    ephys: Module,\n    unit_key: dict[str, Any],\n    y_range: float = 60,\n) -&gt; go.Figure:\n\"\"\"Plot the peak waveform (in red) and waveforms from its neighboring sites on a spatial coordinate.\n\n    Args:\n        ephys (Module): Imported ephys module object.\n        unit_key (dict[str, Any]): Key dictionary from ephys.CuratedClustering.Unit table.\n        y_range (float, optional): Vertical range to show waveforms relative to the peak waveform in \u03bcm. Defaults to 60.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n\n    sampling_rate = (ephys.EphysRecording &amp; unit_key).fetch1(\n        \"sampling_rate\"\n    ) / 1e3  # in kHz\n\n    probe_type, peak_electrode = (ephys.CuratedClustering.Unit &amp; unit_key).fetch1(\n        \"probe_type\", \"electrode\"\n    )  # electrode where the peak waveform was found\n\n    electrodes, coord_y = (\n        probe.ProbeType.Electrode &amp; f\"probe_type='{probe_type}'\"\n    ).fetch(\"electrode\", \"y_coord\")\n\n    peak_electrode_shank = (\n        probe.ProbeType.Electrode\n        &amp; f\"probe_type='{probe_type}'\"\n        &amp; f\"electrode={peak_electrode}\"\n    ).fetch1(\"shank\")\n\n    peak_coord_y = coord_y[electrodes == peak_electrode][0]\n\n    coord_ylim_low = (\n        coord_y.min()\n        if (peak_coord_y - y_range) &lt;= coord_y.min()\n        else peak_coord_y - y_range\n    )\n    coord_ylim_high = (\n        coord_y.max()\n        if (peak_coord_y + y_range) &gt;= coord_y.max()\n        else peak_coord_y + y_range\n    )\n\n    tbl = (\n        (probe.ProbeType.Electrode)\n        &amp; f\"probe_type = '{probe_type}'\"\n        &amp; f\"y_coord BETWEEN {coord_ylim_low} AND {coord_ylim_high}\"\n        &amp; f\"shank={peak_electrode_shank}\"\n    )\n    electrodes_to_plot, x_coords, y_coords = tbl.fetch(\n        \"electrode\", \"x_coord\", \"y_coord\"\n    )\n\n    coords = np.array([x_coords, y_coords]).T  # x, y coordinates\n\n    waveforms = (\n        ephys.WaveformSet.Waveform\n        &amp; unit_key\n        &amp; f\"electrode IN {tuple(electrodes_to_plot)}\"\n    ).fetch(\"waveform_mean\")\n    waveforms = np.stack(waveforms)  # all mean waveforms of a given neuron\n\n    x_min, x_max = np.min(coords[:, 0]), np.max(coords[:, 0])\n    y_min, y_max = np.min(coords[:, 1]), np.max(coords[:, 1])\n\n    # Spacing between recording sites (in um)\n    x_inc = np.diff(np.sort((coords[coords[:, 1] == coords[0, 1]][:, 0]))).mean() / 2\n    y_inc = np.diff(np.sort((coords[coords[:, 0] == coords[0, 0]][:, 1]))).mean() / 2\n    time = np.arange(waveforms.shape[1]) / sampling_rate\n\n    x_scale_factor = x_inc / (time[-1] + 1 / sampling_rate)  # correspond to 1 ms\n    time_scaled = time * x_scale_factor\n\n    wf_amps = waveforms.max(axis=1) - waveforms.min(axis=1)\n    max_amp = wf_amps.max()\n    y_scale_factor = y_inc / max_amp\n\n    unique_x_loc = np.sort(np.unique(coords[:, 0]))\n    xtick_label = [str(int(x)) for x in unique_x_loc]\n    xtick_loc = time_scaled[len(time_scaled) // 2 + 1] + unique_x_loc\n\n    # Plot figure\n    fig = go.Figure()\n    for electrode, wf, coord in zip(electrodes_to_plot, waveforms, coords):\n\n        wf_scaled = wf * y_scale_factor\n        wf_scaled -= wf_scaled.mean()\n        color = \"red\" if electrode == peak_electrode else \"rgb(51, 76.5, 204)\"\n\n        fig.add_trace(\n            go.Scatter(\n                x=time_scaled + coord[0],\n                y=wf_scaled + coord[1],\n                mode=\"lines\",\n                line=dict(color=color, width=1.5),\n                hovertemplate=f\"electrode {electrode}&lt;br&gt;\"\n                + f\"x ={coord[0]: .0f} \u03bcm&lt;br&gt;\"\n                + f\"y ={coord[1]: .0f} \u03bcm&lt;extra&gt;&lt;/extra&gt;\",\n            )\n        )\n        fig.update_layout(\n            title=\"Depth Waveforms\",\n            xaxis_title=\"Electrode position (\u03bcm)\",\n            yaxis_title=\"Distance from the probe tip (\u03bcm)\",\n            template=\"simple_white\",\n            width=400,\n            height=600,\n            xaxis_range=[x_min - x_inc / 2, x_max + x_inc * 1.2],\n            yaxis_range=[y_min - y_inc * 2, y_max + y_inc * 2],\n        )\n\n    fig.update_layout(showlegend=False)\n    fig.update_xaxes(tickvals=xtick_loc, ticktext=xtick_label)\n\n    # Add a scale bar\n    x0 = xtick_loc[0] - (x_scale_factor * 1.5)\n    y0 = y_min - (y_inc * 1.5)\n\n    fig.add_trace(\n        go.Scatter(\n            x=[x0, x0 + x_scale_factor],\n            y=[y0, y0],\n            mode=\"lines\",\n            line=dict(color=\"black\", width=2),\n            hovertemplate=\"1 ms&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=[x0, x0],\n            y=[y0, y0 + y_inc],\n            mode=\"lines\",\n            line=dict(color=\"black\", width=2),\n            hovertemplate=f\"{max_amp: .2f} \u03bcV&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    return fig\n</code></pre>"},{"location":"api/element_array_ephys/plotting/unit_level/#element_array_ephys.plotting.unit_level.plot_waveform","title":"<code>plot_waveform(waveform, sampling_rate)</code>","text":"<p>Plot unit waveform.</p> <p>Parameters:</p> Name Type Description Default <code>waveform</code> <code>np.ndarray</code> <p>Amplitude of a spike waveform in \u03bcV.</p> required <code>sampling_rate</code> <code>float</code> <p>Sampling rate in kHz.</p> required <p>Returns:</p> Type Description <code>go.Figure</code> <p>go.Figure: Plotly figure object for showing the amplitude of a waveform (y-axis in \u03bcV) over time (x-axis).</p> Source code in <code>element_array_ephys/plotting/unit_level.py</code> <pre><code>def plot_waveform(waveform: np.ndarray, sampling_rate: float) -&gt; go.Figure:\n\"\"\"Plot unit waveform.\n\n    Args:\n        waveform (np.ndarray): Amplitude of a spike waveform in \u03bcV.\n        sampling_rate (float): Sampling rate in kHz.\n\n    Returns:\n        go.Figure: Plotly figure object for showing the amplitude of a waveform (y-axis in \u03bcV) over time (x-axis).\n    \"\"\"\n    waveform_df = pd.DataFrame(data={\"waveform\": waveform})\n    waveform_df[\"timestamp\"] = waveform_df.index / sampling_rate\n\n    fig = go.Figure()\n    fig.add_trace(\n        go.Scatter(\n            x=waveform_df[\"timestamp\"],\n            y=waveform_df[\"waveform\"],\n            mode=\"lines\",\n            line=dict(color=\"rgb(0, 160, 223)\", width=2),  # DataJoint Blue\n            hovertemplate=\"%{y:.2f} \u03bcV&lt;br&gt;\" + \"%{x:.2f} ms&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n    fig.update_layout(\n        title=\"Avg. waveform\",\n        xaxis_title=\"Time (ms)\",\n        yaxis_title=\"Voltage (\u03bcV)\",\n        template=\"simple_white\",\n        width=350,\n        height=350,\n    )\n    return fig\n</code></pre>"},{"location":"api/element_array_ephys/plotting/widget/","title":"widget.py","text":""},{"location":"api/element_array_ephys/readers/kilosort/","title":"kilosort.py","text":""},{"location":"api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort","title":"<code>Kilosort</code>","text":"Source code in <code>element_array_ephys/readers/kilosort.py</code> <pre><code>class Kilosort:\n\n    _kilosort_core_files = [\n        \"params.py\",\n        \"amplitudes.npy\",\n        \"channel_map.npy\",\n        \"channel_positions.npy\",\n        \"pc_features.npy\",\n        \"pc_feature_ind.npy\",\n        \"similar_templates.npy\",\n        \"spike_templates.npy\",\n        \"spike_times.npy\",\n        \"template_features.npy\",\n        \"template_feature_ind.npy\",\n        \"templates.npy\",\n        \"templates_ind.npy\",\n        \"whitening_mat.npy\",\n        \"whitening_mat_inv.npy\",\n        \"spike_clusters.npy\",\n    ]\n\n    _kilosort_additional_files = [\n        \"spike_times_sec.npy\",\n        \"spike_times_sec_adj.npy\",\n        \"cluster_groups.csv\",\n        \"cluster_KSLabel.tsv\",\n    ]\n\n    kilosort_files = _kilosort_core_files + _kilosort_additional_files\n\n    def __init__(self, kilosort_dir):\n        self._kilosort_dir = pathlib.Path(kilosort_dir)\n        self._files = {}\n        self._data = None\n        self._clusters = None\n\n        self.validate()\n\n        params_filepath = kilosort_dir / \"params.py\"\n        self._info = {\n            \"time_created\": datetime.fromtimestamp(params_filepath.stat().st_ctime),\n            \"time_modified\": datetime.fromtimestamp(params_filepath.stat().st_mtime),\n        }\n\n    @property\n    def data(self):\n        if self._data is None:\n            self._load()\n        return self._data\n\n    @property\n    def info(self):\n        return self._info\n\n    def validate(self):\n\"\"\"\n        Check if this is a valid set of kilosort outputs - i.e. all crucial files exist\n        \"\"\"\n        missing_files = []\n        for f in Kilosort._kilosort_core_files:\n            full_path = self._kilosort_dir / f\n            if not full_path.exists():\n                missing_files.append(f)\n        if missing_files:\n            raise FileNotFoundError(\n                f\"Kilosort files missing in ({self._kilosort_dir}):\" f\" {missing_files}\"\n            )\n\n    def _load(self):\n        self._data = {}\n        for kilosort_filename in Kilosort.kilosort_files:\n            kilosort_filepath = self._kilosort_dir / kilosort_filename\n\n            if not kilosort_filepath.exists():\n                log.debug(\"skipping {} - does not exist\".format(kilosort_filepath))\n                continue\n\n            base, ext = path.splitext(kilosort_filename)\n            self._files[base] = kilosort_filepath\n\n            if kilosort_filename == \"params.py\":\n                log.debug(\"loading params.py {}\".format(kilosort_filepath))\n                # params.py is a 'key = val' file\n                params = {}\n                for line in open(kilosort_filepath, \"r\").readlines():\n                    k, v = line.strip(\"\\n\").split(\"=\")\n                    params[k.strip()] = convert_to_number(v.strip())\n                log.debug(\"params: {}\".format(params))\n                self._data[base] = params\n\n            if ext == \".npy\":\n                log.debug(\"loading npy {}\".format(kilosort_filepath))\n                d = np.load(\n                    kilosort_filepath,\n                    mmap_mode=\"r\",\n                    allow_pickle=False,\n                    fix_imports=False,\n                )\n                self._data[base] = (\n                    np.reshape(d, d.shape[0]) if d.ndim == 2 and d.shape[1] == 1 else d\n                )\n\n        self._data[\"channel_map\"] = self._data[\"channel_map\"].flatten()\n\n        # Read the Cluster Groups\n        for cluster_pattern, cluster_col_name in zip(\n            [\"cluster_group.*\", \"cluster_KSLabel.*\"], [\"group\", \"KSLabel\"]\n        ):\n            try:\n                cluster_file = next(self._kilosort_dir.glob(cluster_pattern))\n            except StopIteration:\n                pass\n            else:\n                cluster_file_suffix = cluster_file.suffix\n                assert cluster_file_suffix in (\".tsv\", \".xlsx\")\n                break\n        else:\n            raise FileNotFoundError(\n                'Neither \"cluster_groups\" nor \"cluster_KSLabel\" file found!'\n            )\n\n        if cluster_file_suffix == \".tsv\":\n            df = pd.read_csv(cluster_file, sep=\"\\t\", header=0)\n        elif cluster_file_suffix == \".xlsx\":\n            df = pd.read_excel(cluster_file, engine=\"openpyxl\")\n        else:\n            df = pd.read_csv(cluster_file, delimiter=\"\\t\")\n\n        self._data[\"cluster_groups\"] = np.array(df[cluster_col_name].values)\n        self._data[\"cluster_ids\"] = np.array(df[\"cluster_id\"].values)\n\n    def get_best_channel(self, unit):\n        template_idx = self.data[\"spike_templates\"][\n            np.where(self.data[\"spike_clusters\"] == unit)[0][0]\n        ]\n        channel_templates = self.data[\"templates\"][template_idx, :, :]\n        max_channel_idx = np.abs(channel_templates).max(axis=0).argmax()\n        max_channel = self.data[\"channel_map\"][max_channel_idx]\n\n        return max_channel, max_channel_idx\n\n    def extract_spike_depths(self):\n\"\"\"Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m\"\"\"\n\n        if \"pc_features\" in self.data:\n            ycoords = self.data[\"channel_positions\"][:, 1]\n            pc_features = self.data[\"pc_features\"][:, 0, :]  # 1st PC only\n            pc_features = np.where(pc_features &lt; 0, 0, pc_features)\n\n            # ---- compute center of mass of these features (spike depths) ----\n\n            # which channels for each spike?\n            spk_feature_ind = self.data[\"pc_feature_ind\"][\n                self.data[\"spike_templates\"], :\n            ]\n            # ycoords of those channels?\n            spk_feature_ycoord = ycoords[spk_feature_ind]\n            # center of mass is sum(coords.*features)/sum(features)\n            self._data[\"spike_depths\"] = np.sum(\n                spk_feature_ycoord * pc_features**2, axis=1\n            ) / np.sum(pc_features**2, axis=1)\n        else:\n            self._data[\"spike_depths\"] = None\n\n        # ---- extract spike sites ----\n        max_site_ind = np.argmax(np.abs(self.data[\"templates\"]).max(axis=1), axis=1)\n        spike_site_ind = max_site_ind[self.data[\"spike_templates\"]]\n        self._data[\"spike_sites\"] = self.data[\"channel_map\"][spike_site_ind]\n</code></pre>"},{"location":"api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.extract_spike_depths","title":"<code>extract_spike_depths()</code>","text":"<p>Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m</p> Source code in <code>element_array_ephys/readers/kilosort.py</code> <pre><code>def extract_spike_depths(self):\n\"\"\"Reimplemented from https://github.com/cortex-lab/spikes/blob/master/analysis/ksDriftmap.m\"\"\"\n\n    if \"pc_features\" in self.data:\n        ycoords = self.data[\"channel_positions\"][:, 1]\n        pc_features = self.data[\"pc_features\"][:, 0, :]  # 1st PC only\n        pc_features = np.where(pc_features &lt; 0, 0, pc_features)\n\n        # ---- compute center of mass of these features (spike depths) ----\n\n        # which channels for each spike?\n        spk_feature_ind = self.data[\"pc_feature_ind\"][\n            self.data[\"spike_templates\"], :\n        ]\n        # ycoords of those channels?\n        spk_feature_ycoord = ycoords[spk_feature_ind]\n        # center of mass is sum(coords.*features)/sum(features)\n        self._data[\"spike_depths\"] = np.sum(\n            spk_feature_ycoord * pc_features**2, axis=1\n        ) / np.sum(pc_features**2, axis=1)\n    else:\n        self._data[\"spike_depths\"] = None\n\n    # ---- extract spike sites ----\n    max_site_ind = np.argmax(np.abs(self.data[\"templates\"]).max(axis=1), axis=1)\n    spike_site_ind = max_site_ind[self.data[\"spike_templates\"]]\n    self._data[\"spike_sites\"] = self.data[\"channel_map\"][spike_site_ind]\n</code></pre>"},{"location":"api/element_array_ephys/readers/kilosort/#element_array_ephys.readers.kilosort.Kilosort.validate","title":"<code>validate()</code>","text":"<p>Check if this is a valid set of kilosort outputs - i.e. all crucial files exist</p> Source code in <code>element_array_ephys/readers/kilosort.py</code> <pre><code>def validate(self):\n\"\"\"\n    Check if this is a valid set of kilosort outputs - i.e. all crucial files exist\n    \"\"\"\n    missing_files = []\n    for f in Kilosort._kilosort_core_files:\n        full_path = self._kilosort_dir / f\n        if not full_path.exists():\n            missing_files.append(f)\n    if missing_files:\n        raise FileNotFoundError(\n            f\"Kilosort files missing in ({self._kilosort_dir}):\" f\" {missing_files}\"\n        )\n</code></pre>"},{"location":"api/element_array_ephys/readers/kilosort_triggering/","title":"kilosort_triggering.py","text":""},{"location":"api/element_array_ephys/readers/kilosort_triggering/#element_array_ephys.readers.kilosort_triggering.OpenEphysKilosortPipeline","title":"<code>OpenEphysKilosortPipeline</code>","text":"<p>An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline  for one Neuropixels probe in one recording session using the Open Ephys acquisition software.</p> <p>Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting Which is based on <code>ecephys_spike_sorting</code> routines from Allen Institute https://github.com/AllenInstitute/ecephys_spike_sorting</p> Source code in <code>element_array_ephys/readers/kilosort_triggering.py</code> <pre><code>class OpenEphysKilosortPipeline:\n\"\"\"\n    An object of OpenEphysKilosortPipeline manages the state of the Kilosort data processing pipeline\n     for one Neuropixels probe in one recording session using the Open Ephys acquisition software.\n\n    Primarily calling routines specified from:\n    https://github.com/jenniferColonell/ecephys_spike_sorting\n    Which is based on `ecephys_spike_sorting` routines from Allen Institute\n    https://github.com/AllenInstitute/ecephys_spike_sorting\n    \"\"\"\n\n    _modules = [\n        \"depth_estimation\",\n        \"median_subtraction\",\n        \"kilosort_helper\",\n        \"kilosort_postprocessing\",\n        \"noise_templates\",\n        \"mean_waveforms\",\n        \"quality_metrics\",\n    ]\n\n    _input_json_args = list(inspect.signature(createInputJson).parameters)\n\n    def __init__(\n        self, npx_input_dir: str, ks_output_dir: str, params: dict, KS2ver: str\n    ):\n\n        self._npx_input_dir = pathlib.Path(npx_input_dir)\n\n        self._ks_output_dir = pathlib.Path(ks_output_dir)\n        self._ks_output_dir.mkdir(parents=True, exist_ok=True)\n\n        self._params = params\n        self._KS2ver = KS2ver\n\n        self._json_directory = self._ks_output_dir / \"json_configs\"\n        self._json_directory.mkdir(parents=True, exist_ok=True)\n\n        self._median_subtraction_status = {}\n        self.ks_input_params = None\n        self._modules_input_hash = None\n        self._modules_input_hash_fp = None\n\n    def make_chanmap_file(self):\n        continuous_file = self._npx_input_dir / \"continuous.dat\"\n        self._chanmap_filepath = self._ks_output_dir / \"chanMap.mat\"\n\n        _write_channel_map_file(\n            channel_ind=self._params[\"channel_ind\"],\n            x_coords=self._params[\"x_coords\"],\n            y_coords=self._params[\"y_coords\"],\n            shank_ind=self._params[\"shank_ind\"],\n            connected=self._params[\"connected\"],\n            probe_name=self._params[\"probe_type\"],\n            ap_band_file=continuous_file.as_posix(),\n            bit_volts=self._params[\"uVPerBit\"],\n            sample_rate=self._params[\"sample_rate\"],\n            save_path=self._chanmap_filepath.as_posix(),\n            is_0_based=True,\n        )\n\n    def generate_modules_input_json(self):\n        self.make_chanmap_file()\n        self._module_input_json = (\n            self._json_directory / f\"{self._npx_input_dir.name}-input.json\"\n        )\n\n        continuous_file = self._get_raw_data_filepaths()\n\n        lf_dir = self._npx_input_dir.as_posix()\n        try:\n            # old probe folder convention with 100.0, 100.1, 100.2, 100.3, etc.\n            name, num = re.search(r\"(.+\\.)(\\d)+$\", lf_dir).groups()\n        except AttributeError:\n            # new probe folder convention with -AP or -LFP\n            assert lf_dir.endswith(\"AP\")\n            lf_dir = re.sub(\"-AP$\", \"-LFP\", lf_dir)\n        else:\n            lf_dir = f\"{name}{int(num) + 1}\"\n        lf_file = pathlib.Path(lf_dir) / \"continuous.dat\"\n\n        params = {}\n        for k, v in self._params.items():\n            value = str(v) if isinstance(v, list) else v\n            if f\"ks_{k}\" in self._input_json_args:\n                params[f\"ks_{k}\"] = value\n            if k in self._input_json_args:\n                params[k] = value\n\n        self.ks_input_params = createInputJson(\n            self._module_input_json.as_posix(),\n            KS2ver=self._KS2ver,\n            npx_directory=self._npx_input_dir.as_posix(),\n            spikeGLX_data=False,\n            continuous_file=continuous_file.as_posix(),\n            lf_file=lf_file.as_posix(),\n            extracted_data_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_tmp=self._ks_output_dir.as_posix(),\n            ks_make_copy=True,\n            noise_template_use_rf=self._params.get(\"noise_template_use_rf\", False),\n            use_C_Waves=False,\n            c_Waves_snr_um=self._params.get(\"c_Waves_snr_um\", 160),\n            qm_isi_thresh=self._params.get(\"refPerMS\", 2.0) / 1000,\n            kilosort_repository=_get_kilosort_repository(self._KS2ver),\n            chanMap_path=self._chanmap_filepath.as_posix(),\n            **params,\n        )\n\n        self._modules_input_hash = dict_to_uuid(self.ks_input_params)\n\n    def run_modules(self):\n        print(\"---- Running Modules ----\")\n        self.generate_modules_input_json()\n        module_input_json = self._module_input_json.as_posix()\n        module_logfile = module_input_json.replace(\n            \"-input.json\", \"-run_modules-log.txt\"\n        )\n\n        for module in self._modules:\n            module_status = self._get_module_status(module)\n            if module_status[\"completion_time\"] is not None:\n                continue\n\n            if module == \"median_subtraction\" and self._median_subtraction_status:\n                median_subtraction_status = self._get_module_status(\n                    \"median_subtraction\"\n                )\n                median_subtraction_status[\"duration\"] = self._median_subtraction_status[\n                    \"duration\"\n                ]\n                median_subtraction_status[\"completion_time\"] = datetime.strptime(\n                    median_subtraction_status[\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n                ) + timedelta(seconds=median_subtraction_status[\"duration\"])\n                self._update_module_status(\n                    {\"median_subtraction\": median_subtraction_status}\n                )\n                continue\n\n            module_output_json = self._get_module_output_json_filename(module)\n            command = [\n                sys.executable,\n                \"-W\",\n                \"ignore\",\n                \"-m\",\n                \"ecephys_spike_sorting.modules.\" + module,\n                \"--input_json\",\n                module_input_json,\n                \"--output_json\",\n                module_output_json,\n            ]\n\n            start_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": None,\n                        \"duration\": None,\n                    }\n                }\n            )\n            with open(module_logfile, \"a\") as f:\n                subprocess.check_call(command, stdout=f)\n            completion_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": completion_time,\n                        \"duration\": (completion_time - start_time).total_seconds(),\n                    }\n                }\n            )\n\n        self._update_total_duration()\n\n    def _get_raw_data_filepaths(self):\n        raw_ap_fp = self._npx_input_dir / \"continuous.dat\"\n\n        if \"median_subtraction\" not in self._modules:\n            return raw_ap_fp\n\n        # median subtraction step will overwrite original continuous.dat file with the corrected version\n        # to preserve the original raw data - make a copy here and work on the copied version\n        assert \"depth_estimation\" in self._modules\n        continuous_file = self._ks_output_dir / \"continuous.dat\"\n        if continuous_file.exists():\n            if raw_ap_fp.stat().st_mtime &lt; continuous_file.stat().st_mtime:\n                # if the copied continuous.dat was actually modified,\n                # median_subtraction may have been completed - let's check\n                module_input_json = self._module_input_json.as_posix()\n                module_logfile = module_input_json.replace(\n                    \"-input.json\", \"-run_modules-log.txt\"\n                )\n                with open(module_logfile, \"r\") as f:\n                    previous_line = \"\"\n                    for line in f.readlines():\n                        if line.startswith(\n                            \"ecephys spike sorting: median subtraction module\"\n                        ) and previous_line.startswith(\"Total processing time:\"):\n                            # regex to search for the processing duration - a float value\n                            duration = int(\n                                re.search(\"\\d+\\.?\\d+\", previous_line).group()\n                            )\n                            self._median_subtraction_status[\"duration\"] = duration\n                            return continuous_file\n                        previous_line = line\n\n        shutil.copy2(raw_ap_fp, continuous_file)\n        return continuous_file\n\n    def _update_module_status(self, updated_module_status={}):\n        if self._modules_input_hash is None:\n            raise RuntimeError('\"generate_modules_input_json()\" not yet performed!')\n\n        self._modules_input_hash_fp = (\n            self._json_directory / f\".{self._modules_input_hash}.json\"\n        )\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            modules_status = {**modules_status, **updated_module_status}\n        else:\n            modules_status = {\n                module: {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n                for module in self._modules\n            }\n        with open(self._modules_input_hash_fp, \"w\") as f:\n            json.dump(modules_status, f, default=str)\n\n    def _get_module_status(self, module):\n        if self._modules_input_hash_fp is None:\n            self._update_module_status()\n\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            if modules_status[module][\"completion_time\"] is None:\n                # additional logic to read from the \"-output.json\" file for this module as well\n                # handle cases where the module has finished successfully,\n                # but the \"_modules_input_hash_fp\" is not updated (for whatever reason),\n                # resulting in this module not registered as completed in the \"_modules_input_hash_fp\"\n                module_output_json_fp = pathlib.Path(\n                    self._get_module_output_json_filename(module)\n                )\n                if module_output_json_fp.exists():\n                    with open(module_output_json_fp) as f:\n                        module_run_output = json.load(f)\n                    modules_status[module][\"duration\"] = module_run_output[\n                        \"execution_time\"\n                    ]\n                    modules_status[module][\"completion_time\"] = datetime.strptime(\n                        modules_status[module][\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n                    ) + timedelta(seconds=module_run_output[\"execution_time\"])\n            return modules_status[module]\n\n        return {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n\n    def _get_module_output_json_filename(self, module):\n        module_input_json = self._module_input_json.as_posix()\n        module_output_json = module_input_json.replace(\n            \"-input.json\",\n            \"-\" + module + \"-\" + str(self._modules_input_hash) + \"-output.json\",\n        )\n        return module_output_json\n\n    def _update_total_duration(self):\n        with open(self._modules_input_hash_fp) as f:\n            modules_status = json.load(f)\n        cumulative_execution_duration = sum(\n            v[\"duration\"] or 0\n            for k, v in modules_status.items()\n            if k not in (\"cumulative_execution_duration\", \"total_duration\")\n        )\n        total_duration = (\n            datetime.strptime(\n                modules_status[self._modules[-1]][\"completion_time\"],\n                \"%Y-%m-%d %H:%M:%S.%f\",\n            )\n            - datetime.strptime(\n                modules_status[self._modules[0]][\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n            )\n        ).total_seconds()\n        self._update_module_status(\n            {\n                \"cumulative_execution_duration\": cumulative_execution_duration,\n                \"total_duration\": total_duration,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/readers/kilosort_triggering/#element_array_ephys.readers.kilosort_triggering.SGLXKilosortPipeline","title":"<code>SGLXKilosortPipeline</code>","text":"<p>An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline  for one Neuropixels probe in one recording session using the Spike GLX acquisition software.</p> <p>Primarily calling routines specified from: https://github.com/jenniferColonell/ecephys_spike_sorting</p> Source code in <code>element_array_ephys/readers/kilosort_triggering.py</code> <pre><code>class SGLXKilosortPipeline:\n\"\"\"\n    An object of SGLXKilosortPipeline manages the state of the Kilosort data processing pipeline\n     for one Neuropixels probe in one recording session using the Spike GLX acquisition software.\n\n    Primarily calling routines specified from:\n    https://github.com/jenniferColonell/ecephys_spike_sorting\n    \"\"\"\n\n    _modules = [\n        \"kilosort_helper\",\n        \"kilosort_postprocessing\",\n        \"noise_templates\",\n        \"mean_waveforms\",\n        \"quality_metrics\",\n    ]\n\n    _default_catgt_params = {\n        \"catGT_car_mode\": \"gblcar\",\n        \"catGT_loccar_min_um\": 40,\n        \"catGT_loccar_max_um\": 160,\n        \"catGT_cmd_string\": \"-prb_fld -out_prb_fld -gfix=0.4,0.10,0.02\",\n        \"ni_present\": False,\n        \"ni_extract_string\": \"-XA=0,1,3,500 -iXA=1,3,3,0  -XD=-1,1,50 -XD=-1,2,1.7 -XD=-1,3,5 -iXD=-1,3,5\",\n    }\n\n    _input_json_args = list(inspect.signature(createInputJson).parameters)\n\n    def __init__(\n        self,\n        npx_input_dir: str,\n        ks_output_dir: str,\n        params: dict,\n        KS2ver: str,\n        run_CatGT=False,\n        ni_present=False,\n        ni_extract_string=None,\n    ):\n\n        self._npx_input_dir = pathlib.Path(npx_input_dir)\n\n        self._ks_output_dir = pathlib.Path(ks_output_dir)\n        self._ks_output_dir.mkdir(parents=True, exist_ok=True)\n\n        self._params = params\n        self._KS2ver = KS2ver\n        self._run_CatGT = run_CatGT\n        self._run_CatGT = run_CatGT\n        self._default_catgt_params[\"ni_present\"] = ni_present\n        self._default_catgt_params[\"ni_extract_string\"] = (\n            ni_extract_string or self._default_catgt_params[\"ni_extract_string\"]\n        )\n\n        self._json_directory = self._ks_output_dir / \"json_configs\"\n        self._json_directory.mkdir(parents=True, exist_ok=True)\n\n        self._CatGT_finished = False\n        self.ks_input_params = None\n        self._modules_input_hash = None\n        self._modules_input_hash_fp = None\n\n    def parse_input_filename(self):\n        meta_filename = next(self._npx_input_dir.glob(\"*.ap.meta\")).name\n        match = re.search(\"(.*)_g(\\d)_t(\\d+|cat)\\.imec(\\d?)\\.ap\\.meta\", meta_filename)\n        session_str, gate_str, trigger_str, probe_str = match.groups()\n        return session_str, gate_str, trigger_str, probe_str or \"0\"\n\n    def generate_CatGT_input_json(self):\n        if not self._run_CatGT:\n            print(\"run_CatGT is set to False, skipping...\")\n            return\n\n        session_str, gate_str, trig_str, probe_str = self.parse_input_filename()\n\n        first_trig, last_trig = SpikeGLX_utils.ParseTrigStr(\n            \"start,end\", probe_str, gate_str, self._npx_input_dir.as_posix()\n        )\n        trigger_str = repr(first_trig) + \",\" + repr(last_trig)\n\n        self._catGT_input_json = (\n            self._json_directory / f\"{session_str}{probe_str}_CatGT-input.json\"\n        )\n\n        catgt_params = {\n            k: self._params.get(k, v) for k, v in self._default_catgt_params.items()\n        }\n\n        ni_present = catgt_params.pop(\"ni_present\")\n        ni_extract_string = catgt_params.pop(\"ni_extract_string\")\n\n        catgt_params[\"catGT_stream_string\"] = \"-ap -ni\" if ni_present else \"-ap\"\n        sync_extract = \"-SY=\" + probe_str + \",-1,6,500\"\n        extract_string = sync_extract + (f\" {ni_extract_string}\" if ni_present else \"\")\n        catgt_params[\"catGT_cmd_string\"] += f\" {extract_string}\"\n\n        input_meta_fullpath, continuous_file = self._get_raw_data_filepaths()\n\n        # create symbolic link to the actual data files - as CatGT expects files to follow a certain naming convention\n        continuous_file_symlink = (\n            continuous_file.parent\n            / f\"{session_str}_g{gate_str}\"\n            / f\"{session_str}_g{gate_str}_imec{probe_str}\"\n            / f\"{session_str}_g{gate_str}_t{trig_str}.imec{probe_str}.ap.bin\"\n        )\n        continuous_file_symlink.parent.mkdir(parents=True, exist_ok=True)\n        if continuous_file_symlink.exists():\n            continuous_file_symlink.unlink()\n        continuous_file_symlink.symlink_to(continuous_file)\n        input_meta_fullpath_symlink = (\n            input_meta_fullpath.parent\n            / f\"{session_str}_g{gate_str}\"\n            / f\"{session_str}_g{gate_str}_imec{probe_str}\"\n            / f\"{session_str}_g{gate_str}_t{trig_str}.imec{probe_str}.ap.meta\"\n        )\n        input_meta_fullpath_symlink.parent.mkdir(parents=True, exist_ok=True)\n        if input_meta_fullpath_symlink.exists():\n            input_meta_fullpath_symlink.unlink()\n        input_meta_fullpath_symlink.symlink_to(input_meta_fullpath)\n\n        createInputJson(\n            self._catGT_input_json.as_posix(),\n            KS2ver=self._KS2ver,\n            npx_directory=self._npx_input_dir.as_posix(),\n            spikeGLX_data=True,\n            catGT_run_name=session_str,\n            gate_string=gate_str,\n            trigger_string=trigger_str,\n            probe_string=probe_str,\n            continuous_file=continuous_file.as_posix(),\n            input_meta_path=input_meta_fullpath.as_posix(),\n            extracted_data_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_tmp=self._ks_output_dir.as_posix(),\n            kilosort_repository=_get_kilosort_repository(self._KS2ver),\n            **{k: v for k, v in catgt_params.items() if k in self._input_json_args},\n        )\n\n    def run_CatGT(self, force_rerun=False):\n        if self._run_CatGT and (not self._CatGT_finished or force_rerun):\n            self.generate_CatGT_input_json()\n\n            print(\"---- Running CatGT ----\")\n            catGT_input_json = self._catGT_input_json.as_posix()\n            catGT_output_json = catGT_input_json.replace(\n                \"CatGT-input.json\", \"CatGT-output.json\"\n            )\n\n            command = (\n                sys.executable\n                + \" -W ignore -m ecephys_spike_sorting.modules.\"\n                + \"catGT_helper\"\n                + \" --input_json \"\n                + catGT_input_json\n                + \" --output_json \"\n                + catGT_output_json\n            )\n            subprocess.check_call(command.split(\" \"))\n\n            self._CatGT_finished = True\n\n    def generate_modules_input_json(self):\n        session_str, _, _, probe_str = self.parse_input_filename()\n        self._module_input_json = (\n            self._json_directory / f\"{session_str}_imec{probe_str}-input.json\"\n        )\n\n        input_meta_fullpath, continuous_file = self._get_raw_data_filepaths()\n\n        params = {}\n        for k, v in self._params.items():\n            value = str(v) if isinstance(v, list) else v\n            if f\"ks_{k}\" in self._input_json_args:\n                params[f\"ks_{k}\"] = value\n            if k in self._input_json_args:\n                params[k] = value\n\n        self.ks_input_params = createInputJson(\n            self._module_input_json.as_posix(),\n            KS2ver=self._KS2ver,\n            npx_directory=self._npx_input_dir.as_posix(),\n            spikeGLX_data=True,\n            continuous_file=continuous_file.as_posix(),\n            input_meta_path=input_meta_fullpath.as_posix(),\n            extracted_data_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_directory=self._ks_output_dir.as_posix(),\n            kilosort_output_tmp=self._ks_output_dir.as_posix(),\n            ks_make_copy=True,\n            noise_template_use_rf=self._params.get(\"noise_template_use_rf\", False),\n            c_Waves_snr_um=self._params.get(\"c_Waves_snr_um\", 160),\n            qm_isi_thresh=self._params.get(\"refPerMS\", 2.0) / 1000,\n            kilosort_repository=_get_kilosort_repository(self._KS2ver),\n            **params,\n        )\n\n        self._modules_input_hash = dict_to_uuid(self.ks_input_params)\n\n    def run_modules(self):\n        if self._run_CatGT and not self._CatGT_finished:\n            self.run_CatGT()\n\n        print(\"---- Running Modules ----\")\n        self.generate_modules_input_json()\n        module_input_json = self._module_input_json.as_posix()\n        module_logfile = module_input_json.replace(\n            \"-input.json\", \"-run_modules-log.txt\"\n        )\n\n        for module in self._modules:\n            module_status = self._get_module_status(module)\n            if module_status[\"completion_time\"] is not None:\n                continue\n\n            module_output_json = self._get_module_output_json_filename(module)\n            command = (\n                sys.executable\n                + \" -W ignore -m ecephys_spike_sorting.modules.\"\n                + module\n                + \" --input_json \"\n                + module_input_json\n                + \" --output_json \"\n                + module_output_json\n            )\n\n            start_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": None,\n                        \"duration\": None,\n                    }\n                }\n            )\n            with open(module_logfile, \"a\") as f:\n                subprocess.check_call(command.split(\" \"), stdout=f)\n            completion_time = datetime.utcnow()\n            self._update_module_status(\n                {\n                    module: {\n                        \"start_time\": start_time,\n                        \"completion_time\": completion_time,\n                        \"duration\": (completion_time - start_time).total_seconds(),\n                    }\n                }\n            )\n\n        self._update_total_duration()\n\n    def _get_raw_data_filepaths(self):\n        session_str, gate_str, _, probe_str = self.parse_input_filename()\n\n        if self._CatGT_finished:\n            catGT_dest = self._ks_output_dir\n            run_str = session_str + \"_g\" + gate_str\n            run_folder = \"catgt_\" + run_str\n            prb_folder = run_str + \"_imec\" + probe_str\n            data_directory = catGT_dest / run_folder / prb_folder\n        else:\n            data_directory = self._npx_input_dir\n        try:\n            meta_fp = next(data_directory.glob(f\"{session_str}*.ap.meta\"))\n            bin_fp = next(data_directory.glob(f\"{session_str}*.ap.bin\"))\n        except StopIteration:\n            raise RuntimeError(\n                f\"No ap meta/bin files found in {data_directory} - CatGT error?\"\n            )\n\n        return meta_fp, bin_fp\n\n    def _update_module_status(self, updated_module_status={}):\n        if self._modules_input_hash is None:\n            raise RuntimeError('\"generate_modules_input_json()\" not yet performed!')\n\n        self._modules_input_hash_fp = (\n            self._json_directory / f\".{self._modules_input_hash}.json\"\n        )\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            modules_status = {**modules_status, **updated_module_status}\n        else:\n            modules_status = {\n                module: {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n                for module in self._modules\n            }\n        with open(self._modules_input_hash_fp, \"w\") as f:\n            json.dump(modules_status, f, default=str)\n\n    def _get_module_status(self, module):\n        if self._modules_input_hash_fp is None:\n            self._update_module_status()\n\n        if self._modules_input_hash_fp.exists():\n            with open(self._modules_input_hash_fp) as f:\n                modules_status = json.load(f)\n            if modules_status[module][\"completion_time\"] is None:\n                # additional logic to read from the \"-output.json\" file for this module as well\n                # handle cases where the module has finished successfully,\n                # but the \"_modules_input_hash_fp\" is not updated (for whatever reason),\n                # resulting in this module not registered as completed in the \"_modules_input_hash_fp\"\n                module_output_json_fp = pathlib.Path(\n                    self._get_module_output_json_filename(module)\n                )\n                if module_output_json_fp.exists():\n                    with open(module_output_json_fp) as f:\n                        module_run_output = json.load(f)\n                    modules_status[module][\"duration\"] = module_run_output[\n                        \"execution_time\"\n                    ]\n                    modules_status[module][\"completion_time\"] = datetime.strptime(\n                        modules_status[module][\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n                    ) + timedelta(seconds=module_run_output[\"execution_time\"])\n            return modules_status[module]\n\n        return {\"start_time\": None, \"completion_time\": None, \"duration\": None}\n\n    def _get_module_output_json_filename(self, module):\n        module_input_json = self._module_input_json.as_posix()\n        module_output_json = module_input_json.replace(\n            \"-input.json\",\n            \"-\" + module + \"-\" + str(self._modules_input_hash) + \"-output.json\",\n        )\n        return module_output_json\n\n    def _update_total_duration(self):\n        with open(self._modules_input_hash_fp) as f:\n            modules_status = json.load(f)\n        cumulative_execution_duration = sum(\n            v[\"duration\"] or 0\n            for k, v in modules_status.items()\n            if k not in (\"cumulative_execution_duration\", \"total_duration\")\n        )\n        total_duration = (\n            datetime.strptime(\n                modules_status[self._modules[-1]][\"completion_time\"],\n                \"%Y-%m-%d %H:%M:%S.%f\",\n            )\n            - datetime.strptime(\n                modules_status[self._modules[0]][\"start_time\"], \"%Y-%m-%d %H:%M:%S.%f\"\n            )\n        ).total_seconds()\n        self._update_module_status(\n            {\n                \"cumulative_execution_duration\": cumulative_execution_duration,\n                \"total_duration\": total_duration,\n            }\n        )\n</code></pre>"},{"location":"api/element_array_ephys/readers/openephys/","title":"openephys.py","text":""},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":"<p>The Open Ephys Record Node saves Neuropixels data in binary format according to the following the directory structure: (https://open-ephys.github.io/gui-docs/User-Manual/Recording-data/Binary-format.html)</p> <p>Record Node 102 -- experiment1 (equivalent to one experimental session - multi probes, multi recordings per probe)    -- recording1    -- recording2       -- continuous          -- Neuropix-PXI-100.0 (probe0 ap)          -- Neuropix-PXI-100.1 (probe0 lf)          -- Neuropix-PXI-100.2 (probe1 ap)          -- Neuropix-PXI-100.3 (probe1 lf)          ...       -- events       -- spikes       -- structure.oebin -- experiment 2    ... -- settings.xml -- settings2.xml ...</p>"},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys","title":"<code>OpenEphys</code>","text":"Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>class OpenEphys:\n    def __init__(self, experiment_dir):\n        self.session_dir = pathlib.Path(experiment_dir)\n\n        if self.session_dir.name.startswith(\"recording\"):\n            openephys_file = pyopenephys.File(\n                self.session_dir.parent.parent\n            )  # this is on the Record Node level\n            self._is_recording_folder = True\n        else:\n            openephys_file = pyopenephys.File(\n                self.session_dir.parent\n            )  # this is on the Record Node level\n            self._is_recording_folder = False\n\n        # extract the \"recordings\" for this session\n        self.experiment = next(\n            experiment\n            for experiment in openephys_file.experiments\n            if pathlib.Path(experiment.absolute_foldername)\n            == (\n                self.session_dir.parent\n                if self._is_recording_folder\n                else self.session_dir\n            )\n        )\n\n        # extract probe data\n        self.probes = self.load_probe_data()\n\n        #\n        self._recording_time = None\n\n    @property\n    def recording_time(self):\n        if self._recording_time is None:\n            recording_datetimes = []\n            for probe in self.probes.values():\n                recording_datetimes.extend(probe.recording_info[\"recording_datetimes\"])\n            self._recording_time = sorted(recording_datetimes)[0]\n        return self._recording_time\n\n    def load_probe_data(self):  # noqa: C901\n\"\"\"\n        Loop through all Open Ephys \"signalchains/processors\", identify the processor for\n         the Neuropixels probe(s), extract probe info\n            Loop through all recordings, associate recordings to\n            the matching probes, extract recording info\n\n        Yielding multiple \"Probe\" objects, each containing meta information\n         and timeseries data associated with each probe\n        \"\"\"\n\n        probes = {}\n        sigchain_iter = (\n            self.experiment.settings[\"SIGNALCHAIN\"]\n            if isinstance(self.experiment.settings[\"SIGNALCHAIN\"], list)\n            else [self.experiment.settings[\"SIGNALCHAIN\"]]\n        )\n        for sigchain in sigchain_iter:\n            processor_iter = (\n                sigchain[\"PROCESSOR\"]\n                if isinstance(sigchain[\"PROCESSOR\"], list)\n                else [sigchain[\"PROCESSOR\"]]\n            )\n            for processor in processor_iter:\n                if processor[\"@pluginName\"] in (\"Neuropix-3a\", \"Neuropix-PXI\"):\n                    if \"STREAM\" in processor:  # only on version &gt;= 0.6.0\n                        ap_streams = [\n                            stream\n                            for stream in processor[\"STREAM\"]\n                            if not stream[\"@name\"].endswith(\"LFP\")\n                        ]\n                    else:\n                        ap_streams = None\n\n                    if (\n                        processor[\"@pluginName\"] == \"Neuropix-3a\"\n                        or \"NP_PROBE\" not in processor[\"EDITOR\"]\n                    ):\n                        editor_probe_key = \"PROBE\"\n                    elif processor[\"@pluginName\"] == \"Neuropix-PXI\":\n                        editor_probe_key = \"NP_PROBE\"\n                    else:\n                        raise NotImplementedError\n\n                    probe_indices = (\n                        (0,)\n                        if isinstance(processor[\"EDITOR\"][editor_probe_key], dict)\n                        else range(len(processor[\"EDITOR\"][editor_probe_key]))\n                    )\n\n                else:  # not a processor for Neuropixels probe\n                    continue\n\n                for probe_index in probe_indices:\n                    probe = Probe(processor, probe_index)\n                    if ap_streams:\n                        probe.probe_info[\"ap_stream\"] = ap_streams[probe_index]\n                    probes[probe.probe_SN] = probe\n\n        for probe_index, probe_SN in enumerate(probes):\n\n            probe = probes[probe_SN]\n\n            for rec in self.experiment.recordings:\n                if (\n                    self._is_recording_folder\n                    and rec.absolute_foldername != self.session_dir\n                ):\n                    continue\n\n                assert len(rec._oebin[\"continuous\"]) == len(rec.analog_signals), (\n                    f\"Mismatch in the number of continuous data\"\n                    f' - expecting {len(rec._oebin[\"continuous\"])} (from structure.oebin file),'\n                    f\" found {len(rec.analog_signals)} (in continuous folder)\"\n                )\n\n                for continuous_info, analog_signal in zip(\n                    rec._oebin[\"continuous\"], rec.analog_signals\n                ):\n                    if continuous_info[\"source_processor_id\"] != probe.processor_id:\n                        continue\n\n                    # determine if this is continuous data for AP or LFP for the current probe\n                    if \"ap_stream\" in probe.probe_info:\n                        if (\n                            probe.probe_info[\"ap_stream\"][\"@name\"].split(\"-\")[0]\n                            != continuous_info[\"stream_name\"].split(\"-\")[0]\n                        ):\n                            continue  # not continuous data for the current probe\n                        match = re.search(\"-(AP|LFP)$\", continuous_info[\"stream_name\"])\n                        if match:\n                            continuous_type = match.groups()[0].lower()\n                        else:\n                            continuous_type = \"ap\"\n                    elif \"source_processor_sub_idx\" in continuous_info:\n                        if (\n                            continuous_info[\"source_processor_sub_idx\"]\n                            == probe_index * 2\n                        ):  # ap data\n                            assert (\n                                continuous_info[\"sample_rate\"]\n                                == analog_signal.sample_rate\n                                == 30000\n                            )\n                            continuous_type = \"ap\"\n                        elif (\n                            continuous_info[\"source_processor_sub_idx\"]\n                            == probe_index * 2 + 1\n                        ):  # lfp data\n                            assert (\n                                continuous_info[\"sample_rate\"]\n                                == analog_signal.sample_rate\n                                == 2500\n                            )\n                            continuous_type = \"lfp\"\n                        else:\n                            continue  # not continuous data for the current probe\n                    else:\n                        raise ValueError(\n                            f'Unable to infer type (AP or LFP) for the continuous data from:\\n\\t{continuous_info[\"folder_name\"]}'\n                        )\n\n                    if continuous_type == \"ap\":\n                        probe.recording_info[\"recording_count\"] += 1\n                        probe.recording_info[\"recording_datetimes\"].append(\n                            rec.datetime\n                            + datetime.timedelta(seconds=float(rec.start_time))\n                        )\n                        probe.recording_info[\"recording_durations\"].append(\n                            float(rec.duration)\n                        )\n                        probe.recording_info[\"recording_files\"].append(\n                            rec.absolute_foldername\n                            / \"continuous\"\n                            / continuous_info[\"folder_name\"]\n                        )\n                    elif continuous_type == \"lfp\":\n                        probe.recording_info[\"recording_lfp_files\"].append(\n                            rec.absolute_foldername\n                            / \"continuous\"\n                            / continuous_info[\"folder_name\"]\n                        )\n\n                    meta = getattr(probe, continuous_type + \"_meta\")\n                    if not meta:\n                        # channel indices - 0-based indexing\n                        channels_indices = [\n                            int(re.search(r\"\\d+$\", chn_name).group()) - 1\n                            for chn_name in analog_signal.channel_names\n                        ]\n\n                        meta.update(\n                            **continuous_info,\n                            channels_indices=channels_indices,\n                            channels_ids=analog_signal.channel_ids,\n                            channels_names=analog_signal.channel_names,\n                            channels_gains=analog_signal.gains,\n                        )\n\n                    signal = getattr(probe, continuous_type + \"_analog_signals\")\n                    signal.append(analog_signal)\n\n        return probes\n</code></pre>"},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.OpenEphys.load_probe_data","title":"<code>load_probe_data()</code>","text":"<p>Loop through all Open Ephys \"signalchains/processors\", identify the processor for  the Neuropixels probe(s), extract probe info     Loop through all recordings, associate recordings to     the matching probes, extract recording info</p> <p>Yielding multiple \"Probe\" objects, each containing meta information  and timeseries data associated with each probe</p> Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>def load_probe_data(self):  # noqa: C901\n\"\"\"\n    Loop through all Open Ephys \"signalchains/processors\", identify the processor for\n     the Neuropixels probe(s), extract probe info\n        Loop through all recordings, associate recordings to\n        the matching probes, extract recording info\n\n    Yielding multiple \"Probe\" objects, each containing meta information\n     and timeseries data associated with each probe\n    \"\"\"\n\n    probes = {}\n    sigchain_iter = (\n        self.experiment.settings[\"SIGNALCHAIN\"]\n        if isinstance(self.experiment.settings[\"SIGNALCHAIN\"], list)\n        else [self.experiment.settings[\"SIGNALCHAIN\"]]\n    )\n    for sigchain in sigchain_iter:\n        processor_iter = (\n            sigchain[\"PROCESSOR\"]\n            if isinstance(sigchain[\"PROCESSOR\"], list)\n            else [sigchain[\"PROCESSOR\"]]\n        )\n        for processor in processor_iter:\n            if processor[\"@pluginName\"] in (\"Neuropix-3a\", \"Neuropix-PXI\"):\n                if \"STREAM\" in processor:  # only on version &gt;= 0.6.0\n                    ap_streams = [\n                        stream\n                        for stream in processor[\"STREAM\"]\n                        if not stream[\"@name\"].endswith(\"LFP\")\n                    ]\n                else:\n                    ap_streams = None\n\n                if (\n                    processor[\"@pluginName\"] == \"Neuropix-3a\"\n                    or \"NP_PROBE\" not in processor[\"EDITOR\"]\n                ):\n                    editor_probe_key = \"PROBE\"\n                elif processor[\"@pluginName\"] == \"Neuropix-PXI\":\n                    editor_probe_key = \"NP_PROBE\"\n                else:\n                    raise NotImplementedError\n\n                probe_indices = (\n                    (0,)\n                    if isinstance(processor[\"EDITOR\"][editor_probe_key], dict)\n                    else range(len(processor[\"EDITOR\"][editor_probe_key]))\n                )\n\n            else:  # not a processor for Neuropixels probe\n                continue\n\n            for probe_index in probe_indices:\n                probe = Probe(processor, probe_index)\n                if ap_streams:\n                    probe.probe_info[\"ap_stream\"] = ap_streams[probe_index]\n                probes[probe.probe_SN] = probe\n\n    for probe_index, probe_SN in enumerate(probes):\n\n        probe = probes[probe_SN]\n\n        for rec in self.experiment.recordings:\n            if (\n                self._is_recording_folder\n                and rec.absolute_foldername != self.session_dir\n            ):\n                continue\n\n            assert len(rec._oebin[\"continuous\"]) == len(rec.analog_signals), (\n                f\"Mismatch in the number of continuous data\"\n                f' - expecting {len(rec._oebin[\"continuous\"])} (from structure.oebin file),'\n                f\" found {len(rec.analog_signals)} (in continuous folder)\"\n            )\n\n            for continuous_info, analog_signal in zip(\n                rec._oebin[\"continuous\"], rec.analog_signals\n            ):\n                if continuous_info[\"source_processor_id\"] != probe.processor_id:\n                    continue\n\n                # determine if this is continuous data for AP or LFP for the current probe\n                if \"ap_stream\" in probe.probe_info:\n                    if (\n                        probe.probe_info[\"ap_stream\"][\"@name\"].split(\"-\")[0]\n                        != continuous_info[\"stream_name\"].split(\"-\")[0]\n                    ):\n                        continue  # not continuous data for the current probe\n                    match = re.search(\"-(AP|LFP)$\", continuous_info[\"stream_name\"])\n                    if match:\n                        continuous_type = match.groups()[0].lower()\n                    else:\n                        continuous_type = \"ap\"\n                elif \"source_processor_sub_idx\" in continuous_info:\n                    if (\n                        continuous_info[\"source_processor_sub_idx\"]\n                        == probe_index * 2\n                    ):  # ap data\n                        assert (\n                            continuous_info[\"sample_rate\"]\n                            == analog_signal.sample_rate\n                            == 30000\n                        )\n                        continuous_type = \"ap\"\n                    elif (\n                        continuous_info[\"source_processor_sub_idx\"]\n                        == probe_index * 2 + 1\n                    ):  # lfp data\n                        assert (\n                            continuous_info[\"sample_rate\"]\n                            == analog_signal.sample_rate\n                            == 2500\n                        )\n                        continuous_type = \"lfp\"\n                    else:\n                        continue  # not continuous data for the current probe\n                else:\n                    raise ValueError(\n                        f'Unable to infer type (AP or LFP) for the continuous data from:\\n\\t{continuous_info[\"folder_name\"]}'\n                    )\n\n                if continuous_type == \"ap\":\n                    probe.recording_info[\"recording_count\"] += 1\n                    probe.recording_info[\"recording_datetimes\"].append(\n                        rec.datetime\n                        + datetime.timedelta(seconds=float(rec.start_time))\n                    )\n                    probe.recording_info[\"recording_durations\"].append(\n                        float(rec.duration)\n                    )\n                    probe.recording_info[\"recording_files\"].append(\n                        rec.absolute_foldername\n                        / \"continuous\"\n                        / continuous_info[\"folder_name\"]\n                    )\n                elif continuous_type == \"lfp\":\n                    probe.recording_info[\"recording_lfp_files\"].append(\n                        rec.absolute_foldername\n                        / \"continuous\"\n                        / continuous_info[\"folder_name\"]\n                    )\n\n                meta = getattr(probe, continuous_type + \"_meta\")\n                if not meta:\n                    # channel indices - 0-based indexing\n                    channels_indices = [\n                        int(re.search(r\"\\d+$\", chn_name).group()) - 1\n                        for chn_name in analog_signal.channel_names\n                    ]\n\n                    meta.update(\n                        **continuous_info,\n                        channels_indices=channels_indices,\n                        channels_ids=analog_signal.channel_ids,\n                        channels_names=analog_signal.channel_names,\n                        channels_gains=analog_signal.gains,\n                    )\n\n                signal = getattr(probe, continuous_type + \"_analog_signals\")\n                signal.append(analog_signal)\n\n    return probes\n</code></pre>"},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe","title":"<code>Probe</code>","text":"Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>class Probe:\n    def __init__(self, processor, probe_index=0):\n        processor_node_id = processor.get(\"@nodeId\", processor.get(\"@NodeId\"))\n        if processor_node_id is None:\n            raise KeyError('Neither \"@nodeId\" nor \"@NodeId\" key found')\n\n        self.processor_id = int(processor_node_id)\n\n        if (\n            processor[\"@pluginName\"] == \"Neuropix-3a\"\n            or \"NP_PROBE\" not in processor[\"EDITOR\"]\n        ):\n            self.probe_info = (\n                processor[\"EDITOR\"][\"PROBE\"]\n                if isinstance(processor[\"EDITOR\"][\"PROBE\"], dict)\n                else processor[\"EDITOR\"][\"PROBE\"][probe_index]\n            )\n            self.probe_SN = self.probe_info[\"@probe_serial_number\"]\n            self.probe_model = _probe_model_name_mapping[processor[\"@pluginName\"]]\n            self._channels_connected = {\n                int(re.search(r\"\\d+$\", k).group()): int(v)\n                for k, v in self.probe_info.pop(\"CHANNELSTATUS\").items()\n            }\n        else:  # Neuropix-PXI\n            self.probe_info = (\n                processor[\"EDITOR\"][\"NP_PROBE\"]\n                if isinstance(processor[\"EDITOR\"][\"NP_PROBE\"], dict)\n                else processor[\"EDITOR\"][\"NP_PROBE\"][probe_index]\n            )\n            self.probe_SN = self.probe_info[\"@probe_serial_number\"]\n            self.probe_model = _probe_model_name_mapping[self.probe_info[\"@probe_name\"]]\n\n            if \"ELECTRODE_XPOS\" in self.probe_info:\n                self.probe_info[\"ELECTRODE_XPOS\"] = {\n                    int(re.search(r\"\\d+$\", k).group()): int(v)\n                    for k, v in self.probe_info.pop(\"ELECTRODE_XPOS\").items()\n                }\n                self.probe_info[\"ELECTRODE_YPOS\"] = {\n                    int(re.search(r\"\\d+$\", k).group()): int(v)\n                    for k, v in self.probe_info.pop(\"ELECTRODE_YPOS\").items()\n                }\n                self.probe_info[\"ELECTRODE_SHANK\"] = {\n                    int(re.search(r\"\\d+$\", k).group()): int(v)\n                    for k, v in self.probe_info[\"CHANNELS\"].items()\n                }\n\n            self._channels_connected = {\n                int(re.search(r\"\\d+$\", k).group()): 1\n                for k in self.probe_info.pop(\"CHANNELS\")\n            }\n\n        self.ap_meta = {}\n        self.lfp_meta = {}\n\n        self.ap_analog_signals = []\n        self.lfp_analog_signals = []\n\n        self.recording_info = {\n            \"recording_count\": 0,\n            \"recording_datetimes\": [],\n            \"recording_durations\": [],\n            \"recording_files\": [],\n            \"recording_lfp_files\": [],\n        }\n\n        self._ap_timeseries = None\n        self._ap_timestamps = None\n        self._lfp_timeseries = None\n        self._lfp_timestamps = None\n\n    @property\n    def channels_connected(self):\n        return {\n            chn_idx: self._channels_connected.get(chn_idx, 0)\n            for chn_idx in self.ap_meta[\"channels_indices\"]\n        }\n\n    @property\n    def ap_timeseries(self):\n\"\"\"\n        AP data concatenated across recordings. Shape: (sample x channel)\n        Data are stored as int16 - to convert to microvolts,\n         multiply with self.ap_meta['channels_gains']\n        \"\"\"\n        if self._ap_timeseries is None:\n            self._ap_timeseries = np.hstack(\n                [s.signal for s in self.ap_analog_signals]\n            ).T\n        return self._ap_timeseries\n\n    @property\n    def ap_timestamps(self):\n        if self._ap_timestamps is None:\n            self._ap_timestamps = np.hstack([s.times for s in self.ap_analog_signals])\n        return self._ap_timestamps\n\n    @property\n    def lfp_timeseries(self):\n\"\"\"\n        LFP data concatenated across recordings. Shape: (sample x channel)\n        Data are stored as int16 - to convert to microvolts,\n         multiply with self.lfp_meta['channels_gains']\n        \"\"\"\n        if self._lfp_timeseries is None:\n            self._lfp_timeseries = np.hstack(\n                [s.signal for s in self.lfp_analog_signals]\n            ).T\n        return self._lfp_timeseries\n\n    @property\n    def lfp_timestamps(self):\n        if self._lfp_timestamps is None:\n            self._lfp_timestamps = np.hstack([s.times for s in self.lfp_analog_signals])\n        return self._lfp_timestamps\n\n    def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n        :param spikes: spike times (in second) to extract waveforms\n        :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms\n        :param n_wf: number of spikes per unit to extract the waveforms\n        :param wf_win: number of sample pre and post a spike\n        :return: waveforms (sample x channel x spike)\n        \"\"\"\n        channel_bit_volts = np.array(self.ap_meta[\"channels_gains\"])[channel_ind]\n\n        # ignore spikes at the beginning or end of raw data\n        spikes = spikes[\n            np.logical_and(\n                spikes &gt; (-wf_win[0] / self.ap_meta[\"sample_rate\"]),\n                spikes\n                &lt; (self.ap_timestamps.max() - wf_win[-1] / self.ap_meta[\"sample_rate\"]),\n            )\n        ]\n        # select a randomized set of \"n_wf\" spikes\n        np.random.shuffle(spikes)\n        spikes = spikes[:n_wf]\n        # extract waveforms\n        if len(spikes) &gt; 0:\n            spike_indices = np.searchsorted(self.ap_timestamps, spikes, side=\"left\")\n            # waveform at each spike: (sample x channel x spike)\n            spike_wfs = np.dstack(\n                [\n                    self.ap_timeseries[\n                        int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind\n                    ]\n                    * channel_bit_volts\n                    for spk in spike_indices\n                ]\n            )\n            return spike_wfs\n        else:  # if no spike found, return NaN of size (sample x channel x 1)\n            return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n\n    def compress(self):\n        from mtscomp import compress as mts_compress\n\n        ap_dirs = self.recording_info[\"recording_files\"]\n        lfp_dirs = self.recording_info[\"recording_lfp_files\"]\n\n        meta_mapping = {\"ap\": self.ap_meta, \"lfp\": self.lfp_meta}\n\n        compressed_files = []\n        for continuous_dir, continuous_type in zip(\n            ap_dirs + lfp_dirs, [\"ap\"] * len(ap_dirs) + [\"lfp\"] * len(lfp_dirs)\n        ):\n            dat_fp = continuous_dir / \"continuous.dat\"\n            if not dat_fp.exists():\n                raise FileNotFoundError(\n                    f'Compression error - \"{dat_fp}\" does not exist'\n                )\n            cdat_fp = continuous_dir / \"continuous.cdat\"\n            ch_fp = continuous_dir / \"continuous.ch\"\n\n            if cdat_fp.exists():\n                assert ch_fp.exists()\n                logger.info(f\"Compressed file exists ({cdat_fp}), skipping...\")\n                continue\n\n            try:\n                mts_compress(\n                    dat_fp,\n                    cdat_fp,\n                    ch_fp,\n                    sample_rate=meta_mapping[continuous_type][\"sample_rate\"],\n                    n_channels=meta_mapping[continuous_type][\"num_channels\"],\n                    dtype=np.memmap(dat_fp).dtype,\n                )\n            except Exception as e:\n                cdat_fp.unlink(missing_ok=True)\n                ch_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                compressed_files.append((cdat_fp, ch_fp))\n\n        return compressed_files\n\n    def decompress(self):\n        from mtscomp import decompress as mts_decompress\n\n        ap_dirs = self.recording_info[\"recording_files\"]\n        lfp_dirs = self.recording_info[\"recording_lfp_files\"]\n\n        decompressed_files = []\n        for continuous_dir, continuous_type in zip(\n            ap_dirs + lfp_dirs, [\"ap\"] * len(ap_dirs) + [\"lfp\"] * len(lfp_dirs)\n        ):\n            dat_fp = continuous_dir / \"continuous.dat\"\n\n            if dat_fp.exists():\n                logger.info(f\"Decompressed file exists ({dat_fp}), skipping...\")\n                continue\n\n            cdat_fp = continuous_dir / \"continuous.cdat\"\n            ch_fp = continuous_dir / \"continuous.ch\"\n\n            if not cdat_fp.exists():\n                raise FileNotFoundError(\n                    f'Decompression error - \"{cdat_fp}\" does not exist'\n                )\n\n            try:\n                decomp_arr = mts_decompress(cdat_fp, ch_fp)\n                decomp_arr.tofile(dat_fp)\n            except Exception as e:\n                dat_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                decompressed_files.append(dat_fp)\n\n        return decompressed_files\n</code></pre>"},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.ap_timeseries","title":"<code>ap_timeseries</code>  <code>property</code>","text":"<p>AP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts,  multiply with self.ap_meta['channels_gains']</p>"},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.lfp_timeseries","title":"<code>lfp_timeseries</code>  <code>property</code>","text":"<p>LFP data concatenated across recordings. Shape: (sample x channel) Data are stored as int16 - to convert to microvolts,  multiply with self.lfp_meta['channels_gains']</p>"},{"location":"api/element_array_ephys/readers/openephys/#element_array_ephys.readers.openephys.Probe.extract_spike_waveforms","title":"<code>extract_spike_waveforms(spikes, channel_ind, n_wf=500, wf_win=(-32, 32))</code>","text":"<p>:param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (sample x channel x spike)</p> Source code in <code>element_array_ephys/readers/openephys.py</code> <pre><code>def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n    :param spikes: spike times (in second) to extract waveforms\n    :param channel_ind: channel indices (of meta['channels_ids']) to extract waveforms\n    :param n_wf: number of spikes per unit to extract the waveforms\n    :param wf_win: number of sample pre and post a spike\n    :return: waveforms (sample x channel x spike)\n    \"\"\"\n    channel_bit_volts = np.array(self.ap_meta[\"channels_gains\"])[channel_ind]\n\n    # ignore spikes at the beginning or end of raw data\n    spikes = spikes[\n        np.logical_and(\n            spikes &gt; (-wf_win[0] / self.ap_meta[\"sample_rate\"]),\n            spikes\n            &lt; (self.ap_timestamps.max() - wf_win[-1] / self.ap_meta[\"sample_rate\"]),\n        )\n    ]\n    # select a randomized set of \"n_wf\" spikes\n    np.random.shuffle(spikes)\n    spikes = spikes[:n_wf]\n    # extract waveforms\n    if len(spikes) &gt; 0:\n        spike_indices = np.searchsorted(self.ap_timestamps, spikes, side=\"left\")\n        # waveform at each spike: (sample x channel x spike)\n        spike_wfs = np.dstack(\n            [\n                self.ap_timeseries[\n                    int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind\n                ]\n                * channel_bit_volts\n                for spk in spike_indices\n            ]\n        )\n        return spike_wfs\n    else:  # if no spike found, return NaN of size (sample x channel x 1)\n        return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/","title":"spikeglx.py","text":""},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX","title":"<code>SpikeGLX</code>","text":"Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>class SpikeGLX:\n    def __init__(self, root_dir):\n\"\"\"\n        create neuropixels reader from 'root name' - e.g. the recording:\n\n            /data/rec_1/npx_g0_t0.imec.ap.meta\n            /data/rec_1/npx_g0_t0.imec.ap.bin\n            /data/rec_1/npx_g0_t0.imec.lf.meta\n            /data/rec_1/npx_g0_t0.imec.lf.bin\n\n        would have a 'root name' of:\n\n            /data/rec_1/npx_g0_t0.imec\n\n        only a single recording is read/loaded via the root\n        name &amp; associated meta - no interpretation of g0_t0.imec, etc is\n        performed at this layer.\n        \"\"\"\n        self._apmeta, self._ap_timeseries = None, None\n        self._lfmeta, self._lf_timeseries = None, None\n\n        self.root_dir = pathlib.Path(root_dir)\n\n        try:\n            meta_filepath = next(pathlib.Path(root_dir).glob(\"*.ap.meta\"))\n        except StopIteration:\n            raise FileNotFoundError(f\"No SpikeGLX file (.ap.meta) found at: {root_dir}\")\n\n        self.root_name = meta_filepath.name.replace(\".ap.meta\", \"\")\n\n    @property\n    def apmeta(self):\n        if self._apmeta is None:\n            self._apmeta = SpikeGLXMeta(self.root_dir / (self.root_name + \".ap.meta\"))\n        return self._apmeta\n\n    @property\n    def ap_timeseries(self):\n\"\"\"\n        AP data: (sample x channel)\n        Data are stored as np.memmap with dtype: int16\n        - to convert to microvolts, multiply with self.get_channel_bit_volts('ap')\n        \"\"\"\n        if self._ap_timeseries is None:\n            self.validate_file(\"ap\")\n            self._ap_timeseries = self._read_bin(\n                self.root_dir / (self.root_name + \".ap.bin\")\n            )\n        return self._ap_timeseries\n\n    @property\n    def lfmeta(self):\n        if self._lfmeta is None:\n            self._lfmeta = SpikeGLXMeta(self.root_dir / (self.root_name + \".lf.meta\"))\n        return self._lfmeta\n\n    @property\n    def lf_timeseries(self):\n\"\"\"\n        LFP data: (sample x channel)\n        Data are stored as np.memmap with dtype: int16\n        - to convert to microvolts, multiply with self.get_channel_bit_volts('lf')\n        \"\"\"\n        if self._lf_timeseries is None:\n            self.validate_file(\"lf\")\n            self._lf_timeseries = self._read_bin(\n                self.root_dir / (self.root_name + \".lf.bin\")\n            )\n        return self._lf_timeseries\n\n    def get_channel_bit_volts(self, band=\"ap\"):\n\"\"\"\n        Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels\n        Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n                dataVolts = dataInt * Vmax / Imax / gain\n        \"\"\"\n        vmax = float(self.apmeta.meta[\"imAiRangeMax\"])\n\n        if band == \"ap\":\n            imax = IMAX[self.apmeta.probe_model]\n            imroTbl_data = self.apmeta.imroTbl[\"data\"]\n            imroTbl_idx = 3\n            chn_ind = self.apmeta.get_recording_channels_indices(exclude_sync=True)\n\n        elif band == \"lf\":\n            imax = IMAX[self.lfmeta.probe_model]\n            imroTbl_data = self.lfmeta.imroTbl[\"data\"]\n            imroTbl_idx = 4\n            chn_ind = self.lfmeta.get_recording_channels_indices(exclude_sync=True)\n        else:\n            raise ValueError(f'Unsupported band: {band} - Must be \"ap\" or \"lf\"')\n\n        # extract channels' gains\n        if \"imDatPrb_dock\" in self.apmeta.meta:\n            # NP 2.0; APGain = 80 for all AP (LF is computed from AP)\n            chn_gains = [AP_GAIN] * len(imroTbl_data)\n        else:\n            # 3A, 3B1, 3B2 (NP 1.0)\n            chn_gains = [c[imroTbl_idx] for c in imroTbl_data]\n\n        chn_gains = np.array(chn_gains)[chn_ind]\n\n        return vmax / imax / chn_gains * 1e6  # convert to uV as well\n\n    def _read_bin(self, fname):\n        nchan = self.apmeta.meta[\"nSavedChans\"]\n        dtype = np.dtype((np.int16, nchan))\n        return np.memmap(fname, dtype, \"r\")\n\n    def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n        :param spikes: spike times (in second) to extract waveforms\n        :param channel_ind: channel indices (of shankmap) to extract the waveforms from\n        :param n_wf: number of spikes per unit to extract the waveforms\n        :param wf_win: number of sample pre and post a spike\n        :return: waveforms (in uV) - shape: (sample x channel x spike)\n        \"\"\"\n        channel_bit_volts = self.get_channel_bit_volts(\"ap\")[channel_ind]\n\n        data = self.ap_timeseries\n\n        spikes = np.round(spikes * self.apmeta.meta[\"imSampRate\"]).astype(\n            int\n        )  # convert to sample\n        # ignore spikes at the beginning or end of raw data\n        spikes = spikes[\n            np.logical_and(spikes &gt; -wf_win[0], spikes &lt; data.shape[0] - wf_win[-1])\n        ]\n\n        np.random.shuffle(spikes)\n        spikes = spikes[:n_wf]\n        if len(spikes) &gt; 0:\n            # waveform at each spike: (sample x channel x spike)\n            spike_wfs = np.dstack(\n                [\n                    data[int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind]\n                    * channel_bit_volts\n                    for spk in spikes\n                ]\n            )\n            return spike_wfs\n        else:  # if no spike found, return NaN of size (sample x channel x 1)\n            return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n\n    def validate_file(self, file_type=\"ap\"):\n        file_path = self.root_dir / (self.root_name + f\".{file_type}.bin\")\n        file_size = file_path.stat().st_size\n\n        if file_type == \"ap\":\n            meta = self.apmeta\n        elif file_type == \"lf\":\n            meta = self.lfmeta\n        else:\n            raise KeyError(f\"Unknown file_type {file_type} - must be 'ap' or 'lf'\")\n\n        if file_size != meta.meta[\"fileSizeBytes\"]:\n            raise IOError(\n                f\"File size error! {file_path} may be corrupted or in transfer?\"\n            )\n\n    def compress(self):\n        from mtscomp import compress as mts_compress\n\n        ap_file = self.root_dir / (self.root_name + \".ap.bin\")\n        lfp_file = self.root_dir / (self.root_name + \".lf.bin\")\n\n        meta_mapping = {\"ap\": self.apmeta, \"lfp\": self.lfmeta}\n\n        compressed_files = []\n        for bin_fp, band_type in zip([ap_file, lfp_file], [\"ap\", \"lfp\"]):\n            if not bin_fp.exists():\n                raise FileNotFoundError(\n                    f'Compression error - \"{bin_fp}\" does not exist'\n                )\n            cbin_fp = bin_fp.parent / f\"{bin_fp.stem}.cbin\"\n            ch_fp = bin_fp.parent / f\"{bin_fp.stem}.ch\"\n\n            if cbin_fp.exists():\n                assert ch_fp.exists()\n                logger.info(f\"Compressed file exists ({cbin_fp}), skipping...\")\n                continue\n\n            try:\n                mts_compress(\n                    bin_fp,\n                    cbin_fp,\n                    ch_fp,\n                    sample_rate=meta_mapping[band_type][\"sample_rate\"],\n                    n_channels=meta_mapping[band_type][\"num_channels\"],\n                    dtype=np.memmap(bin_fp).dtype,\n                )\n            except Exception as e:\n                cbin_fp.unlink(missing_ok=True)\n                ch_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                compressed_files.append((cbin_fp, ch_fp))\n\n        return compressed_files\n\n    def decompress(self):\n        from mtscomp import decompress as mts_decompress\n\n        ap_file = self.root_dir / (self.root_name + \".ap.bin\")\n        lfp_file = self.root_dir / (self.root_name + \".lf.bin\")\n\n        decompressed_files = []\n        for bin_fp, band_type in zip([ap_file, lfp_file], [\"ap\", \"lfp\"]):\n            if bin_fp.exists():\n                logger.info(f\"Decompressed file exists ({bin_fp}), skipping...\")\n                continue\n\n            cbin_fp = bin_fp.parent / f\"{bin_fp.stem}.cbin\"\n            ch_fp = bin_fp.parent / f\"{bin_fp.stem}.ch\"\n\n            if not cbin_fp.exists():\n                raise FileNotFoundError(\n                    f'Decompression error - \"{cbin_fp}\" does not exist'\n                )\n\n            try:\n                decomp_arr = mts_decompress(cbin_fp, ch_fp)\n                decomp_arr.tofile(bin_fp)\n            except Exception as e:\n                bin_fp.unlink(missing_ok=True)\n                raise e\n            else:\n                decompressed_files.append(bin_fp)\n\n        return decompressed_files\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.ap_timeseries","title":"<code>ap_timeseries</code>  <code>property</code>","text":"<p>AP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('ap')</p>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.lf_timeseries","title":"<code>lf_timeseries</code>  <code>property</code>","text":"<p>LFP data: (sample x channel) Data are stored as np.memmap with dtype: int16 - to convert to microvolts, multiply with self.get_channel_bit_volts('lf')</p>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.__init__","title":"<code>__init__(root_dir)</code>","text":"<p>create neuropixels reader from 'root name' - e.g. the recording:</p> <pre><code>/data/rec_1/npx_g0_t0.imec.ap.meta\n/data/rec_1/npx_g0_t0.imec.ap.bin\n/data/rec_1/npx_g0_t0.imec.lf.meta\n/data/rec_1/npx_g0_t0.imec.lf.bin\n</code></pre> <p>would have a 'root name' of:</p> <pre><code>/data/rec_1/npx_g0_t0.imec\n</code></pre> <p>only a single recording is read/loaded via the root name &amp; associated meta - no interpretation of g0_t0.imec, etc is performed at this layer.</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def __init__(self, root_dir):\n\"\"\"\n    create neuropixels reader from 'root name' - e.g. the recording:\n\n        /data/rec_1/npx_g0_t0.imec.ap.meta\n        /data/rec_1/npx_g0_t0.imec.ap.bin\n        /data/rec_1/npx_g0_t0.imec.lf.meta\n        /data/rec_1/npx_g0_t0.imec.lf.bin\n\n    would have a 'root name' of:\n\n        /data/rec_1/npx_g0_t0.imec\n\n    only a single recording is read/loaded via the root\n    name &amp; associated meta - no interpretation of g0_t0.imec, etc is\n    performed at this layer.\n    \"\"\"\n    self._apmeta, self._ap_timeseries = None, None\n    self._lfmeta, self._lf_timeseries = None, None\n\n    self.root_dir = pathlib.Path(root_dir)\n\n    try:\n        meta_filepath = next(pathlib.Path(root_dir).glob(\"*.ap.meta\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No SpikeGLX file (.ap.meta) found at: {root_dir}\")\n\n    self.root_name = meta_filepath.name.replace(\".ap.meta\", \"\")\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.extract_spike_waveforms","title":"<code>extract_spike_waveforms(spikes, channel_ind, n_wf=500, wf_win=(-32, 32))</code>","text":"<p>:param spikes: spike times (in second) to extract waveforms :param channel_ind: channel indices (of shankmap) to extract the waveforms from :param n_wf: number of spikes per unit to extract the waveforms :param wf_win: number of sample pre and post a spike :return: waveforms (in uV) - shape: (sample x channel x spike)</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def extract_spike_waveforms(self, spikes, channel_ind, n_wf=500, wf_win=(-32, 32)):\n\"\"\"\n    :param spikes: spike times (in second) to extract waveforms\n    :param channel_ind: channel indices (of shankmap) to extract the waveforms from\n    :param n_wf: number of spikes per unit to extract the waveforms\n    :param wf_win: number of sample pre and post a spike\n    :return: waveforms (in uV) - shape: (sample x channel x spike)\n    \"\"\"\n    channel_bit_volts = self.get_channel_bit_volts(\"ap\")[channel_ind]\n\n    data = self.ap_timeseries\n\n    spikes = np.round(spikes * self.apmeta.meta[\"imSampRate\"]).astype(\n        int\n    )  # convert to sample\n    # ignore spikes at the beginning or end of raw data\n    spikes = spikes[\n        np.logical_and(spikes &gt; -wf_win[0], spikes &lt; data.shape[0] - wf_win[-1])\n    ]\n\n    np.random.shuffle(spikes)\n    spikes = spikes[:n_wf]\n    if len(spikes) &gt; 0:\n        # waveform at each spike: (sample x channel x spike)\n        spike_wfs = np.dstack(\n            [\n                data[int(spk + wf_win[0]) : int(spk + wf_win[-1]), channel_ind]\n                * channel_bit_volts\n                for spk in spikes\n            ]\n        )\n        return spike_wfs\n    else:  # if no spike found, return NaN of size (sample x channel x 1)\n        return np.full((len(range(*wf_win)), len(channel_ind), 1), np.nan)\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLX.get_channel_bit_volts","title":"<code>get_channel_bit_volts(band='ap')</code>","text":"<p>Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels</p> https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip <p>dataVolts = dataInt * Vmax / Imax / gain</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def get_channel_bit_volts(self, band=\"ap\"):\n\"\"\"\n    Extract the recorded AP and LF channels' int16 to microvolts - no Sync (SY) channels\n    Following the steps specified in: https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n            dataVolts = dataInt * Vmax / Imax / gain\n    \"\"\"\n    vmax = float(self.apmeta.meta[\"imAiRangeMax\"])\n\n    if band == \"ap\":\n        imax = IMAX[self.apmeta.probe_model]\n        imroTbl_data = self.apmeta.imroTbl[\"data\"]\n        imroTbl_idx = 3\n        chn_ind = self.apmeta.get_recording_channels_indices(exclude_sync=True)\n\n    elif band == \"lf\":\n        imax = IMAX[self.lfmeta.probe_model]\n        imroTbl_data = self.lfmeta.imroTbl[\"data\"]\n        imroTbl_idx = 4\n        chn_ind = self.lfmeta.get_recording_channels_indices(exclude_sync=True)\n    else:\n        raise ValueError(f'Unsupported band: {band} - Must be \"ap\" or \"lf\"')\n\n    # extract channels' gains\n    if \"imDatPrb_dock\" in self.apmeta.meta:\n        # NP 2.0; APGain = 80 for all AP (LF is computed from AP)\n        chn_gains = [AP_GAIN] * len(imroTbl_data)\n    else:\n        # 3A, 3B1, 3B2 (NP 1.0)\n        chn_gains = [c[imroTbl_idx] for c in imroTbl_data]\n\n    chn_gains = np.array(chn_gains)[chn_ind]\n\n    return vmax / imax / chn_gains * 1e6  # convert to uV as well\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta","title":"<code>SpikeGLXMeta</code>","text":"Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>class SpikeGLXMeta:\n    def __init__(self, meta_filepath):\n\"\"\"\n        Some good processing references:\n            https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n            https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m\n        \"\"\"\n\n        self.fname = meta_filepath\n        self.meta = _read_meta(meta_filepath)\n\n        # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0)\n        probe_model = self.meta.get(\"imDatPrb_type\", 1)\n        if probe_model &lt;= 1:\n            if \"typeEnabled\" in self.meta:\n                self.probe_model = \"neuropixels 1.0 - 3A\"\n            elif \"typeImEnabled\" in self.meta:\n                self.probe_model = \"neuropixels 1.0 - 3B\"\n        elif probe_model == 1100:\n            self.probe_model = \"neuropixels UHD\"\n        elif probe_model == 21:\n            self.probe_model = \"neuropixels 2.0 - SS\"\n        elif probe_model == 24:\n            self.probe_model = \"neuropixels 2.0 - MS\"\n        else:\n            self.probe_model = str(probe_model)\n\n        # Get recording time\n        self.recording_time = datetime.strptime(\n            self.meta.get(\"fileCreateTime_original\", self.meta[\"fileCreateTime\"]),\n            \"%Y-%m-%dT%H:%M:%S\",\n        )\n        self.recording_duration = self.meta.get(\"fileTimeSecs\")\n\n        # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B\n        try:\n            self.probe_SN = self.meta.get(\"imProbeSN\", self.meta.get(\"imDatPrb_sn\"))\n        except KeyError:\n            raise KeyError(\n                \"Probe Serial Number not found in\"\n                ' either \"imProbeSN\" or \"imDatPrb_sn\"'\n            )\n\n        self.chanmap = (\n            self._parse_chanmap(self.meta[\"~snsChanMap\"])\n            if \"~snsChanMap\" in self.meta\n            else None\n        )\n        self.shankmap = (\n            self._parse_shankmap(self.meta[\"~snsShankMap\"])\n            if \"~snsShankMap\" in self.meta\n            else None\n        )\n        self.imroTbl = (\n            self._parse_imrotbl(self.meta[\"~imroTbl\"])\n            if \"~imroTbl\" in self.meta\n            else None\n        )\n\n        # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap\n        self.recording_channels = np.arange(len(self.imroTbl[\"data\"]))[\n            self.get_recording_channels_indices(exclude_sync=True)\n        ]\n\n    @staticmethod\n    def _parse_chanmap(raw):\n\"\"\"\n        https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#channel-map\n        Parse channel map header structure. Converts:\n\n            '(x,y,z)(c0,x:y)...(cI,x:y),(sy0;x:y)'\n\n        e.g:\n\n            '(384,384,1)(AP0;0:0)...(AP383;383:383)(SY0;768:768)'\n\n        into dict of form:\n\n            {'shape': [x,y,z], 'c0': [x,y], ... }\n        \"\"\"\n\n        res = {}\n        for u in (i.rstrip(\")\").split(\";\") for i in raw.split(\"(\") if i != \"\"):\n            if (len(u)) == 1:\n                res[\"shape\"] = u[0].split(\",\")\n            else:\n                res[u[0]] = u[1].split(\":\")\n\n        return res\n\n    @staticmethod\n    def _parse_shankmap(raw):\n\"\"\"\n        The shankmap contains details on the shank info\n            for each electrode sites of the sites being recorded only\n\n        https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#shank-map\n        Parse shank map header structure. Converts:\n\n            '(x,y,z)(a:b:c:d)...(a:b:c:d)'\n\n        e.g:\n\n            '(1,2,480)(0:0:192:1)...(0:1:191:1)'\n\n        into dict of form:\n\n            {'shape': [x,y,z], 'data': [[a,b,c,d],...]}\n        \"\"\"\n        res = {\"shape\": None, \"data\": []}\n\n        for u in (i.rstrip(\")\") for i in raw.split(\"(\") if i != \"\"):\n            if \",\" in u:\n                res[\"shape\"] = [int(d) for d in u.split(\",\")]\n            else:\n                res[\"data\"].append([int(d) for d in u.split(\":\")])\n\n        return res\n\n    @staticmethod\n    def _parse_imrotbl(raw):\n\"\"\"\n        The imro table contains info for all electrode sites (no sync)\n            for a particular electrode configuration (all 384 sites)\n        Note: not all of these 384 sites are necessarily recorded\n\n        https://github.com/billkarsh/SpikeGLX/blob/master/Markdown/UserManual.md#imro-per-channel-settings\n        Parse imro tbl structure. Converts:\n\n            '(X,Y,Z)(A B C D E)...(A B C D E)'\n\n        e.g.:\n\n            '(641251209,3,384)(0 1 0 500 250)...(383 0 0 500 250)'\n\n        into dict of form:\n\n            {'shape': (x,y,z), 'data': []}\n        \"\"\"\n        res = {\"shape\": None, \"data\": []}\n\n        for u in (i.rstrip(\")\") for i in raw.split(\"(\") if i != \"\"):\n            if \",\" in u:\n                res[\"shape\"] = [int(d) for d in u.split(\",\")]\n            else:\n                res[\"data\"].append([int(d) for d in u.split(\" \")])\n\n        return res\n\n    def get_recording_channels_indices(self, exclude_sync=False):\n\"\"\"\n        The indices of recorded channels (in chanmap)\n         with respect to the channels listed in the imro table\n        \"\"\"\n        recorded_chns_ind = [\n            int(v[0])\n            for k, v in self.chanmap.items()\n            if k != \"shape\" and (not k.startswith(\"SY\") if exclude_sync else True)\n        ]\n        orig_chns_ind = self.get_original_chans()\n        _, _, chns_ind = np.intersect1d(\n            orig_chns_ind, recorded_chns_ind, return_indices=True\n        )\n        return chns_ind\n\n    def get_original_chans(self):\n\"\"\"\n        Because you can selectively save channels, the\n        ith channel in the file isn't necessarily the ith acquired channel.\n        Use this function to convert from ith stored to original index.\n\n        Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n            OriginalChans() function\n        \"\"\"\n        if self.meta[\"snsSaveChanSubset\"] == \"all\":\n            # output = int32, 0 to nSavedChans - 1\n            channels = np.arange(0, int(self.meta[\"nSavedChans\"]))\n        else:\n            # parse the channel list self.meta['snsSaveChanSubset']\n            channels = np.arange(0)  # empty array\n            for channel_range in self.meta[\"snsSaveChanSubset\"].split(\",\"):\n                # a block of contiguous channels specified as chan or chan1:chan2 inclusive\n                ix = [int(r) for r in channel_range.split(\":\")]\n                assert len(ix) in (\n                    1,\n                    2,\n                ), f\"Invalid channel range spec '{channel_range}'\"\n                channels = np.append(channels, np.r_[ix[0] : ix[-1] + 1])\n        return channels\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.__init__","title":"<code>__init__(meta_filepath)</code>","text":"Some good processing references <p>https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def __init__(self, meta_filepath):\n\"\"\"\n    Some good processing references:\n        https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n        https://github.com/jenniferColonell/Neuropixels_evaluation_tools/blob/master/SGLXMetaToCoords.m\n    \"\"\"\n\n    self.fname = meta_filepath\n    self.meta = _read_meta(meta_filepath)\n\n    # Infer npx probe model (e.g. 1.0 (3A, 3B) or 2.0)\n    probe_model = self.meta.get(\"imDatPrb_type\", 1)\n    if probe_model &lt;= 1:\n        if \"typeEnabled\" in self.meta:\n            self.probe_model = \"neuropixels 1.0 - 3A\"\n        elif \"typeImEnabled\" in self.meta:\n            self.probe_model = \"neuropixels 1.0 - 3B\"\n    elif probe_model == 1100:\n        self.probe_model = \"neuropixels UHD\"\n    elif probe_model == 21:\n        self.probe_model = \"neuropixels 2.0 - SS\"\n    elif probe_model == 24:\n        self.probe_model = \"neuropixels 2.0 - MS\"\n    else:\n        self.probe_model = str(probe_model)\n\n    # Get recording time\n    self.recording_time = datetime.strptime(\n        self.meta.get(\"fileCreateTime_original\", self.meta[\"fileCreateTime\"]),\n        \"%Y-%m-%dT%H:%M:%S\",\n    )\n    self.recording_duration = self.meta.get(\"fileTimeSecs\")\n\n    # Get probe serial number - 'imProbeSN' for 3A and 'imDatPrb_sn' for 3B\n    try:\n        self.probe_SN = self.meta.get(\"imProbeSN\", self.meta.get(\"imDatPrb_sn\"))\n    except KeyError:\n        raise KeyError(\n            \"Probe Serial Number not found in\"\n            ' either \"imProbeSN\" or \"imDatPrb_sn\"'\n        )\n\n    self.chanmap = (\n        self._parse_chanmap(self.meta[\"~snsChanMap\"])\n        if \"~snsChanMap\" in self.meta\n        else None\n    )\n    self.shankmap = (\n        self._parse_shankmap(self.meta[\"~snsShankMap\"])\n        if \"~snsShankMap\" in self.meta\n        else None\n    )\n    self.imroTbl = (\n        self._parse_imrotbl(self.meta[\"~imroTbl\"])\n        if \"~imroTbl\" in self.meta\n        else None\n    )\n\n    # Channels being recorded, exclude Sync channels - basically a 1-1 mapping to shankmap\n    self.recording_channels = np.arange(len(self.imroTbl[\"data\"]))[\n        self.get_recording_channels_indices(exclude_sync=True)\n    ]\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_original_chans","title":"<code>get_original_chans()</code>","text":"<p>Because you can selectively save channels, the ith channel in the file isn't necessarily the ith acquired channel. Use this function to convert from ith stored to original index.</p> <p>Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip     OriginalChans() function</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def get_original_chans(self):\n\"\"\"\n    Because you can selectively save channels, the\n    ith channel in the file isn't necessarily the ith acquired channel.\n    Use this function to convert from ith stored to original index.\n\n    Credit to https://billkarsh.github.io/SpikeGLX/Support/SpikeGLX_Datafile_Tools.zip\n        OriginalChans() function\n    \"\"\"\n    if self.meta[\"snsSaveChanSubset\"] == \"all\":\n        # output = int32, 0 to nSavedChans - 1\n        channels = np.arange(0, int(self.meta[\"nSavedChans\"]))\n    else:\n        # parse the channel list self.meta['snsSaveChanSubset']\n        channels = np.arange(0)  # empty array\n        for channel_range in self.meta[\"snsSaveChanSubset\"].split(\",\"):\n            # a block of contiguous channels specified as chan or chan1:chan2 inclusive\n            ix = [int(r) for r in channel_range.split(\":\")]\n            assert len(ix) in (\n                1,\n                2,\n            ), f\"Invalid channel range spec '{channel_range}'\"\n            channels = np.append(channels, np.r_[ix[0] : ix[-1] + 1])\n    return channels\n</code></pre>"},{"location":"api/element_array_ephys/readers/spikeglx/#element_array_ephys.readers.spikeglx.SpikeGLXMeta.get_recording_channels_indices","title":"<code>get_recording_channels_indices(exclude_sync=False)</code>","text":"<p>The indices of recorded channels (in chanmap)  with respect to the channels listed in the imro table</p> Source code in <code>element_array_ephys/readers/spikeglx.py</code> <pre><code>def get_recording_channels_indices(self, exclude_sync=False):\n\"\"\"\n    The indices of recorded channels (in chanmap)\n     with respect to the channels listed in the imro table\n    \"\"\"\n    recorded_chns_ind = [\n        int(v[0])\n        for k, v in self.chanmap.items()\n        if k != \"shape\" and (not k.startswith(\"SY\") if exclude_sync else True)\n    ]\n    orig_chns_ind = self.get_original_chans()\n    _, _, chns_ind = np.intersect1d(\n        orig_chns_ind, recorded_chns_ind, return_indices=True\n    )\n    return chns_ind\n</code></pre>"},{"location":"api/element_array_ephys/readers/utils/","title":"utils.py","text":""},{"location":"api/workflow_array_ephys/analysis/","title":"analysis.py","text":""},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment","title":"<code>SpikesAlignment</code>","text":"<p>         Bases: <code>dj.Computed</code></p> <p>Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>@schema\nclass SpikesAlignment(dj.Computed):\n\"\"\"Spike alignment table pairing AlignedTrialSpikes and by-unit PSTH\"\"\"\n\n    definition = \"\"\"\n    -&gt; SpikesAlignmentCondition\n    \"\"\"\n\n    class AlignedTrialSpikes(dj.Part):\n\"\"\"Spike activity aligned to the event time within the designated window\n\n        Attributes:\n            SpikesAlignmentCondition.Trial (foreign key)\n            ephys.CuratedClustering.Unit  (foreign key)\n            SpikesAlignmentCondition.Trial (foreign key)\n            aligned_spike_times (longblob): (s) spike times relative to alignment event\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; ephys.CuratedClustering.Unit\n        -&gt; SpikesAlignmentCondition.Trial\n        ---\n        aligned_spike_times: longblob # (s) spike times relative to alignment event time\n        \"\"\"\n\n    class UnitPSTH(dj.Part):\n\"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit\n\n        Attributes:\n            SpikesAlignment (foreign key)\n            ephys.CuratedClustering.Unit (foreign key)\n            psth (longblob): event-aligned spike peristimulus time histogram (PSTH)\n            psth_edges (longblob)\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; ephys.CuratedClustering.Unit\n        ---\n        psth: longblob  # event-aligned spike peristimulus time histogram (PSTH)\n        psth_edges: longblob  \n        \"\"\"\n\n    def make(self, key: dict):\n\"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH\n\n        Args:\n            key (dict): Dict uniquely identifying one SpikesAlignmentCondition\n        \"\"\"\n        unit_keys, unit_spike_times = (\n            _linking_module.ephys.CuratedClustering.Unit &amp; key\n        ).fetch(\"KEY\", \"spike_times\", order_by=\"unit\")\n        bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n\n        trialized_event_times = (\n            _linking_module.trial.get_trialized_alignment_event_times(\n                key,\n                _linking_module.trial.Trial &amp; (SpikesAlignmentCondition.Trial &amp; key),\n            )\n        )\n\n        min_limit = (trialized_event_times.event - trialized_event_times.start).max()\n        max_limit = (trialized_event_times.end - trialized_event_times.event).max()\n\n        # Spike raster\n        aligned_trial_spikes = []\n        units_spike_raster = {\n            u[\"unit\"]: {**key, **u, \"aligned_spikes\": []} for u in unit_keys\n        }\n        for _, r in trialized_event_times.iterrows():\n            if np.isnan(r.event):\n                continue\n            alignment_start_time = r.event - min_limit\n            alignment_end_time = r.event + max_limit\n            for unit_key, spikes in zip(unit_keys, unit_spike_times):\n                aligned_spikes = (\n                    spikes[\n                        (alignment_start_time &lt;= spikes) &amp; (spikes &lt; alignment_end_time)\n                    ]\n                    - r.event\n                )\n                aligned_trial_spikes.append(\n                    {\n                        **key,\n                        **unit_key,\n                        **r.trial_key,\n                        \"aligned_spike_times\": aligned_spikes,\n                    }\n                )\n                units_spike_raster[unit_key[\"unit\"]][\"aligned_spikes\"].append(\n                    aligned_spikes\n                )\n\n        # PSTH\n        for unit_spike_raster in units_spike_raster.values():\n            spikes = np.concatenate(unit_spike_raster[\"aligned_spikes\"])\n\n            psth, edges = np.histogram(\n                spikes, bins=np.arange(-min_limit, max_limit, bin_size)\n            )\n            unit_spike_raster[\"psth\"] = (\n                psth / len(unit_spike_raster.pop(\"aligned_spikes\")) / bin_size\n            )\n            unit_spike_raster[\"psth_edges\"] = edges[1:]\n\n        self.insert1(key)\n        self.AlignedTrialSpikes.insert(aligned_trial_spikes)\n        self.UnitPSTH.insert(list(units_spike_raster.values()))\n\n    def plot(self, key: dict, unit: int, axs: tuple = None) -&gt; plt.figure.Figure:\n\"\"\"Plot event-aligned and trial-averaged spiking\n\n        Args:\n            key (dict): key of SpikesAlignmentCondition master table\n            unit (int): ID of ephys.CuratedClustering.Unit table\n            axs (tuple, optional): Definition of axes for plot.\n                Default is plt.subplots(2, 1, figsize=(12, 8))\n\n        Returns:\n            fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes\n        \"\"\"\n        from .plotting import plot_psth\n\n        fig = None\n        if axs is None:\n            fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n\n        bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n        trial_ids, aligned_spikes = (\n            self.AlignedTrialSpikes &amp; key &amp; {\"unit\": unit}\n        ).fetch(\"trial_id\", \"aligned_spike_times\")\n        psth, psth_edges = (self.UnitPSTH &amp; key &amp; {\"unit\": unit}).fetch1(\n            \"psth\", \"psth_edges\"\n        )\n\n        xlim = psth_edges[0], psth_edges[-1]\n\n        plot_psth._plot_spike_raster(\n            aligned_spikes,\n            trial_ids=trial_ids,\n            ax=axs[0],\n            title=f\"{dict(**key, unit=unit)}\",\n            xlim=xlim,\n        )\n        plot_psth._plot_psth(psth, psth_edges, bin_size, ax=axs[1], title=\"\", xlim=xlim)\n\n        return fig\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.AlignedTrialSpikes","title":"<code>AlignedTrialSpikes</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Spike activity aligned to the event time within the designated window</p> <p>Attributes:</p> Name Type Description <code>aligned_spike_times</code> <code>longblob</code> <p>(s) spike times relative to alignment event</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>class AlignedTrialSpikes(dj.Part):\n\"\"\"Spike activity aligned to the event time within the designated window\n\n    Attributes:\n        SpikesAlignmentCondition.Trial (foreign key)\n        ephys.CuratedClustering.Unit  (foreign key)\n        SpikesAlignmentCondition.Trial (foreign key)\n        aligned_spike_times (longblob): (s) spike times relative to alignment event\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; ephys.CuratedClustering.Unit\n    -&gt; SpikesAlignmentCondition.Trial\n    ---\n    aligned_spike_times: longblob # (s) spike times relative to alignment event time\n    \"\"\"\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.UnitPSTH","title":"<code>UnitPSTH</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Event-aligned spike peristimulus time histogram (PSTH) by unit</p> <p>Attributes:</p> Name Type Description <code>psth</code> <code>longblob</code> <p>event-aligned spike peristimulus time histogram (PSTH)</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>class UnitPSTH(dj.Part):\n\"\"\"Event-aligned spike peristimulus time histogram (PSTH) by unit\n\n    Attributes:\n        SpikesAlignment (foreign key)\n        ephys.CuratedClustering.Unit (foreign key)\n        psth (longblob): event-aligned spike peristimulus time histogram (PSTH)\n        psth_edges (longblob)\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; ephys.CuratedClustering.Unit\n    ---\n    psth: longblob  # event-aligned spike peristimulus time histogram (PSTH)\n    psth_edges: longblob  \n    \"\"\"\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.make","title":"<code>make(key)</code>","text":"<p>Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Dict uniquely identifying one SpikesAlignmentCondition</p> required Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>def make(self, key: dict):\n\"\"\"Populate SpikesAlignment, AlignedTrialSpikes and UnitPSTH\n\n    Args:\n        key (dict): Dict uniquely identifying one SpikesAlignmentCondition\n    \"\"\"\n    unit_keys, unit_spike_times = (\n        _linking_module.ephys.CuratedClustering.Unit &amp; key\n    ).fetch(\"KEY\", \"spike_times\", order_by=\"unit\")\n    bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n\n    trialized_event_times = (\n        _linking_module.trial.get_trialized_alignment_event_times(\n            key,\n            _linking_module.trial.Trial &amp; (SpikesAlignmentCondition.Trial &amp; key),\n        )\n    )\n\n    min_limit = (trialized_event_times.event - trialized_event_times.start).max()\n    max_limit = (trialized_event_times.end - trialized_event_times.event).max()\n\n    # Spike raster\n    aligned_trial_spikes = []\n    units_spike_raster = {\n        u[\"unit\"]: {**key, **u, \"aligned_spikes\": []} for u in unit_keys\n    }\n    for _, r in trialized_event_times.iterrows():\n        if np.isnan(r.event):\n            continue\n        alignment_start_time = r.event - min_limit\n        alignment_end_time = r.event + max_limit\n        for unit_key, spikes in zip(unit_keys, unit_spike_times):\n            aligned_spikes = (\n                spikes[\n                    (alignment_start_time &lt;= spikes) &amp; (spikes &lt; alignment_end_time)\n                ]\n                - r.event\n            )\n            aligned_trial_spikes.append(\n                {\n                    **key,\n                    **unit_key,\n                    **r.trial_key,\n                    \"aligned_spike_times\": aligned_spikes,\n                }\n            )\n            units_spike_raster[unit_key[\"unit\"]][\"aligned_spikes\"].append(\n                aligned_spikes\n            )\n\n    # PSTH\n    for unit_spike_raster in units_spike_raster.values():\n        spikes = np.concatenate(unit_spike_raster[\"aligned_spikes\"])\n\n        psth, edges = np.histogram(\n            spikes, bins=np.arange(-min_limit, max_limit, bin_size)\n        )\n        unit_spike_raster[\"psth\"] = (\n            psth / len(unit_spike_raster.pop(\"aligned_spikes\")) / bin_size\n        )\n        unit_spike_raster[\"psth_edges\"] = edges[1:]\n\n    self.insert1(key)\n    self.AlignedTrialSpikes.insert(aligned_trial_spikes)\n    self.UnitPSTH.insert(list(units_spike_raster.values()))\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignment.plot","title":"<code>plot(key, unit, axs=None)</code>","text":"<p>Plot event-aligned and trial-averaged spiking</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>key of SpikesAlignmentCondition master table</p> required <code>unit</code> <code>int</code> <p>ID of ephys.CuratedClustering.Unit table</p> required <code>axs</code> <code>tuple</code> <p>Definition of axes for plot. Default is plt.subplots(2, 1, figsize=(12, 8))</p> <code>None</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib.figure.Figure</code> <p>Plot event-aligned and trial-averaged spikes</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>def plot(self, key: dict, unit: int, axs: tuple = None) -&gt; plt.figure.Figure:\n\"\"\"Plot event-aligned and trial-averaged spiking\n\n    Args:\n        key (dict): key of SpikesAlignmentCondition master table\n        unit (int): ID of ephys.CuratedClustering.Unit table\n        axs (tuple, optional): Definition of axes for plot.\n            Default is plt.subplots(2, 1, figsize=(12, 8))\n\n    Returns:\n        fig (matplotlib.figure.Figure): Plot event-aligned and trial-averaged spikes\n    \"\"\"\n    from .plotting import plot_psth\n\n    fig = None\n    if axs is None:\n        fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n\n    bin_size = (SpikesAlignmentCondition &amp; key).fetch1(\"bin_size\")\n    trial_ids, aligned_spikes = (\n        self.AlignedTrialSpikes &amp; key &amp; {\"unit\": unit}\n    ).fetch(\"trial_id\", \"aligned_spike_times\")\n    psth, psth_edges = (self.UnitPSTH &amp; key &amp; {\"unit\": unit}).fetch1(\n        \"psth\", \"psth_edges\"\n    )\n\n    xlim = psth_edges[0], psth_edges[-1]\n\n    plot_psth._plot_spike_raster(\n        aligned_spikes,\n        trial_ids=trial_ids,\n        ax=axs[0],\n        title=f\"{dict(**key, unit=unit)}\",\n        xlim=xlim,\n    )\n    plot_psth._plot_psth(psth, psth_edges, bin_size, ax=axs[1], title=\"\", xlim=xlim)\n\n    return fig\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignmentCondition","title":"<code>SpikesAlignmentCondition</code>","text":"<p>         Bases: <code>dj.Manual</code></p> <p>Alignment activity table</p> <p>Attributes:</p> Name Type Description <code>trial_condition</code> <p>varchar(128) # user-friendly name of condition</p> <code>condition_description</code> <code> varchar(1000), nullable</code> <p>condition description</p> <code>bin_size</code> <code>float</code> <p>Bin-size (in second) used to compute the PSTH Default 0.04</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>@schema\nclass SpikesAlignmentCondition(dj.Manual):\n\"\"\"Alignment activity table\n\n    Attributes:\n        ephys.CuratedClustering (foreign key)\n        event.AlignmentEvent (foreign key)\n        trial_condition: varchar(128) # user-friendly name of condition\n        condition_description ( varchar(1000), nullable): condition description\n        bin_size (float, optional): Bin-size (in second) used to compute the PSTH\n            Default 0.04\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; ephys.CuratedClustering\n    -&gt; event.AlignmentEvent\n    trial_condition: varchar(128) # user-friendly name of condition\n    ---\n    condition_description='': varchar(1000)\n    bin_size=0.04: float # bin-size (in second) used to compute the PSTH\n    \"\"\"\n\n    class Trial(dj.Part):\n\"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\"\n\n        definition = \"\"\"  # Trials on which to compute event-aligned spikes and PSTH\n        -&gt; master\n        -&gt; trial.Trial\n        \"\"\"\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.SpikesAlignmentCondition.Trial","title":"<code>Trial</code>","text":"<p>         Bases: <code>dj.Part</code></p> <p>Trials on which to compute event-aligned spikes and PSTH</p> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>class Trial(dj.Part):\n\"\"\"Trials on which to compute event-aligned spikes and PSTH\"\"\"\n\n    definition = \"\"\"  # Trials on which to compute event-aligned spikes and PSTH\n    -&gt; master\n    -&gt; trial.Trial\n    \"\"\"\n</code></pre>"},{"location":"api/workflow_array_ephys/analysis/#workflow_array_ephys.analysis.activate","title":"<code>activate(schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>","text":"<p>Activate this schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>schema name on the database server</p> required <code>create_schema</code> <code>bool</code> <p>when True (default), create schema in the database if it                 does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>str</code> <p>when True (default), create schema tables in the database                  if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>a module (or name) containing the required dependencies.</p> <code>None</code> Source code in <code>workflow_array_ephys/analysis.py</code> <pre><code>def activate(\n    schema_name, *, create_schema=True, create_tables=True, linking_module=None\n):\n\"\"\"Activate this schema.\n\n    Args:\n        schema_name (str): schema name on the database server\n        create_schema (bool): when True (default), create schema in the database if it\n                            does not yet exist.\n        create_tables (str): when True (default), create schema tables in the database\n                             if they do not yet exist.\n        linking_module (str): a module (or name) containing the required dependencies.\n    \"\"\"\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(linking_module), (\n        \"The argument 'dependency' must \" + \"be a module's name or a module\"\n    )\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=linking_module.__dict__,\n    )\n</code></pre>"},{"location":"api/workflow_array_ephys/export/","title":"export.py","text":"<p>For didactic purposes, import upstream NWB export functions</p> <p>Real use-cases should import these functions directly.</p>"},{"location":"api/workflow_array_ephys/ingest/","title":"ingest.py","text":""},{"location":"api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_alignment","title":"<code>ingest_alignment(alignment_csv_path='./user_data/alignments.csv', skip_duplicates=True, verbose=True)</code>","text":"<p>Ingest event alignment data from local CSVs</p> <p>Note: This is duplicated across wf-array-ephys and wf-calcium-imaging</p> <p>Parameters:</p> Name Type Description Default <code>alignment_csv_path</code> <code>str</code> <p>Relative path to event alignment csv. Defaults to \"./user_data/alignments.csv\".</p> <code>'./user_data/alignments.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>See DataJoint <code>insert</code> function. Default True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_alignment(\n    alignment_csv_path: str = \"./user_data/alignments.csv\",\n    skip_duplicates: bool = True,\n    verbose: bool = True,\n):\n\"\"\"Ingest event alignment data from local CSVs\n\n    Note: This is duplicated across wf-array-ephys and wf-calcium-imaging\n\n    Args:\n        alignment_csv_path (str, optional): Relative path to event alignment csv.\n            Defaults to \"./user_data/alignments.csv\".\n        skip_duplicates (bool, optional): See DataJoint `insert` function. Default True.\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n    \"\"\"\n\n    csvs = [alignment_csv_path]\n    tables = [event.AlignmentEvent()]\n\n    ingest_csv_to_table(csvs, tables, skip_duplicates=skip_duplicates, verbose=verbose)\n</code></pre>"},{"location":"api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_events","title":"<code>ingest_events(recording_csv_path='./user_data/behavior_recordings.csv', block_csv_path='./user_data/blocks.csv', trial_csv_path='./user_data/trials.csv', event_csv_path='./user_data/events.csv', skip_duplicates=True, verbose=True)</code>","text":"<p>Ingest each level of experiment hierarchy for element-trial</p> Ingestion hierarchy includes <p>recording, block (i.e., phases of trials), trials (repeated units), events (optionally 0-duration occurrences within trial).</p> <p>Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging</p> <p>Parameters:</p> Name Type Description Default <code>recording_csv_path</code> <code>str</code> <p>Relative path to recording csv. Defaults to \"./user_data/behavior_recordings.csv\".</p> <code>'./user_data/behavior_recordings.csv'</code> <code>block_csv_path</code> <code>str</code> <p>Relative path to block csv. Defaults to \"./user_data/blocks.csv\".</p> <code>'./user_data/blocks.csv'</code> <code>trial_csv_path</code> <code>str</code> <p>Relative path to trial csv. Defaults to \"./user_data/trials.csv\".</p> <code>'./user_data/trials.csv'</code> <code>event_csv_path</code> <code>str</code> <p>Relative path to event csv. Defaults to \"./user_data/events.csv\".</p> <code>'./user_data/events.csv'</code> <code>skip_duplicates</code> <code>bool</code> <p>See DataJoint <code>insert</code> function. Default True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_events(\n    recording_csv_path: str = \"./user_data/behavior_recordings.csv\",\n    block_csv_path: str = \"./user_data/blocks.csv\",\n    trial_csv_path: str = \"./user_data/trials.csv\",\n    event_csv_path: str = \"./user_data/events.csv\",\n    skip_duplicates: bool = True,\n    verbose: bool = True,\n):\n\"\"\"Ingest each level of experiment hierarchy for element-trial\n\n    Ingestion hierarchy includes:\n        recording, block (i.e., phases of trials), trials (repeated units),\n        events (optionally 0-duration occurrences within trial).\n\n    Note: This ingestion function is duplicated across wf-array-ephys and wf-calcium-imaging\n\n    Args:\n        recording_csv_path (str, optional): Relative path to recording csv.\n            Defaults to \"./user_data/behavior_recordings.csv\".\n        block_csv_path (str, optional): Relative path to block csv.\n            Defaults to \"./user_data/blocks.csv\".\n        trial_csv_path (str, optional): Relative path to trial csv.\n            Defaults to \"./user_data/trials.csv\".\n        event_csv_path (str, optional): Relative path to event csv.\n            Defaults to \"./user_data/events.csv\".\n        skip_duplicates (bool, optional): See DataJoint `insert` function. Default True.\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n    \"\"\"\n    csvs = [\n        recording_csv_path,\n        recording_csv_path,\n        block_csv_path,\n        block_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        trial_csv_path,\n        event_csv_path,\n        event_csv_path,\n        event_csv_path,\n    ]\n    tables = [\n        event.BehaviorRecording(),\n        event.BehaviorRecording.File(),\n        trial.Block(),\n        trial.Block.Attribute(),\n        trial.TrialType(),\n        trial.Trial(),\n        trial.Trial.Attribute(),\n        trial.BlockTrial(),\n        event.EventType(),\n        event.Event(),\n        trial.TrialEvent(),\n    ]\n\n    # Allow direct insert required because element-event has Imported that should be Manual\n    ingest_csv_to_table(\n        csvs,\n        tables,\n        skip_duplicates=skip_duplicates,\n        verbose=verbose,\n        allow_direct_insert=True,\n    )\n</code></pre>"},{"location":"api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_sessions","title":"<code>ingest_sessions(session_csv_path='./user_data/sessions.csv', verbose=True)</code>","text":"<p>Ingest SpikeGLX and OpenEphys files from directories listed in csv</p> <p>Parameters:</p> Name Type Description Default <code>session_csv_path</code> <code>str</code> <p>List of sessions. Defaults to \"./user_data/sessions.csv\".</p> <code>'./user_data/sessions.csv'</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Neither SpikeGLX nor OpenEphys recording files found in dir</p> <code>NotImplementedError</code> <p>Acquisition software provided does not match those implemented - SpikeGLX and OpenEphys</p> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_sessions(\n    session_csv_path: str = \"./user_data/sessions.csv\", verbose: bool = True\n):\n\"\"\"Ingest SpikeGLX and OpenEphys files from directories listed in csv\n\n    Args:\n        session_csv_path (str, optional): List of sessions.\n            Defaults to \"./user_data/sessions.csv\".\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n\n    Raises:\n        FileNotFoundError: Neither SpikeGLX nor OpenEphys recording files found in dir\n        NotImplementedError: Acquisition software provided does not match those\n            implemented - SpikeGLX and OpenEphys\n    \"\"\"\n\n    # ---------- Insert new \"Session\" and \"ProbeInsertion\" ---------\n    with open(session_csv_path, newline=\"\") as f:\n        input_sessions = list(csv.DictReader(f, delimiter=\",\"))\n\n    # Folder structure: root / subject / session / probe / .ap.meta\n    session_list, session_dir_list, = (\n        [],\n        [],\n    )\n    session_note_list, session_experimenter_list, lab_user_list = [], [], []\n    probe_list, probe_insertion_list = [], []\n\n    for sess in input_sessions:\n        session_dir = find_full_path(get_ephys_root_data_dir(), sess[\"session_dir\"])\n        session_datetimes, insertions = [], []\n\n        # search session dir and determine acquisition software\n        for ephys_pattern, ephys_acq_type in zip(\n            [\"*.ap.meta\", \"*.oebin\"], [\"SpikeGLX\", \"OpenEphys\"]\n        ):\n            ephys_meta_filepaths = [fp for fp in session_dir.rglob(ephys_pattern)]\n            if len(ephys_meta_filepaths):\n                acq_software = ephys_acq_type\n                break\n        else:\n            raise FileNotFoundError(\n                \"Ephys recording data not found! Neither SpikeGLX \"\n                + \"nor OpenEphys recording files found in: \"\n                + f\"{session_dir}\"\n            )\n\n        if acq_software == \"SpikeGLX\":\n            for meta_filepath in ephys_meta_filepaths:\n                spikeglx_meta = spikeglx.SpikeGLXMeta(meta_filepath)\n\n                probe_key = {\n                    \"probe_type\": spikeglx_meta.probe_model,\n                    \"probe\": spikeglx_meta.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n\n                probe_dir = meta_filepath.parent\n                probe_number = re.search(\"(imec)?\\d{1}$\", probe_dir.name).group()\n                probe_number = int(probe_number.replace(\"imec\", \"\"))\n\n                insertions.append(\n                    {\n                        \"probe\": spikeglx_meta.probe_SN,\n                        \"insertion_number\": int(probe_number),\n                    }\n                )\n                session_datetimes.append(spikeglx_meta.recording_time)\n        elif acq_software == \"OpenEphys\":\n            loaded_oe = openephys.OpenEphys(session_dir)\n            session_datetimes.append(loaded_oe.experiment.datetime)\n            for probe_idx, oe_probe in enumerate(loaded_oe.probes.values()):\n                probe_key = {\n                    \"probe_type\": oe_probe.probe_model,\n                    \"probe\": oe_probe.probe_SN,\n                }\n                if (\n                    probe_key[\"probe\"] not in [p[\"probe\"] for p in probe_list]\n                    and probe_key not in probe.Probe()\n                ):\n                    probe_list.append(probe_key)\n                insertions.append(\n                    {\"probe\": oe_probe.probe_SN, \"insertion_number\": probe_idx}\n                )\n        else:\n            raise NotImplementedError(\n                \"Unknown acquisition software: \" + f\"{acq_software}\"\n            )\n\n        # new session/probe-insertion\n        session_key = {\n            \"subject\": sess[\"subject\"],\n            \"session_datetime\": min(session_datetimes),\n        }\n        if session_key not in session.Session():\n            session_list.append(session_key)\n            root_dir = find_root_directory(get_ephys_root_data_dir(), session_dir)\n            session_dir_list.append(\n                {\n                    **session_key,\n                    \"session_dir\": session_dir.relative_to(root_dir).as_posix(),\n                }\n            )\n            session_note_list.append(\n                {**session_key, \"session_note\": sess[\"session_note\"]}\n            )\n            session_experimenter_list.append({**session_key, \"user\": sess[\"user\"]})\n            lab_user_list.append((sess[\"user\"], \"\", \"\"))  # empty email/phone\n            probe_insertion_list.extend(\n                [{**session_key, **insertion} for insertion in insertions]\n            )\n\n    session.Session.insert(session_list)\n    lab.User.insert(lab_user_list, skip_duplicates=True)\n    session.SessionDirectory.insert(session_dir_list)\n    session.SessionNote.insert(session_note_list)\n    session.SessionExperimenter.insert(session_experimenter_list)\n    if verbose:\n        print(f\"\\n---- Insert {len(session_list)} entry(s) into session.Session ----\")\n\n    probe.Probe.insert(probe_list)\n    if verbose:\n        print(f\"\\n---- Insert {len(probe_list)} entry(s) into probe.Probe ----\")\n\n    ephys.ProbeInsertion.insert(probe_insertion_list)\n    if verbose:\n        print(\n            f\"\\n---- Insert {len(probe_insertion_list)} entry(s) into \"\n            + \"ephys.ProbeInsertion ----\"\n        )\n        print(\"\\n---- Successfully completed ingest_subjects ----\")\n</code></pre>"},{"location":"api/workflow_array_ephys/ingest/#workflow_array_ephys.ingest.ingest_subjects","title":"<code>ingest_subjects(subject_csv_path='./user_data/subjects.csv', verbose=True)</code>","text":"<p>Ingest subjects listed in the subject column of ./user_data/subjects.csv</p> <p>Parameters:</p> Name Type Description Default <code>subject_csv_path</code> <code>str</code> <p>Relative path to subject csv. Defaults to \"./user_data/subjects.csv\".</p> <code>'./user_data/subjects.csv'</code> <code>verbose</code> <code>bool</code> <p>Print number inserted (i.e., table length change). Defaults to True.</p> <code>True</code> Source code in <code>workflow_array_ephys/ingest.py</code> <pre><code>def ingest_subjects(\n    subject_csv_path: str = \"./user_data/subjects.csv\", verbose: bool = True\n):\n\"\"\"Ingest subjects listed in the subject column of ./user_data/subjects.csv\n\n    Args:\n        subject_csv_path (str, optional): Relative path to subject csv.\n            Defaults to \"./user_data/subjects.csv\".\n        verbose (bool, optional): Print number inserted (i.e., table length change).\n            Defaults to True.\n    \"\"\"\n    # -------------- Insert new \"Subject\" --------------\n    with open(subject_csv_path, newline=\"\") as f:\n        input_subjects = list(csv.DictReader(f, delimiter=\",\"))\n    if verbose:\n        previous_length = len(subject.Subject.fetch())\n    subject.Subject.insert(input_subjects, skip_duplicates=True)\n    if verbose:\n        insert_length = len(subject.Subject.fetch()) - previous_length\n        print(f\"\\n---- Insert {insert_length} entry(s) into \" + \"subject.Subject ----\")\n        print(\"\\n---- Successfully completed ingest_subjects ----\")\n</code></pre>"},{"location":"api/workflow_array_ephys/localization/","title":"localization.py","text":"<p>Load CCF files.</p> <p>When imported, checks to see that CCF (nrrd and query.csv) files in the  ephys_root_data_dir have been loaded into  element_electrode_localization.coordinate_framework. Default voxel resolution is 100. To load other resolutions, please modify this script.</p>"},{"location":"api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_electrode_localization_dir","title":"<code>get_electrode_localization_dir(probe_insertion_key)</code>","text":"<p>Return root directory of localization data for a given probe</p> <p>Parameters:</p> Name Type Description Default <code>probe_insertion_key</code> <code>dict</code> <p>key uniquely identifying one ephys.EphysRecording</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Full path to localization data for either SpikeGLX or OpenEphys</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_electrode_localization_dir(probe_insertion_key: dict) -&gt; str:\n\"\"\"Return root directory of localization data for a given probe\n\n    Args:\n        probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording\n\n    Returns:\n        path (str): Full path to localization data for either SpikeGLX or OpenEphys\n    \"\"\"\n    from .pipeline import ephys\n\n    acq_software = (ephys.EphysRecording &amp; probe_insertion_key).fetch1(\"acq_software\")\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = pathlib.Path(\n            (\n                ephys.EphysRecording.EphysFile\n                &amp; probe_insertion_key\n                &amp; 'file_path LIKE \"%.ap.meta\"'\n            ).fetch1(\"file_path\")\n        )\n        probe_dir = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath.parent\n        )\n    elif acq_software == \"Open Ephys\":\n        probe_path = (ephys.EphysRecording.EphysFile &amp; probe_insertion_key).fetch1(\n            \"file_path\"\n        )\n        probe_dir = find_full_path(get_ephys_root_data_dir(), probe_path)\n\n    return probe_dir\n</code></pre>"},{"location":"api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n    return dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n</code></pre>"},{"location":"api/workflow_array_ephys/localization/#workflow_array_ephys.localization.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Return relative path from SessionDirectory table given key</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>Key uniquely identifying a session</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Relative path of session directory</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Return relative path from SessionDirectory table given key\n\n    Args:\n        session_key (dict): Key uniquely identifying a session\n\n    Returns:\n        path (str): Relative path of session directory\n    \"\"\"\n    from .pipeline import session\n\n    session_dir = (session.SessionDirectory &amp; session_key).fetch1(\"session_dir\")\n    return session_dir\n</code></pre>"},{"location":"api/workflow_array_ephys/paths/","title":"paths.py","text":""},{"location":"api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_electrode_localization_dir","title":"<code>get_electrode_localization_dir(probe_insertion_key)</code>","text":"<p>Return root directory of localization data for a given probe</p> <p>Parameters:</p> Name Type Description Default <code>probe_insertion_key</code> <code>dict</code> <p>key uniquely identifying one ephys.EphysRecording</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Full path to localization data for either SpikeGLX or OpenEphys</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_electrode_localization_dir(probe_insertion_key: dict) -&gt; str:\n\"\"\"Return root directory of localization data for a given probe\n\n    Args:\n        probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording\n\n    Returns:\n        path (str): Full path to localization data for either SpikeGLX or OpenEphys\n    \"\"\"\n    from .pipeline import ephys\n\n    acq_software = (ephys.EphysRecording &amp; probe_insertion_key).fetch1(\"acq_software\")\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = pathlib.Path(\n            (\n                ephys.EphysRecording.EphysFile\n                &amp; probe_insertion_key\n                &amp; 'file_path LIKE \"%.ap.meta\"'\n            ).fetch1(\"file_path\")\n        )\n        probe_dir = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath.parent\n        )\n    elif acq_software == \"Open Ephys\":\n        probe_path = (ephys.EphysRecording.EphysFile &amp; probe_insertion_key).fetch1(\n            \"file_path\"\n        )\n        probe_dir = find_full_path(get_ephys_root_data_dir(), probe_path)\n\n    return probe_dir\n</code></pre>"},{"location":"api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n    return dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n</code></pre>"},{"location":"api/workflow_array_ephys/paths/#workflow_array_ephys.paths.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Return relative path from SessionDirectory table given key</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>Key uniquely identifying a session</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Relative path of session directory</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Return relative path from SessionDirectory table given key\n\n    Args:\n        session_key (dict): Key uniquely identifying a session\n\n    Returns:\n        path (str): Relative path of session directory\n    \"\"\"\n    from .pipeline import session\n\n    session_dir = (session.SessionDirectory &amp; session_key).fetch1(\"session_dir\")\n    return session_dir\n</code></pre>"},{"location":"api/workflow_array_ephys/pipeline/","title":"pipeline.py","text":""},{"location":"api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_electrode_localization_dir","title":"<code>get_electrode_localization_dir(probe_insertion_key)</code>","text":"<p>Return root directory of localization data for a given probe</p> <p>Parameters:</p> Name Type Description Default <code>probe_insertion_key</code> <code>dict</code> <p>key uniquely identifying one ephys.EphysRecording</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Full path to localization data for either SpikeGLX or OpenEphys</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_electrode_localization_dir(probe_insertion_key: dict) -&gt; str:\n\"\"\"Return root directory of localization data for a given probe\n\n    Args:\n        probe_insertion_key (dict): key uniquely identifying one ephys.EphysRecording\n\n    Returns:\n        path (str): Full path to localization data for either SpikeGLX or OpenEphys\n    \"\"\"\n    from .pipeline import ephys\n\n    acq_software = (ephys.EphysRecording &amp; probe_insertion_key).fetch1(\"acq_software\")\n\n    if acq_software == \"SpikeGLX\":\n        spikeglx_meta_filepath = pathlib.Path(\n            (\n                ephys.EphysRecording.EphysFile\n                &amp; probe_insertion_key\n                &amp; 'file_path LIKE \"%.ap.meta\"'\n            ).fetch1(\"file_path\")\n        )\n        probe_dir = find_full_path(\n            get_ephys_root_data_dir(), spikeglx_meta_filepath.parent\n        )\n    elif acq_software == \"Open Ephys\":\n        probe_path = (ephys.EphysRecording.EphysFile &amp; probe_insertion_key).fetch1(\n            \"file_path\"\n        )\n        probe_dir = find_full_path(get_ephys_root_data_dir(), probe_path)\n\n    return probe_dir\n</code></pre>"},{"location":"api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_ephys_root_data_dir","title":"<code>get_ephys_root_data_dir()</code>","text":"<p>Return root directory for ephys from 'ephys_root_data_dir' in dj.config</p> <p>Returns:</p> Name Type Description <code>path</code> <code>any</code> <p>List of path(s) if available or None</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_ephys_root_data_dir():\n\"\"\"Return root directory for ephys from 'ephys_root_data_dir' in dj.config\n\n    Returns:\n        path (any): List of path(s) if available or None\n    \"\"\"\n    return dj.config.get(\"custom\", {}).get(\"ephys_root_data_dir\", None)\n</code></pre>"},{"location":"api/workflow_array_ephys/pipeline/#workflow_array_ephys.pipeline.get_session_directory","title":"<code>get_session_directory(session_key)</code>","text":"<p>Return relative path from SessionDirectory table given key</p> <p>Parameters:</p> Name Type Description Default <code>session_key</code> <code>dict</code> <p>Key uniquely identifying a session</p> required <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>Relative path of session directory</p> Source code in <code>workflow_array_ephys/paths.py</code> <pre><code>def get_session_directory(session_key: dict) -&gt; str:\n\"\"\"Return relative path from SessionDirectory table given key\n\n    Args:\n        session_key (dict): Key uniquely identifying a session\n\n    Returns:\n        path (str): Relative path of session directory\n    \"\"\"\n    from .pipeline import session\n\n    session_dir = (session.SessionDirectory &amp; session_key).fetch1(\"session_dir\")\n    return session_dir\n</code></pre>"},{"location":"api/workflow_array_ephys/process/","title":"process.py","text":""},{"location":"api/workflow_array_ephys/process/#workflow_array_ephys.process.run","title":"<code>run(display_progress=True, reserve_jobs=False, suppress_errors=False)</code>","text":"<p>Execute all populate commands in Element Array Ephys</p> <p>Parameters:</p> Name Type Description Default <code>display_progress</code> <code>bool</code> <p>See DataJoint <code>populate</code>. Defaults to True.</p> <code>True</code> <code>reserve_jobs</code> <code>bool</code> <p>See DataJoint <code>populate</code>. Defaults to False.</p> <code>False</code> <code>suppress_errors</code> <code>bool</code> <p>See DataJoint <code>populate</code>. Defaults to False.</p> <code>False</code> Source code in <code>workflow_array_ephys/process.py</code> <pre><code>def run(\n    display_progress: bool = True,\n    reserve_jobs: bool = False,\n    suppress_errors: bool = False,\n):\n\"\"\"Execute all populate commands in Element Array Ephys\n\n    Args:\n        display_progress (bool, optional): See DataJoint `populate`. Defaults to True.\n        reserve_jobs (bool, optional): See DataJoint `populate`. Defaults to False.\n        suppress_errors (bool, optional): See DataJoint `populate`. Defaults to False.\n    \"\"\"\n\n    populate_settings = {\n        \"display_progress\": display_progress,\n        \"reserve_jobs\": reserve_jobs,\n        \"suppress_errors\": suppress_errors,\n    }\n\n    print(\"\\n---- Populate ephys.EphysRecording ----\")\n    ephys.EphysRecording.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.LFP ----\")\n    ephys.LFP.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.Clustering ----\")\n    ephys.Clustering.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.CuratedClustering ----\")\n    ephys.CuratedClustering.populate(**populate_settings)\n\n    print(\"\\n---- Populate ephys.WaveformSet ----\")\n    ephys.WaveformSet.populate(**populate_settings)\n</code></pre>"},{"location":"api/workflow_array_ephys/version/","title":"version.py","text":"<p>Package metadata Update the Docker image tag in <code>docker-compose.yaml</code> to match</p>"},{"location":"api/workflow_array_ephys/plotting/plot_psth/","title":"plot_psth.py","text":""}]}